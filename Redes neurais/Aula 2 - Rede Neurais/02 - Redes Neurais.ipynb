{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes Neurais\n",
    "\n",
    "Na aula de hoje, vamos explorar os seguintes tópicos em Python:\n",
    "\n",
    "- TensorFlow e Keras;\n",
    "- MLP;\n",
    "- Treinando uma Rede Neural;\n",
    "- MNIST;\n",
    "- Exercícios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow e Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hoje vamos trabalhar com __as principais bibliotecas de redes neurais__, sendo a principal delas desenvolvida para Python, o [__TensorFlow__](https://www.tensorflow.org/).<br>\n",
    " <br>\n",
    "<img src=\"https://www.ambientelivre.com.br/images/logos_open_source/tensorflow_logo.png\" width=500>\n",
    " <br>\n",
    "O __TensorFlow__ é um __framework extremamente poderoso__, mas que apresenta uma __sintaxe consideravelmeente complicada__. Para facilitar a vida e descomplicar Redes Neurais, que surgiu uma outra biblioteca construída encima do TensorFlow, conhecida como [__Keras__](https://keras.io/) <br>\n",
    " <br>\n",
    " <img src=\"https://www.pyimagesearch.com/wp-content/uploads/2020/02/keras_autoencoders_header.png\" width=500>\n",
    " <br>\n",
    "Diferente com o que estamos acostumados do _SciKit-Learn_, o __Keras__ é focada exclusivamente em Redes Neurais, oferencendo implementação de funções de alto nível, construindo essas redes de forma prática e simples!<br><br>\n",
    " \n",
    "Claro que infelizmente até o __Keras__ têm suas limitações, quando precisar de modificações muito especificas, pode ser precise recorrer ao __TensorFlow__, apesar do __Keras__ possuir a __maioria das principais arquiteturas__ de Redes Neurais do __TensorFLow__ completamente integradas. \n",
    "<br><br>\n",
    "Para instalar tanto o __TensorFlow__ quanto o __Keras__ basta seguir o código abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalando as Bibliotecas\n",
    "#!pip install tensorflow\n",
    "#!pip install keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Layer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vimos na aula anterior a respeito do _Perceptron_, onde montamos uma rede neural a partir de um neurônio. É importante o conceito do _Perceptron_ para justamente entender o que existe por trás das redes neurais mais complexas.\n",
    "\n",
    "As redes __Multi Layer Perceptron__ (pela sigla MLP) sõa todos tipos de redes onde temos __diversas camadas de neurônios totalmente conectada__.\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*piYTTh83qsQJVUMOZKmN5w.png\" width= 600>\n",
    "\n",
    "<br>\n",
    "\n",
    "Conhecido já os principais conceitos por trás das redes neurais e agora fazendo a abstração para redes mais complexas, vamos treinar uma rede agora baseado no _MLP_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinando uma Rede Neural"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A \"aprendizagem\" da Rede Neural a partir dos dados se dá através de duas etapas, o **Forward Propagation** e o **Backward Propagation**.\n",
    "\n",
    "### Forward Propagation\n",
    "\n",
    "No __Forward Propagation__, a informação propaga na direção habitual (da esquerda para direita) na rede neural: features são lidas na camada de input, passam pelo processamento nas camadas ocultas, e a resposta (target) é predita na camada de output. \n",
    "\n",
    "Para que a predição seja realizada, os neurônios nas camadas ocultas realizam as seguintes duas etapas de cálculo:\n",
    "\n",
    "- Uma combinação linear entre o output (que denotamos pela letra **a**) da camada enterior e os pesos da camada atual. Isto é, se tivermos n ligações. a combinação linear é:\n",
    "\n",
    "![](https://latex.codecogs.com/gif.latex?z%5E%7Batual%7D%20%3D%20W_0%5E%7Batual%7D%20&plus;%20%5Cleft%20%28%20W_1%5E%7Batual%7D%5Ctimes%20a_1%5E%7Banterior%7D%20%5Cright%29%20&plus;%20%5Cleft%28%20W_2%5E%7Batual%7D%20%5Ctimes%20a_2%5E%7Banterior%7D%20%5Cright%20%29%20&plus;%20%5Ccdots%20&plus;%20%5Cleft%20%28%20W_n%5E%7Batual%7D%20%5Ctimes%20a_n%5E%7Banterior%7D%20%5Cright%29)\n",
    "\n",
    "- Aplica-se uma __função de ativação não-linear__ à combinação linear acima. As principais funções de ativação utilizadas são:\n",
    "<br><br>\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1000/1*4ZEDRpFuCIpUjNgjDdT2Lg.png\n",
    "\" width=\"800\" />\n",
    "\n",
    "O cálculo realizado por um único neurônio é bem parecido com um **perceptron**, ilustrado a seguir:\n",
    "<br>\n",
    "\n",
    "<img src=\"https://img2.gratispng.com/20180619/oav/kisspng-multilayer-perceptron-machine-learning-statistical-5b2996bdb9dcd2.4724873615294522217613.jpg\" width=\"400\" />\n",
    "<br>\n",
    "\n",
    "A escolha das funções de ativação também pode ser variável, mas costuma-se utilizar:\n",
    "\n",
    "- **ReLu** nas camadas ocultas;\n",
    "- **Sigmoid** (para problemas de classificação binários) ou **Softmax** (para probelams de classificação multiclasse) na camada de output.\n",
    "\n",
    "\n",
    "Ao fim do __Forward Propagation__, na camada de output, calculamos a **Função de Perda**, que quantifica qual a **diferença entre as predições feitas pela rede neural e os valores reais do target dos dados**. Cada tipo de problema tem uma função de perda própria.\n",
    "<br><br>\n",
    "\n",
    "Queremos que as predições sejam sempre o mais próximas o possível dos valores reais. Então, o que fazemos é **minimizar** a função de perda. \n",
    "\n",
    "\n",
    "### Backward Propagation\n",
    "\n",
    "Isto é feito ao propagarmos a informação na direção contrária (da direita para esquerda) na rede neural, o que caracteriza o chamado __Backward Propagation__. \n",
    "\n",
    "Para minimizar a função de perda, utilizamos um **Otimizador**, que são objetos que representam o procedimento matemático de minimização da função de perda. Os principais otimizadores utilizados são: __gradiente descendente (GD)__, **Adam** e **RMSProp**.\n",
    "\n",
    "Este processo de __Forward e Backward Propagation__ é feito iterativamente, ou seja por diversas vezes no desenvolvimento do modelo. Cada rodada é chamada de __época__ (**epoch**).\n",
    "\n",
    "O objetivo do Backward Propagation é **determinar os Pesos que miminizem a Função de Perda.** A cada epoch, os pesos são **atualizados**, de modo que a função de perda é sempre reduzida em direção ao seu mínimo!\n",
    "\n",
    "Para quem quiser saber mais, segue uma sugestão de leitura adicional: [Neural Networks Explained](https://medium.com/datadriveninvestor/neural-networks-explained-6e21c70d7818)\n",
    "\n",
    "Vamos agora ao nosso exemplo prático: construiremos nossa própria rede neural!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) O exemplo clássico: MNIST\n",
    "\n",
    "Neste exemplo, usaremos o **[MNIST](https://www.kaggle.com/c/digit-recognizer/data)**, o famoso dataset de dígitos (números de 0 a 9) escritos à mão, onde vocês podem conhecer mais a respeito neste [link](http://yann.lecun.com/exdb/mnist/)\n",
    "\n",
    "<br><br>\n",
    "<img src=\"https://i2.wp.com/syncedreview.com/wp-content/uploads/2019/06/MNIST.png?fit=530%2C297&ssl=1\" width=700>\n",
    "\n",
    "<br><br>\n",
    "O objetivo do nosso modelo será o de **classificar digítos, com base em imagens**. Assim sendo, temos um **problema de classificação multiclasse** (pois os dados serão classificados em uma dentre 10 classes possíveis, de 0 a 9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importe as bibliotecas padrões\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Algumas observações sobre a base :__\n",
    "\n",
    "- A base do MNIST é composta por imagens de números manuscritos, onde cada imagem é uma **matriz 28 x 28**, contendo assim **784 pixels**; \n",
    "\n",
    "- As imagens estão em escala de cinza, na qual cada pixel pode variar de **0 a 255**, e foram centralizadas, de forma que o número não fique \"cortado\" por estar na borda;\n",
    "\n",
    "- A base está dividida em 60 mil casos para treino e 10 mil casos para teste;\n",
    "\n",
    "\n",
    "Na função ``carrega_mnist()``, o objetivo é ler a base e retornar dois numpy arrays (a = 60000 ou a = 10000, a depender do tipo): \n",
    "\n",
    "- o primeiro da forma (a, 784), que são os 784 pixels da imagem organizados de forma sequencial;\n",
    "- o segundo da forma (a, 1), que é o identificador (label) da imagem, sendo um número que varia de 0 a 9.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função essencial para ler a base do MNIST\n",
    "import os\n",
    "import gzip\n",
    "\n",
    "def carrega_mnist(caminho, tipo = 'train'):\n",
    "    label_caminho = os.path.join(caminho, '%s-labels-idx1-ubyte.gz' % tipo)\n",
    "    imagens_caminho = os.path.join(caminho, '%s-images-idx3-ubyte.gz' % tipo)\n",
    "        \n",
    "    with gzip.open(label_caminho, 'rb') as lbpath:\n",
    "        lbpath.read(8)\n",
    "        buffer = lbpath.read()\n",
    "        labels = np.frombuffer(buffer, dtype=np.uint8)\n",
    "\n",
    "    with gzip.open(imagens_caminho, 'rb') as imgpath:\n",
    "        imgpath.read(16)\n",
    "        buffer = imgpath.read()\n",
    "        imagens = np.frombuffer(buffer, \n",
    "                               dtype=np.uint8).reshape(\n",
    "            len(labels), 784).astype(np.float64)\n",
    " \n",
    "    return imagens, labels\n",
    "\n",
    "X_train, y_train = carrega_mnist('')\n",
    "X_test, y_test = carrega_mnist('', \"t10k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print do tamanho de X\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print do tamanho de y\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fazendo uma visualização do Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uma imagem qualquer do dataset de treino\n",
    "idx = 32\n",
    "features = X_train[idx]\n",
    "target = y_train[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Reshape das features\n",
    "img = features.reshape(28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print da Label do número\n",
    "print(\"A label é: \", target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostra a imagem do número\n",
    "plt.imshow(img, cmap = 'gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré-Processamento dos Dados\n",
    "\n",
    "Um processo muito importante quando trabalhamos com Redes Neurais é o __Rescalamento__, pois é interessante que o os valores sejam rescalados **entre 0 e 1 para que o tempo de treinamento seja otimizado**. Não iremos enttrar em tantos detalhes sobre o motivo dessa otimização, mas [nesse link](https://stackoverflow.com/questions/4674623/why-do-we-have-to-normalize-the-input-for-an-artificial-neural-network) poderam ter mais detalhes sobre esse processo. \n",
    "\n",
    "Então, fazemos a divisão correpondentes aos pixels das imagens pelo pixel de valor máximo do **conjunto de treino**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descobrir o valor maximo dentro dos dados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normaliza os dados\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em modelos de rede neural de classificação, necessariamente **devemos ter o target transformado em variáveis dummy**, pois nossa resposta precisa ter a estrutura de output correta, como vimos na animação (GIF) acima. Podemos fazer isso com o __OneHot Encoder__, como já vimos anteriormente.<br><br>\n",
    "\n",
    "Esse tipo de mapeamento é um vetor com a mesma quantidade de classes existentes, com cada posição desse vetor correspondendo a uma das classes. (No nosso caso, como temos 10 classes possíveis, este vetor terá 10 elementos!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Mostra os primeiro valores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega o OneHotEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instancia o OHEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converte usando o OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resultado para os mesmos primeiros dados\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conjunto de Validação\n",
    "\n",
    "Em modelos de rede neural, é importante que exista um terceiro conjunto além de treino e teste, o **conjunto de validação**.\n",
    "\n",
    "Durante o treinamento, a depender do caso, **risco de ocorrer overfitting é bastante alto**.\n",
    "\n",
    "Uma forma de verificar e evitar isso é por meio do conjunto de validação, o qual é um subconjunto do conjunto de treino que não é utilizado pelo otimizador.  \n",
    "\n",
    "Espera-se que a o valor da função de perda (loss) vá diminuindo a cada época, tanto para o conjunto de treino como para o conjunto de validação. Porém, se a função de perda diminui para o conjunto de treino e aumenta para o conjunto de validação (ocorrendo assim um descolamento), podemos concluir que está ocorrendo um overfitting para o conjunto de treino.\n",
    "\n",
    "Deve-se então interromper o processo de treinamento. Esse comportamento pode ser visto com mais facilidade nos próximos tópicos.\n",
    "\n",
    "Para fazer a divisão, usamos a função ``train_test_split`` do ``sklearn`` e definimos que o conjunto de validação será 10% do conjunto de treino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega o train_test_split\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separa a base de treino, em treino e validação\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shapes das bases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print dos shapes ddas targets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arquitetura da Rede\n",
    "\n",
    "Faremos uma rede simples, com:\n",
    "\n",
    "- **3 camadas ocultas**;\n",
    "- **25 neurônios cada**; \n",
    "- **Camadas densas**;\n",
    "- **função de ativação ReLu nas camadas escondidas**;\n",
    "- **10 neurônios e ativação Softmax na camada de saída**. \n",
    "\n",
    "Para a construção da rede neural, utilizaremos a biblioteca [Keras](https://keras.io/).\n",
    "\n",
    "Primeiro definimos que nossa rede é do tipo **sequencial**, utilizando a função ``Sequential()``. \n",
    "\n",
    "O Keras funciona de forma super intuitiva: construímos **a arquitetura** de nossa rede neural camada a camada, construindo assim a rede neural da forma que queiramos!\n",
    "\n",
    "[Clique aqui](https://keras.io/api/layers/) para conhecer melhor sobre a estrutura de camadas do Keras, e dar uma olhada nas camadas disponíveis!\n",
    "\n",
    "Dessa forma, utilizamos o método ``.add()`` para adicionar as camadas, uma após a outra, de forma que a saída da camada anterior camada seja a entrada da próxima camada. \n",
    "\n",
    "> A primeira camada adicionada é uma **camada densa (fully connected)** com 25 neurônios. Para isso, é usada a função ``Dense()``, que conecta todos os valores de input e todos o neurônios da camada. \n",
    "Na primeira camada, é necessário **definir a dimensão dos dados de entrada**. Em camadas subsequentes isso não será necessário, já que o Keras automaticamente determinará que a dimensão de entrada será a dimensão da saída da camada anterior. \n",
    "No caso da rede em questão, os **784 pixels da imagem estarão todos conectados, um a um, aos 25 neurônios da primeira camada, produzindo 25 saídas nessa camada**.\n",
    "\n",
    "> Após a camada densa, é necessário inserir **a camada de ativação**, por meio da função ``Activation()``. Ao usar essa função, é necessário definir qual função de fato será usada passando uma string como parâmetro pra função. Dentre as disponíveis no Keras estão ``'tanh'``, ``'sigmoid'``, ``'softmax'`` e ``'relu'``. Nessa camada, a `ReLu` será usada.\n",
    "\n",
    "Esse processo de ``Dense()`` e ``Activation()`` é repetido mais duas vezes com os mesmos parâmetros. \n",
    "\n",
    "> Posteriormente, por ser um problema de classificação com 10 classes diferentes, é importante que a **a camada de saída possua 10 neurônios**. Dessa forma, cada neurônio representará uma classe, e a predição final será a correspondente ao neurônio **com maior valor de saída**. \n",
    "Outro fator importante é o uso de uma **camada de ativação Softmax após essa última camada com 10 neurônios**. \n",
    "A ativação Softmax funciona como uma versão mais generalizada da função sigmóide (ou logística), usada para **predições multiclasse**\n",
    "Ou seja, com valores entre 0 e 1, a saída de cada neurônio pode ser interpretada como a **probabilidade daquela instância pertencer àquela classe**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega as camadas para a Rede\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instancia a Rede\n",
    "\n",
    "\n",
    "# Definindo camada de input e primeira camada oculta\n",
    "\n",
    "\n",
    "# Definir segunda camada oculta\n",
    "\n",
    "\n",
    "# Definir terceira camada oculta\n",
    "\n",
    "\n",
    "# Camada de output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A rede então deve ser compilada usando o método ``compile()``. Devem ser passados a [função de perda](https://keras.io/losses/) e o [otimizador](https://keras.io/optimizers/) a serem utilizados. \n",
    "\n",
    "Para a função de perda, usamos ``categorial_crossentropy`` por ser um **problema de classificação multiclasse** (não binário). \n",
    "\n",
    "Já o otimizador escolhido foi o ``RMSprop()``.\n",
    "\n",
    "Diferentes funções de perda possuem diferentes propósitos, devendo ser escolhido caso a caso. Já os otimizadores possuem maior liberdade de escolha, não existindo uma regra fechada.\n",
    "\n",
    "Artigo sobre `RMSprop`: [aqui](https://towardsdatascience.com/understanding-rmsprop-faster-neural-network-learning-62e116fcf29a)\n",
    "\n",
    "Artigo sobre o Gradiente descendente, Adam e outros métodos: [aqui](https://towardsdatascience.com/how-do-we-train-neural-networks-edd985562b73)\n",
    "\n",
    "A arquitetura da rede pode ser visualizada com o método ``.summary()``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega o otimizador\n",
    "from tensorflow.keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Compila a rede\n",
    "\n",
    "\n",
    "# Define os pesos\n",
    "\n",
    "\n",
    "# Print do Sumário\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# visualizando os pesos iniciais\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento\n",
    "\n",
    "Agora vem a parte de treinamento da rede neural. Para isso, basta usar o método ``.fit()`` e definir a quantidade de epochs, conjunto de treino e conjunto de validação. O parâmetro ``verbose`` define se informações de treinamento devem ser exibidas na tela a cada epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit da Rede\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O que está acontecendo acima?\n",
    "\n",
    "O treinamento de uma rede neural (e de todo modelo de machine learning, na verdade) consiste em um **problema de otimização**, em que queremos encontrar **o valor mínimo da função de perda**\n",
    "\n",
    "No nosso caso, a função de perda é a \"__categorical crossentropy__, cuja expressão matemática é:\n",
    "\n",
    "$$ L(y, \\hat{y}) = - \\sum_i{y_i \\log \\hat{y}_i}$$, \n",
    "\n",
    "onde $\\hat{y}$, a previsão do modelo, é uma expressão bem complicada, que relaciona todos os pesos e funções de ativação de toda a rede neural.\n",
    "\n",
    "Como mencionamos, as redes neurais utilizam o procedimento de **Backward Propagation** em seu treinamento (cujo objetivo é **determinar os pesos**):\n",
    "\n",
    "<img src=\"https://machinelearningknowledge.ai/wp-content/uploads/2019/10/Backpropagation.gif\" width=600>\n",
    "\n",
    "Matematicamente, o Backward Propagation é implementado através de algum **método de otimização**, com o fim de **determinar os pesos que minimizam a função de perda**. O principal método utilizado para este fim é o **gradiente descendente**:\n",
    "\n",
    "<img src=\"https://thumbs.gfycat.com/AngryInconsequentialDiplodocus-size_restricted.gif\" width=1000>\n",
    "\n",
    "Então, o que está acontecendo acima, é que **em cada epoch**, o Keras faz o Forward Propagation, avalia as predições, e depois segue com o Backward Propagation para atualizar os pesos. Este processo é realizado pelo número de vezes (epochs) que determinarmos!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após o __modelo treinado__, agora devemos **fazer predições** e **avaliar a performance**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria a predict para a Rede Neural\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retorna o y_test ao original\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega as funções\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Print do classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bibliotecas para a matriz de confusão\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Cria a matriz de confusão\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O modelo teve uma performance muito boa, com uma acurácia de 96%, mas algumas coisas devem ser observadas a partir desse resultado como por exemplo a possibilidade de __Overfitting!__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O **Overfitting** é algo que pode ser bastante comum em redes neurais se não for bem tratado, porque ele é um modelo altamente não linear. Relembrando, o overfitting está intimamente relacionado com o tradeoff viés-variância:\n",
    "\n",
    "<img src=\"https://www.learnopencv.com/wp-content/uploads/2017/02/Bias-Variance-Tradeoff-In-Machine-Learning-1.png\" width=600>\n",
    "\n",
    "Podemos visualizar esta característica em nosso modelo ao **plotar** o valor da **função de perda** a cada epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss e val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot train Test Loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pelo gráfico, é possível perceber que mais ou menos a partir da 15ª época a função de perda continuou decrescedo pro conjunto de treinamento, mas no conjunto de validação isso se manteve fixo por um tempo e depois começou a perder desempenho.\n",
    "\n",
    "A curva acima é muito parecida com o nosso exemplo do tradeoff vié-variância, não é mesmo?!\n",
    "\n",
    "Temos, portanto, um forte indício que está começando a ocorrer overfitting!\n",
    "\n",
    "Para evitarmos isso, pode ser interessante que o **treinamento seja interrompido antes do overfitting começar a ocorrer**! Esta técnica é conhecida como **early stopping**:\n",
    "<br><br>\n",
    "\n",
    "### Early Stopping\n",
    "\n",
    "Para interromper um treinamento, podemos usar o callback de ``EarlyStopping()``. Esse callback pode receber alguns parâmetros, como o que ele deve monitorar (``monitor``), a diferença mínima que deve ser considerada pra ser um avanço (``min_delta``) e a quantidade de épocas que devem se manter sem avanço até de fato interromper o treinamento (``patience``). No exemplo a seguir, estamos monitorando a função de perda do conjunto de validação, consideramos um avanço mínimo o valor de 0.001 e paciência de 10 épocas. \n",
    "\n",
    "Ou seja, **se a função de perda na validação não tiver uma redução de pelo menos 0.001 por 10 épocas**, o treinamento será interrompido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Carrega o callback\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define o EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retomando as valores de pesos iniciais igual ao primeiro treinamento\n",
    "\n",
    "\n",
    "# Roda novamente o historico\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É possível perceber que o treinamento foi interrompido pouco depois da __epoch 15__, bem antes do total de epochs definido inicialmente. Vamos agora fazer predições e avaliar o modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss e val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot da Train Test Loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria o y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega o classification_report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Print dos resultados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de Confusão\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Cria a matriz de Confusão\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora garantindo que não houvesse o deslocamento entre o __treino__ e a __validação__ obtemos uma resultado muito bom para o nosso modeolo, e isso é um forte indicativo de que o Early Stopping ajudou a evitar o Overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vimos acima uma forma de evitar o Overfitting em Redes Neurais, mas esta definitivamente não é a única maneira! Existem inúmeras técnicas para mitigar este problema, entre elas a aplicação de **técnicas de regularização** (leia sobre isso [aqui](https://towardsdatascience.com/how-to-improve-a-neural-network-with-regularization-8a18ecda9fe3) e [aqui](https://towardsdatascience.com/regularization-techniques-and-their-implementation-in-tensorflow-keras-c06e7551e709))), cuja implementação existe no Keras! (mais informações [aqui](https://keras.io/api/layers/regularization_layers/) e [aqui](https://keras.io/api/layers/regularizers/)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1)__ Refaça o exercício exemplo da aula para fixar conceitos!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2)__ Preencha os códigos nos __locais indicados!!!__\n",
    "\n",
    "O exercicio será montar uma Rede Neural de uma importante base de dados, muito conhecida também, chamada de __Fashion MNIST__, onde queremos classificar roupas em algumas categorias. Está base de dados contém 70,000 imagens em tons de cinza em 10 categorias. As imagens mostram artigos individuais de roupas com baixa resolução (28 por 28 pixels), como vemos aqui:\n",
    "\n",
    "<img src=\"https://tensorflow.org/images/fashion-mnist-sprite.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow e tf.keras\n",
    "import tensorflow\n",
    "import keras\n",
    "\n",
    "# Bibliotecas Auxiliares\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Baixando a nossa base de dados\n",
    "\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregamos 4 arrays NumPy divididos em treino e teste, e além disso as imagens são arrays NumPy de 28x28, com os valores de pixels entre 0 to 255. As labels (alvo da classificação) são um array de inteiros, no intervalo de 0 a 9. Esse corresponde com a classe de roupa que cada imagem representa:\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Label</th>\n",
    "    <th>Classe</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>0</td>\n",
    "    <td>Camisetas/Top (T-shirt/top)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>1</td>\n",
    "    <td>Calça (Trouser)</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>2</td>\n",
    "    <td>Suéter (Pullover)</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>3</td>\n",
    "    <td>Vestidos (Dress)</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>4</td>\n",
    "    <td>Casaco (Coat)</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>5</td>\n",
    "    <td>Sandálias (Sandal)</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>6</td>\n",
    "    <td>Camisas (Shirt)</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>7</td>\n",
    "    <td>Tênis (Sneaker)</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>8</td>\n",
    "    <td>Bolsa (Bag)</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>9</td>\n",
    "    <td>Botas (Ankle boot)</td>\n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['T-shirt/top', \n",
    "               'Trouser', \n",
    "               'Pullover', \n",
    "               'Dress', \n",
    "               'Coat',\n",
    "               'Sandal', \n",
    "               'Shirt', \n",
    "               'Sneaker', \n",
    "               'Bag', \n",
    "               'Ankle boot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré-Processamento dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(train_images[0])\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape\n",
    "\n",
    "#Faça o reshape da train_images dividindo por 255. e salvando na mesma variavel\n",
    "### COLOQUE SEU CODIGO AQUI ###\n",
    "\n",
    "\n",
    "#Faça o reshape da test_images dividindo por 255. e salvando na mesma variavel\n",
    "### COLOQUE SEU CODIGO AQUI ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alguns plots da base de teste\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(train_images[i], cmap=plt.cm.binary)\n",
    "    plt.xlabel(class_names[train_labels[i]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uma outra forma de criarmos a Rede Neural camada a camada é da seguinte forma:\n",
    "\n",
    "model = keras.Sequential([\n",
    "        keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        keras.layers.Dense(128, activation='relu'),\n",
    "        keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crie um compile() com os seguintes parametros:\n",
    "# optimizer = 'adam'\n",
    "# loss = 'sparse_categorical_crossentropy'\n",
    "# metrics=['accuracy']\n",
    "\n",
    "### COLOQUE SEU CODIGO AQUI ###\n",
    "\n",
    "# Crie o summary() para o modelo\n",
    "### COLOQUE SEU CODIGO AQUI ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faça o fit do modelo passando train_images, train_labels e epochs = 10\n",
    "### COLOQUE SEU CODIGO AQUI ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliando a acurácia no teste\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
    "\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels = np.argmax(model.predict(test_images), axis = 1)\n",
    "pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Faça o print do classification_report para pred_labels e test_labels\n",
    "### COLOQUE SEU CODIGO AQUI ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Crie a matriz de confusão para o pred_labels e test_labels\n",
    "### COLOQUE SEU CODIGO AQUI ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trazendo a probabilidade de cada classe\n",
    "predictions = model.predict(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para plotar as imagens\n",
    "def plot_image(i, predictions_array, true_label, img):\n",
    "  predictions_array, true_label, img = predictions_array[i], true_label[i], img[i]\n",
    "  plt.grid(False)\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "\n",
    "  plt.imshow(img, cmap=plt.cm.binary)\n",
    "\n",
    "  predicted_label = np.argmax(predictions_array)\n",
    "  if predicted_label == true_label:\n",
    "    color = 'blue'\n",
    "  else:\n",
    "    color = 'red'\n",
    "\n",
    "  plt.xlabel(\"{} {:2.0f}% ({})\".format(class_names[predicted_label],\n",
    "                                100*np.max(predictions_array),\n",
    "                                class_names[true_label]),\n",
    "                                color=color)\n",
    "\n",
    "# Cria o gráfico com as probabilidades\n",
    "def plot_value_array(i, predictions_array, true_label):\n",
    "  predictions_array, true_label = predictions_array[i], true_label[i]\n",
    "  plt.grid(False)\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "  thisplot = plt.bar(range(10), predictions_array, color=\"#777777\")\n",
    "  plt.ylim([0, 1])\n",
    "  predicted_label = np.argmax(predictions_array)\n",
    "\n",
    "  thisplot[predicted_label].set_color('red')\n",
    "  thisplot[true_label].set_color('blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste com o id = 1\n",
    "\n",
    "i = 0\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.subplot(1,2,1)\n",
    "plot_image(i, predictions, test_labels, test_images)\n",
    "plt.subplot(1,2,2)\n",
    "plot_value_array(i, predictions,  test_labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plota o primeiro X test images, e as labels preditas, e as labels verdadeiras.\n",
    "# Colore as predições corretas de azul e as incorretas de vermelho.\n",
    "\n",
    "num_rows = 5\n",
    "num_cols = 3\n",
    "num_images = num_rows*num_cols\n",
    "plt.figure(figsize=(2*2*num_cols, 2*num_rows))\n",
    "for i in range(num_images):\n",
    "  plt.subplot(num_rows, 2*num_cols, 2*i+1)\n",
    "  plot_image(i, predictions, test_labels, test_images)\n",
    "  plt.subplot(num_rows, 2*num_cols, 2*i+2)\n",
    "  plot_value_array(i, predictions, test_labels)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
