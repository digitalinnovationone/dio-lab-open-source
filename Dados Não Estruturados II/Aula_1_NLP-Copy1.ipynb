{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J2HFkxmHfd_R"
   },
   "source": [
    "## Aula 1 - Processamento de Linguagem Natual\n",
    "\n",
    "Na aula de hoje, vamos explorar os seguintes t√≥picos em Python:\n",
    "\n",
    "1) Dados Estruturados e N√£o Estruturados.  \n",
    "2) Introdu√ß√£o a NLP.  \n",
    "3) Processamento de Textos.  \n",
    "4) Exerc√≠cios.  \n",
    "\n",
    "<img src=\"https://i1.wp.com/thedatascientist.com/wp-content/uploads/2018/09/data_science_wordcloud.png?fit=1584%2C1008&ssl=1\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hgvh29EJh6UK"
   },
   "source": [
    "Primeiramente, precisamos entender qual a diferen√ßa enre as duas fontes de dados mais comuns, sendo elas dados **estruturados** e **n√£o estruturados**. Definimos ele como:\n",
    "<br><br>\n",
    "- **Dados Estruturados:** S√£o dados que seguem uma estrutura mais r√≠gida com um padr√£o fixo e constante. Por exemplo: Tabelas e DataFrames;<br><br>\n",
    "- **Dados N√£o estruturados:** Como j√° diz o nome, s√£o dados que n√£o tem uma estrutura bem estabelecida e necessitam de um processamento adicional para trabalharmos com eles. Exemplos: √°udios, v√≠deos, textos e etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whQI1eePiJwx"
   },
   "source": [
    "### Introdu√ß√£o ao Processamento de Linguagem Natural (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bwjo0xx9h-Pi"
   },
   "source": [
    "O Processamento de Linguagem Natural, mas conhecido como NLP, √© a abordagem onde trabalhamos com **dados n√£o estruturados** do tipo **Texto**. O objetivo de trabalharmos com textos √© extrair de informa√ß√£o e teor lingu√≠stico das nossas bases de textos e converter isso de uma forma n√∫merica, onde poderemos utilizar em nossos modelos de *Machine Learning*.<br><br>\n",
    "Temos como exemplos de aplica√ß√µes de NLP como:\n",
    "- An√°lise de Sentimentos em review de filmes e produtos ou mensagens em redes sociais;\n",
    "- Filtro de E-Mails Spams e N√£o Spams;\n",
    "- Identifica√ß√£o de textos a partir de constru√ß√µes lingu√≠sticas (descobrir se um texto foi escrito ou n√£o por Machado de assis);\n",
    "- Tradutores de Idiomas;\n",
    "- ChatBots;\n",
    "- Corretores Ortogr√°ficos;\n",
    "- Classifica√ß√£o de textos de acordo com o conte√∫do do texto (Esportes, Pol√≠tica, Economia e etc).\n",
    "<br><br>\n",
    "Nesta aula iremos aprender a partir dos nossos dados textuais a como processar, tratar e transformar os dados de uma maneira que os modelos de *Machine Learning* entendam.<br><br>\n",
    "\n",
    "A principal biblioteca de refer√™ncia para NLP chama-se [NLTK - Natural Language Toll Kit](https://www.nltk.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IjXHV5QOjWDc"
   },
   "source": [
    "### Processamento de Textos\n",
    "\n",
    "Antes de mais nada, precisamos filtrar e tratar os nossos textos, de forma a deixar apenas o conte√∫do de mais relevantes para a nossa an√°lise. Existem alguns processos importantes para trabalhar com os textos (n√£o necessariamente voc√™ precisa aplicar todos os procesos)\n",
    "\n",
    "- Remo√ß√£o de Stopwords;\n",
    "- Limpeza de Textos;\n",
    "- Tokeniza√ß√£o;\n",
    "- Normaliza√ß√£o do Texto.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9je2oXCjjrwc"
   },
   "source": [
    "### Stopwords\n",
    "\n",
    "Stopwords s√£o palavras que aparecem com uma frequ√™ncia muito alta nos textos, mas que n√£o trazem um teor de conte√∫do relevante para o nosso modelo. Vamos entender isso na pr√°tica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages (3.7)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages (from nltk) (4.62.3)\n",
      "Requirement already satisfied: click in /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages (from nltk) (8.0.3)\n",
      "Requirement already satisfied: joblib in /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages (from nltk) (2022.3.15)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the '/opt/homebrew/Caskroom/miniforge/base/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3IR-pN2DjwXJ",
    "outputId": "9c8b59ea-653b-457e-b82e-d36bc3f07b67"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/cecilia/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/cecilia/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package rslp to /Users/cecilia/nltk_data...\n",
      "[nltk_data]   Package rslp is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')  # https://www.nltk.org/_modules/nltk/tokenize/punkt.html\n",
    "nltk.download('rslp')   # Stemmer em portugu√™s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_en = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_pt = nltk.corpus.stopwords.words('portuguese')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jR2RHfCOn83i"
   },
   "source": [
    "Baixada a fun√ß√£o de Stopwords, vamos definir um set de stopwords onde teremos uma lista com todas as stopwords em ingl√™s j√° identificadas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['my', 'house', 'is', 'black', 'and', 'white']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = ['my', 'house', 'is', 'black', 'and', 'white']\n",
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A9Dpg-8ApIxa"
   },
   "source": [
    "Vamos agora aplicar a remo√ß√£o de Stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my True\n",
      "house False\n",
      "is True\n",
      "black False\n",
      "and True\n",
      "white False\n"
     ]
    }
   ],
   "source": [
    "print(example[0], example[0] in words_en)\n",
    "print(example[1], example[1] in words_en)\n",
    "print(example[2], example[2] in words_en)\n",
    "print(example[3], example[3] in words_en)\n",
    "print(example[4], example[4] in words_en)\n",
    "print(example[5], example[5] in words_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "house\n",
      "black\n",
      "white\n"
     ]
    }
   ],
   "source": [
    "for word in example:\n",
    "    if not word in words_en:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wnYz481KOjkM"
   },
   "source": [
    "### Limpeza do Texto\n",
    "\n",
    "Existem alguns cuidados com rela√ß√£o a grafia das palavras e elementos em um texto que devemos tomar bastante cuidado antes de fazer qualquer outra coisa. Esses pontos s√£o:<br><br>\n",
    "- Transformar todas as palavras para MAI√öSCULAS ou min√∫sculas;\n",
    "- Remover caracteres especiais;\n",
    "- Remover d√≠gitos (quando n√£o forem relevantes);\n",
    "- Remover acentua√ß√£o (caso t√≠pico de quando trabalhamos com textos em Portugu√™s);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "farmac√™utico\n",
      "relogio\n",
      "parada\n"
     ]
    }
   ],
   "source": [
    "words = ['farMac√™uTICO', 'relogio', 'Parada']\n",
    "for word in words:\n",
    "    print(word.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HYRTjdWiTKWu"
   },
   "source": [
    "### Remo√ß√£o de d√≠gitos, caracteres especiais e qualquer outro item que n√£o queremos no texto\n",
    "\n",
    "Para essa etapa do processo, iremos utilizar uma biblioteca auxiliar [RegEx (Regular Expression)](https://docs.python.org/3/library/re.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "iE4HC6guTNGR"
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2n2D1Od-TVVx"
   },
   "source": [
    "Importada a biblioteca, vamos utilizar a fun√ß√£o *re.sub*, para substituir os elementos que n√£o queremos nos nossos textos:\n",
    "\n",
    "**Procure por re.sub**  \n",
    "https://docs.python.org/3/library/re.html\n",
    "\n",
    "Obs: Utilize o site https://regex101.com/ para criar o regex e o re.sub para substituir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'S√£o mais de  cursos de Data Science!!!! #datascience #machinelearning'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frase = 'S√£o mais de 50 cursos de Data Science!!!! #datascience #machinelearning'\n",
    "re.sub(r'\\d', '', frase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'S o mais de cursos de Data Science datascience machinelearning'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r'[^a-zA-Z]+','', frase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zuqfqkjuabuH"
   },
   "source": [
    "Utilizem a documenta√ß√£o para descobrir mais c√≥digos para filtrar elementos ou mesmo deem uma olhada nesse artigo, que resume de uma forma bem visual as aplica√ß√µes do RegEx: [clique aqui](https://amitness.com/regex/)\n",
    "\n",
    "Tutorial de regex https://blog.geekhunter.com.br/python-regex/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HeoTPv_4agE2"
   },
   "source": [
    "Hoje os emojis fazem parte da comunica√ß√£o via mensagens, por isso iremos ver como utilizar frases contento emojis e trata-los de forma adequada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "fCZwZhJLaTnj"
   },
   "outputs": [],
   "source": [
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Python √© üëç'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texto = 'Python √© :thumbs_up:'\n",
    "emoji.emojize(texto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c9HCV1QKbjXG"
   },
   "source": [
    "Para ver a lista completa:\n",
    "\n",
    "https://www.webfx.com/tools/emoji-cheat-sheet/\n",
    "\n",
    "Link biblioteca: https://github.com/carpedm20/emoji/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 1\n",
    "Utilizar Regex para validar CPFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern=r\"^(\\d{3}.){2}\\d{3}-\\d{2}$\" ### qual √© a expressao regular do regex neste caso\n",
    "cpf = '000.000.000-00'\n",
    "bool(re.match(pattern, cpf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZap5orRdLUE"
   },
   "source": [
    "### Exerc√≠cio 2\n",
    "\n",
    "Utilizar Regex para valida√ß√£o de emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email = 'cflavs.7@gmail.com'\n",
    "pattern = r\"([-a-zA-Z0-9.`?{}]+@\\w+\\.\\w+)\"  \n",
    "patterns = re.compile(pattern)\n",
    "bool(re.match(patterns, email))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "laoYA1wYg6Yj"
   },
   "source": [
    "Podemos utilizar NLP para identificar frases que contenham uma palavra chave!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K5f4aJfl7QCC"
   },
   "source": [
    "### Remo√ß√£o de Acentua√ß√£o\n",
    "Para a remo√ß√£o de acentua√ß√£o, iremos utilizar uma bibloteca chamada *Unidecode*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a7hpJwTa7SIY",
    "outputId": "066869e7-ca3f-45b3-9b60-c6598f820639"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: unidecode in /usr/local/lib/python3.7/dist-packages (1.3.4)\n"
     ]
    }
   ],
   "source": [
    "# Caso precise instalar a biblioteca, descomente o c√≥digo abaixo\n",
    "!pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "8eLSKg7A7TS5"
   },
   "outputs": [],
   "source": [
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cec√≠lia\n"
     ]
    }
   ],
   "source": [
    "string = 'Cec√≠lia'\n",
    "print(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cecilia'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unidecode(string.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HcoeEmpC9Kbc"
   },
   "source": [
    "## Tokeniza√ß√£o\n",
    "\n",
    "Tokeniza√ß√£o √© um processo onde transformamos um texto de uma string √∫nica em fragmentos desse texto na forma de *tokens*, que nada mais s√£o do que as pr√≥prias palavras! Para isso, vamos utilizar a fun√ß√£o *word_tokenize* do NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "ucLVo_6j9L9x"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Python', '√©', 'legal', 'e', 'python', '√©', 'divertido']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Python', '√©', 'legal', 'e', 'python', '', '', '', '√©', 'divertido']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = 'Python √© legal e python √© divertido'\n",
    "print(word_tokenize(string))\n",
    "string.split(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4qed6o8G9Vib"
   },
   "source": [
    "## Normaliza√ß√£o de Textos\n",
    "\n",
    "**Normaliza√ß√£o de Textos (Text Normalization)** √© o procedimento que consiste em **padronizar** o texto, de modo a evitar que varia√ß√µes tornem os modelos demasiadamente complexos. Por exemplo: tratar singular/plural como a mesma coisa, ou ent√£o eliminar conjuga√ß√£o de verbos. Outras componentes comuns da normaliza√ß√£o s√£o a de eliminar palavras que n√£o agregam muito significado, ou palavras muito raras.\n",
    "\n",
    "Abaixo alguns exemplos de a√ß√µes de Text Normalization que podem ser aplicadas no pr√©-processamento de dados textuais:\n",
    "\n",
    "**Stemming** - Redu√ß√£o de tokens √† sua raiz invariante atrav√©s da **remo√ß√£o de prefixos ou sufixos**. Baseado em heur√≠stica<br>\n",
    "**Lemmatization** - Redu√ß√£o de tokens √† sua raiz invariante atrav√©s da **an√°lise lingu√≠stica do token**. Baseado em dicion√°rio l√©xico<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ieG5enJ79aUg"
   },
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "jA-tVw5S9RGm"
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import *\n",
    "from nltk.stem import RSLPStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "say\n",
      "write\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "words = ['saying', 'writing', 'running']\n",
    "for word in words:\n",
    "    print(stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corr\n",
      "corr\n"
     ]
    }
   ],
   "source": [
    "stemmer = RSLPStemmer()\n",
    "words_pt = ['correndo', 'corra']\n",
    "for word in words_pt:\n",
    "    print(stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m spacy download \"pt_core_news_sm\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"pt_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[', \"'\", 'correr', \"'\", ',', \"'\", 'corra', \"'\", '_']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(str([palavra for palavra in words_pt]))\n",
    "[token.lemma_ for token in doc if token.pos_ == 'NOUN']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parts-of-Speech (POS)\n",
    "\n",
    "https://www.guru99.com/pos-tagging-chunking-nltk.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/cecilia/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('learning', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('datascience', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('cool', 'JJ')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "words = ['learning', 'is', 'datascience', 'is', 'cool']\n",
    "pos_tag(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NASA', 'NNP'), ('awarded', 'VBD'), ('elon', 'NN'), ('musk', 'NN'), ('‚Äô', 'NNP'), ('s', 'VBZ'), ('SpaceX', 'NNP'), ('a', 'DT'), ('$', '$'), ('2.9', 'CD'), ('billion', 'CD'), ('contract', 'NN'), ('to', 'TO'), ('build', 'VB'), ('the', 'DT'), ('lunar', 'NN'), ('lander', 'NN'), ('.', '.')]\n",
      "(S\n",
      "  (ORGANIZATION NASA/NNP)\n",
      "  awarded/VBD\n",
      "  (PERSON Elon/NNP Musk/NNP)\n",
      "  ‚Äô/NNP\n",
      "  s/VBD\n",
      "  (ORGANIZATION SpaceX/NNP)\n",
      "  a/DT\n",
      "  $/$\n",
      "  2.9/CD\n",
      "  billion/CD\n",
      "  contract/NN\n",
      "  to/TO\n",
      "  build/VB\n",
      "  the/DT\n",
      "  lunar/NN\n",
      "  lander/NN\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "text = \"NASA awarded elon musk‚Äôs SpaceX a $2.9 billion contract to build the lunar lander.\"\n",
    "tokens = word_tokenize(text)\n",
    "print(pos_tag(tokens))\n",
    "print(nltk.ne_chunk(tag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YxH2uhI4A8mq"
   },
   "source": [
    "## Pipeline de Processamento de Textos\n",
    "\n",
    "Conhecendo todos os tipos de processamentos que podemos utilizar, uma forma √∫til e organizada para isso √© construirmos uma fun√ß√£i que receba o nosso dados originais e realizada todos os processamentos que queremos nos textos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S4qLqa46BBUi"
   },
   "source": [
    "Crie uma classe que seja capaz de:\n",
    "\n",
    "- Metodo para remover acentua√ß√£o\n",
    "- Metodo de remover digitos\n",
    "- Metodo de remover caracteres especiais\n",
    "- Metodo de normalizar o texto em caixa baixa\n",
    "- Metodo para criar os tokens\n",
    "- Metodo para filtrar stopwords\n",
    "- Metodo para pegar o stemming\n",
    "- Metodo para pegar o lemma\n",
    "- Metodo de pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testar o pipeline em diferentes strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "xOfCHWPENHWc"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I grew up (b. 1965) watching and loving the Th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When I put this movie in my DVD player, and sa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Why do people who do not know what a particula...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Even though I have great interest in Biblical ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Im a die hard Dads Army fan and nothing will e...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>This is the kind of picture John Lassiter woul...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>A MUST SEE! I saw WHIPPED at a press screening...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>NBC should be ashamed. I wouldn't allow my chi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>This movie is a clumsy mishmash of various gho...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>Formula movie about the illegitimate son of a ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label\n",
       "0     I grew up (b. 1965) watching and loving the Th...      0\n",
       "1     When I put this movie in my DVD player, and sa...      0\n",
       "2     Why do people who do not know what a particula...      0\n",
       "3     Even though I have great interest in Biblical ...      0\n",
       "4     Im a die hard Dads Army fan and nothing will e...      1\n",
       "...                                                 ...    ...\n",
       "4995  This is the kind of picture John Lassiter woul...      1\n",
       "4996  A MUST SEE! I saw WHIPPED at a press screening...      1\n",
       "4997  NBC should be ashamed. I wouldn't allow my chi...      0\n",
       "4998  This movie is a clumsy mishmash of various gho...      0\n",
       "4999  Formula movie about the illegitimate son of a ...      0\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies = pd.read_csv('movies.csv', index_col=0)\n",
    "movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_sample = movies.sample(frac=0.1, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = PreProcesssPhrase()\n",
    "pipeline = [\n",
    "    'remove_digits',\n",
    "    'remove_special_char',\n",
    "    'word_lower',\n",
    "    'tokenizer',\n",
    "    'remove_stopwords',\n",
    "    'stemmer'\n",
    "]\n",
    "movies_sample[\"filtered_words\"] = movies_sample['text'].apply(preprocess.pipeline, methods=pipeline)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Aula 1 - NLP - Aux.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
