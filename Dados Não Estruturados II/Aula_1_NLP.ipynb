{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J2HFkxmHfd_R"
   },
   "source": [
    "## Aula 1 - Processamento de Linguagem Natual\n",
    "\n",
    "Na aula de hoje, vamos explorar os seguintes tópicos em Python:\n",
    "\n",
    "1) Dados Estruturados e Não Estruturados.  \n",
    "2) Introdução a NLP.  \n",
    "3) Processamento de Textos.  \n",
    "4) Exercícios.  \n",
    "\n",
    "<img src=\"https://i1.wp.com/thedatascientist.com/wp-content/uploads/2018/09/data_science_wordcloud.png?fit=1584%2C1008&ssl=1\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hgvh29EJh6UK"
   },
   "source": [
    "Primeiramente, precisamos entender qual a diferença enre as duas fontes de dados mais comuns, sendo elas dados **estruturados** e **não estruturados**. Definimos ele como:\n",
    "<br><br>\n",
    "- **Dados Estruturados:** São dados que seguem uma estrutura mais rígida com um padrão fixo e constante. Por exemplo: Tabelas e DataFrames;<br><br>\n",
    "- **Dados Não estruturados:** Como já diz o nome, são dados que não tem uma estrutura bem estabelecida e necessitam de um processamento adicional para trabalharmos com eles. Exemplos: áudios, vídeos, textos e etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whQI1eePiJwx"
   },
   "source": [
    "### Introdução ao Processamento de Linguagem Natural (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bwjo0xx9h-Pi"
   },
   "source": [
    "O Processamento de Linguagem Natural, mas conhecido como NLP, é a abordagem onde trabalhamos com **dados não estruturados** do tipo **Texto**. O objetivo de trabalharmos com textos é extrair de informação e teor linguístico das nossas bases de textos e converter isso de uma forma númerica, onde poderemos utilizar em nossos modelos de *Machine Learning*.<br><br>\n",
    "Temos como exemplos de aplicações de NLP como:\n",
    "- Análise de Sentimentos em review de filmes e produtos ou mensagens em redes sociais;\n",
    "- Filtro de E-Mails Spams e Não Spams;\n",
    "- Identificação de textos a partir de construções linguísticas (descobrir se um texto foi escrito ou não por Machado de assis);\n",
    "- Tradutores de Idiomas;\n",
    "- ChatBots;\n",
    "- Corretores Ortográficos;\n",
    "- Classificação de textos de acordo com o conteúdo do texto (Esportes, Política, Economia e etc).\n",
    "<br><br>\n",
    "Nesta aula iremos aprender a partir dos nossos dados textuais a como processar, tratar e transformar os dados de uma maneira que os modelos de *Machine Learning* entendam.<br><br>\n",
    "\n",
    "A principal biblioteca de referência para NLP chama-se [NLTK - Natural Language Toll Kit](https://www.nltk.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IjXHV5QOjWDc"
   },
   "source": [
    "### Processamento de Textos\n",
    "\n",
    "Antes de mais nada, precisamos filtrar e tratar os nossos textos, de forma a deixar apenas o conteúdo de mais relevantes para a nossa análise. Existem alguns processos importantes para trabalhar com os textos (não necessariamente você precisa aplicar todos os procesos)\n",
    "\n",
    "- Remoção de Stopwords;\n",
    "- Limpeza de Textos;\n",
    "- Tokenização;\n",
    "- Normalização do Texto.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9je2oXCjjrwc"
   },
   "source": [
    "### Stopwords\n",
    "\n",
    "Stopwords são palavras que aparecem com uma frequência muito alta nos textos, mas que não trazem um teor de conteúdo relevante para o nosso modelo. Vamos entender isso na prática:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3IR-pN2DjwXJ",
    "outputId": "9c8b59ea-653b-457e-b82e-d36bc3f07b67"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/cecilia/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/cecilia/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package rslp to /Users/cecilia/nltk_data...\n",
      "[nltk_data]   Package rslp is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')  # https://www.nltk.org/_modules/nltk/tokenize/punkt.html\n",
    "nltk.download('rslp')   # Stemmer em português"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jR2RHfCOn83i"
   },
   "source": [
    "Baixada a função de Stopwords, vamos definir um set de stopwords onde teremos uma lista com todas as stopwords em inglês já identificadas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "-t93Bq2FncDC"
   },
   "outputs": [],
   "source": [
    "stopwords_en = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LllQ0D8sol9G",
    "outputId": "0a61c196-e808-44da-90f1-76e584b5ce1a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ClhJwZvOoqI-",
    "outputId": "37956347-1525-4132-8e86-5f457bfe9f1d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(stopwords_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3OEsva3po-FN",
    "outputId": "3c1edc99-4044-4632-8f22-3bbe9d92deaa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "vcQKI7ZdolbV"
   },
   "outputs": [],
   "source": [
    "stopwords_port = nltk.corpus.stopwords.words('portuguese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-16r_klQolCC",
    "outputId": "2debcb43-b617-4c22-8a74-f045d415e88c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "207"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords_port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y6gP4YowoyYk",
    "outputId": "c957bddd-31db-4403-c6f0-cc154ecb74cf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "207"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(stopwords_port))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TyHAf5Uko0ZQ",
    "outputId": "8ffe9e50-8250-4686-88d6-93e5855775d6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'à',\n",
       " 'ao',\n",
       " 'aos',\n",
       " 'aquela',\n",
       " 'aquelas',\n",
       " 'aquele',\n",
       " 'aqueles',\n",
       " 'aquilo',\n",
       " 'as',\n",
       " 'às',\n",
       " 'até',\n",
       " 'com',\n",
       " 'como',\n",
       " 'da',\n",
       " 'das',\n",
       " 'de',\n",
       " 'dela',\n",
       " 'delas',\n",
       " 'dele',\n",
       " 'deles',\n",
       " 'depois',\n",
       " 'do',\n",
       " 'dos',\n",
       " 'e',\n",
       " 'é',\n",
       " 'ela',\n",
       " 'elas',\n",
       " 'ele',\n",
       " 'eles',\n",
       " 'em',\n",
       " 'entre',\n",
       " 'era',\n",
       " 'eram',\n",
       " 'éramos',\n",
       " 'essa',\n",
       " 'essas',\n",
       " 'esse',\n",
       " 'esses',\n",
       " 'esta',\n",
       " 'está',\n",
       " 'estamos',\n",
       " 'estão',\n",
       " 'estar',\n",
       " 'estas',\n",
       " 'estava',\n",
       " 'estavam',\n",
       " 'estávamos',\n",
       " 'este',\n",
       " 'esteja',\n",
       " 'estejam',\n",
       " 'estejamos',\n",
       " 'estes',\n",
       " 'esteve',\n",
       " 'estive',\n",
       " 'estivemos',\n",
       " 'estiver',\n",
       " 'estivera',\n",
       " 'estiveram',\n",
       " 'estivéramos',\n",
       " 'estiverem',\n",
       " 'estivermos',\n",
       " 'estivesse',\n",
       " 'estivessem',\n",
       " 'estivéssemos',\n",
       " 'estou',\n",
       " 'eu',\n",
       " 'foi',\n",
       " 'fomos',\n",
       " 'for',\n",
       " 'fora',\n",
       " 'foram',\n",
       " 'fôramos',\n",
       " 'forem',\n",
       " 'formos',\n",
       " 'fosse',\n",
       " 'fossem',\n",
       " 'fôssemos',\n",
       " 'fui',\n",
       " 'há',\n",
       " 'haja',\n",
       " 'hajam',\n",
       " 'hajamos',\n",
       " 'hão',\n",
       " 'havemos',\n",
       " 'haver',\n",
       " 'hei',\n",
       " 'houve',\n",
       " 'houvemos',\n",
       " 'houver',\n",
       " 'houvera',\n",
       " 'houverá',\n",
       " 'houveram',\n",
       " 'houvéramos',\n",
       " 'houverão',\n",
       " 'houverei',\n",
       " 'houverem',\n",
       " 'houveremos',\n",
       " 'houveria',\n",
       " 'houveriam',\n",
       " 'houveríamos',\n",
       " 'houvermos',\n",
       " 'houvesse',\n",
       " 'houvessem',\n",
       " 'houvéssemos',\n",
       " 'isso',\n",
       " 'isto',\n",
       " 'já',\n",
       " 'lhe',\n",
       " 'lhes',\n",
       " 'mais',\n",
       " 'mas',\n",
       " 'me',\n",
       " 'mesmo',\n",
       " 'meu',\n",
       " 'meus',\n",
       " 'minha',\n",
       " 'minhas',\n",
       " 'muito',\n",
       " 'na',\n",
       " 'não',\n",
       " 'nas',\n",
       " 'nem',\n",
       " 'no',\n",
       " 'nos',\n",
       " 'nós',\n",
       " 'nossa',\n",
       " 'nossas',\n",
       " 'nosso',\n",
       " 'nossos',\n",
       " 'num',\n",
       " 'numa',\n",
       " 'o',\n",
       " 'os',\n",
       " 'ou',\n",
       " 'para',\n",
       " 'pela',\n",
       " 'pelas',\n",
       " 'pelo',\n",
       " 'pelos',\n",
       " 'por',\n",
       " 'qual',\n",
       " 'quando',\n",
       " 'que',\n",
       " 'quem',\n",
       " 'são',\n",
       " 'se',\n",
       " 'seja',\n",
       " 'sejam',\n",
       " 'sejamos',\n",
       " 'sem',\n",
       " 'ser',\n",
       " 'será',\n",
       " 'serão',\n",
       " 'serei',\n",
       " 'seremos',\n",
       " 'seria',\n",
       " 'seriam',\n",
       " 'seríamos',\n",
       " 'seu',\n",
       " 'seus',\n",
       " 'só',\n",
       " 'somos',\n",
       " 'sou',\n",
       " 'sua',\n",
       " 'suas',\n",
       " 'também',\n",
       " 'te',\n",
       " 'tem',\n",
       " 'tém',\n",
       " 'temos',\n",
       " 'tenha',\n",
       " 'tenham',\n",
       " 'tenhamos',\n",
       " 'tenho',\n",
       " 'terá',\n",
       " 'terão',\n",
       " 'terei',\n",
       " 'teremos',\n",
       " 'teria',\n",
       " 'teriam',\n",
       " 'teríamos',\n",
       " 'teu',\n",
       " 'teus',\n",
       " 'teve',\n",
       " 'tinha',\n",
       " 'tinham',\n",
       " 'tínhamos',\n",
       " 'tive',\n",
       " 'tivemos',\n",
       " 'tiver',\n",
       " 'tivera',\n",
       " 'tiveram',\n",
       " 'tivéramos',\n",
       " 'tiverem',\n",
       " 'tivermos',\n",
       " 'tivesse',\n",
       " 'tivessem',\n",
       " 'tivéssemos',\n",
       " 'tu',\n",
       " 'tua',\n",
       " 'tuas',\n",
       " 'um',\n",
       " 'uma',\n",
       " 'você',\n",
       " 'vocês',\n",
       " 'vos']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords_port"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A9Dpg-8ApIxa"
   },
   "source": [
    "Vamos agora aplicar a remoção de Stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "CvrcEJsQo_tf"
   },
   "outputs": [],
   "source": [
    "example = [\"my\", \"house\", \"is\", \"black\", \"and\", \"white\", \"but\", \"isn't\", \"big\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tUaavlQhtf8g",
    "outputId": "ae132438-d8f4-43bc-9180-1f4904656d34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print('my' in stopwords_en)\n",
    "print('house' in stopwords_en)\n",
    "print('is' in stopwords_en)\n",
    "print('black' in stopwords_en)\n",
    "print('and' in stopwords_en)\n",
    "print('white' in stopwords_en)\n",
    "print('but' in stopwords_en)\n",
    "print(\"isn't\" in stopwords_en)\n",
    "print(\"big\" in stopwords_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wnYz481KOjkM"
   },
   "source": [
    "### Limpeza do Texto\n",
    "\n",
    "Existem alguns cuidados com relação a grafia das palavras e elementos em um texto que devemos tomar bastante cuidado antes de fazer qualquer outra coisa. Esses pontos são:<br><br>\n",
    "- Transformar todas as palavras para MAIÚSCULAS ou minúsculas;\n",
    "- Remover caracteres especiais;\n",
    "- Remover dígitos (quando não forem relevantes);\n",
    "- Remover acentuação (caso típico de quando trabalhamos com textos em Português);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "farmacêutico\n",
      "relogio\n",
      "parada\n"
     ]
    }
   ],
   "source": [
    "words = ['FaRMacêuTIco', 'relogio', 'Parada']\n",
    "for word in words:\n",
    "    print(word.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HYRTjdWiTKWu"
   },
   "source": [
    "### Remoção de dígitos, caracteres especiais e qualquer outro item que não queremos no texto\n",
    "\n",
    "Para essa etapa do processo, iremos utilizar uma biblioteca auxiliar [RegEx (Regular Expression)](https://docs.python.org/3/library/re.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "iE4HC6guTNGR"
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2n2D1Od-TVVx"
   },
   "source": [
    "Importada a biblioteca, vamos utilizar a função *re.sub*, para substituir os elementos que não queremos nos nossos textos:\n",
    "\n",
    "**Procure por re.sub**  \n",
    "https://docs.python.org/3/library/re.html\n",
    "\n",
    "Obs: Utilize o site https://regex101.com/ para criar o regex e o re.sub para substituir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto Original:  São mais de 50 cursos em Data Science!!! #datascience #machinelearning\n",
      "Texto sem números São mais de  cursos em Data Science!!! #datascience #machinelearning\n"
     ]
    }
   ],
   "source": [
    "frase = 'São mais de 50 cursos em Data Science!!! #datascience #machinelearning'\n",
    "\n",
    "# Mostra o texto original\n",
    "print('Texto Original: ', frase)\n",
    "\n",
    "# Removendo números\n",
    "frase = re.sub(r'\\d', '', frase)\n",
    "print('Texto sem números', frase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto sem os caracteres especiais:  S o mais de cursos em Data Science datascience machinelearning\n"
     ]
    }
   ],
   "source": [
    "# Texto agora sem caracteres especiais também\n",
    "frase = re.sub(r\"[^a-zA-Z]+\", ' ', frase)\n",
    "\n",
    "# Mostra o texto modificado\n",
    "print('Texto sem os caracteres especiais: ', frase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zuqfqkjuabuH"
   },
   "source": [
    "Utilizem a documentação para descobrir mais códigos para filtrar elementos ou mesmo deem uma olhada nesse artigo, que resume de uma forma bem visual as aplicações do RegEx: [clique aqui](https://amitness.com/regex/)\n",
    "\n",
    "Tutorial de regex https://blog.geekhunter.com.br/python-regex/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HeoTPv_4agE2"
   },
   "source": [
    "Hoje os emojis fazem parte da comunicação via mensagens, por isso iremos ver como utilizar frases contento emojis e trata-los de forma adequada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "fCZwZhJLaTnj"
   },
   "outputs": [],
   "source": [
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vb4EGxBSatqv",
    "outputId": "975e4fba-e585-4cdd-ba61-c821277f8464"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python is 👍\n"
     ]
    }
   ],
   "source": [
    "texto = 'Python is :thumbs_up:'\n",
    "print(emoji.emojize(texto))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "AcRH6cAZbONN",
    "outputId": "d94993a9-948f-4f52-c001-4cfea0fa8d71"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Python is :thumbs_up:'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texto_com_emoji = 'Python is 👍'\n",
    "emoji.demojize(texto_com_emoji)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c9HCV1QKbjXG"
   },
   "source": [
    "Para ver a lista completa:\n",
    "\n",
    "https://www.webfx.com/tools/emoji-cheat-sheet/\n",
    "\n",
    "Link biblioteca: https://github.com/carpedm20/emoji/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 1\n",
    "Utilizar Regez para validar CPFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = r\"^(\\d{3}.){2}\\d{3}-\\d{2}$\"\n",
    "bool(re.match(pattern, '097.021.074-40'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZap5orRdLUE"
   },
   "source": [
    "### Exercício 2\n",
    "\n",
    "Utilizar Regex para validação de emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email = 'cflavs.7@gmail.com'\n",
    "pattern = r\"\\\"?([-a-zA-Z0-9.`?{}]+@\\w+\\.\\w+)\\\"?\"  \n",
    "patterns = re.compile(pattern)\n",
    "bool(re.match(patterns, email))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "laoYA1wYg6Yj"
   },
   "source": [
    "Podemos utilizar NLP para identificar frases que contenham uma palavra chave!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['People are fighting with covid these days', ' How will we survive covid']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example=\"People are fighting with covid these days. Economy has fallen down. How will we survive covid\" \n",
    "keyword = 'covid'\n",
    "line=re.findall(r'([^.]*'+keyword+'[^.]*)', example)\n",
    "line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K5f4aJfl7QCC"
   },
   "source": [
    "### Remoção de Acentuação\n",
    "Para a remoção de acentuação, iremos utilizar uma bibloteca chamada *Unidecode*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a7hpJwTa7SIY",
    "outputId": "066869e7-ca3f-45b3-9b60-c6598f820639"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: unidecode in /usr/local/lib/python3.7/dist-packages (1.3.4)\n"
     ]
    }
   ],
   "source": [
    "# Caso precise instalar a biblioteca, descomente o código abaixo\n",
    "!pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8eLSKg7A7TS5"
   },
   "outputs": [],
   "source": [
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GFxZKdkJ7VHI",
    "outputId": "49354abb-e88d-4efc-fded-37da7d157852"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "João SeBastião Alvará Vovô Linguinça Expressão\n",
      "Joao SeBastiao Alvara Vovo Linguinca Expressao\n"
     ]
    }
   ],
   "source": [
    "string = 'João SeBastião Alvará Vovô Linguinça Expressão'\n",
    "print(string)\n",
    "\n",
    "string = unidecode(string)\n",
    "print(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HcoeEmpC9Kbc"
   },
   "source": [
    "## Tokenização\n",
    "\n",
    "Tokenização é um processo onde transformamos um texto de uma string única em fragmentos desse texto na forma de *tokens*, que nada mais são do que as próprias palavras! Para isso, vamos utilizar a função *word_tokenize* do NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ucLVo_6j9L9x"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BOwuFw8Q9O2s",
    "outputId": "f0a813b6-fd08-44bd-c3e8-fb278a683f2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A frase original é O rato roeu a roupa do rei de roma.\n",
      "Os tokens são ['O', 'rato', 'roeu', 'a', 'roupa', 'do', 'rei', 'de', 'roma', '.']\n",
      "Com split ['O', 'rato', 'roeu', 'a', 'roupa', 'do', 'rei', 'de', 'roma.']\n"
     ]
    }
   ],
   "source": [
    "string = 'O rato roeu a roupa do rei de roma.'\n",
    "words = word_tokenize(string)\n",
    "\n",
    "print('A frase original é', string)\n",
    "print('Os tokens são', words)\n",
    "print('Com split', string.split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4qed6o8G9Vib"
   },
   "source": [
    "## Normalização de Textos\n",
    "\n",
    "**Normalização de Textos (Text Normalization)** é o procedimento que consiste em **padronizar** o texto, de modo a evitar que variações tornem os modelos demasiadamente complexos. Por exemplo: tratar singular/plural como a mesma coisa, ou então eliminar conjugação de verbos. Outras componentes comuns da normalização são a de eliminar palavras que não agregam muito significado, ou palavras muito raras.\n",
    "\n",
    "Abaixo alguns exemplos de ações de Text Normalization que podem ser aplicadas no pré-processamento de dados textuais:\n",
    "\n",
    "**Stemming** - Redução de tokens à sua raiz invariante através da **remoção de prefixos ou sufixos**. Baseado em heurística<br>\n",
    "**Lemmatization** - Redução de tokens à sua raiz invariante através da **análise linguística do token**. Baseado em dicionário léxico<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ieG5enJ79aUg"
   },
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "jA-tVw5S9RGm"
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import *\n",
    "from nltk.stem import RSLPStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZSZhPhGP9cqF",
    "outputId": "7781f8e5-82b0-4df6-a32f-45558f9c6ee7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "say\n",
      "write\n",
      "run\n",
      "ate\n",
      "work\n"
     ]
    }
   ],
   "source": [
    "# Inicializando o Porter Stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Criando uma lista de palavras\n",
    "words = ['saying', 'writing', 'running', 'ate', 'worked']\n",
    "\n",
    "# Criando uma lista vazia para armazenar as palavras stem\n",
    "stem_words = []\n",
    "\n",
    "# Percorrendo a lista de palavras\n",
    "for w in words:\n",
    "    print(stemmer.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jf0ggw2I9qFx",
    "outputId": "403ce0b5-cf64-4c3a-851a-e470cd236760"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corr\n",
      "corr\n"
     ]
    }
   ],
   "source": [
    "# Criando uma lista de palavras em português\n",
    "words_port = ['correndo', 'corra']\n",
    "\n",
    "# Inicializando o Porter Stem para português (RSLPStemmer)\n",
    "stemmer_port = RSLPStemmer()\n",
    "\n",
    "# Percorrendo a lista de palavras\n",
    "for w in words_port:\n",
    "    print(stemmer_port.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m spacy download \"pt_core_news_sm\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"pt_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(str([palavra for palavra in words_port]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['corra']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.lemma_ for token in doc if token.pos_ == 'NOUN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/cecilia/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/cecilia/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importando o Lemmatizer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "# https://www.nltk.org/howto/wordnet.html\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# NLTK 3.6.6 release: December 2021:\n",
    "# support OMW 1.4, use Multilingual Wordnet Data from OMW with newer Wordnet versions\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializando o Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JaqVvMR-_cfF",
    "outputId": "8cde4c03-ab37-4e2a-f98e-4c12b0fecd91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rocks : rock\n",
      "corpora : corpus\n",
      "running : run\n",
      "went : go\n"
     ]
    }
   ],
   "source": [
    "# Exemplos simples\n",
    "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\")) \n",
    "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\")) \n",
    "\n",
    "# argumento \"pos\" indica a qual classe gramatical o token pertence\n",
    "print(\"running :\", lemmatizer.lemmatize(\"running\", pos = \"v\")) \n",
    "print(\"went :\", lemmatizer.lemmatize(\"went\", pos = 'v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS - Parts of Speech\n",
    "\n",
    " Part-of_Speech Tagging é uma ferramenta auxiliar de alguns modelos mais complexos de NLP, que ajuda na identificação dos elementos gramaticais dentro de uma frase. Ou seja, o pos_tag identifica dentro de um texto quais elementos são verbos, sujeitos, pronomes e assim por diante. A Forma de aplicar o Part-of_Speech Tagging será utilizando a função pos_tag do NLTK\n",
    " \n",
    " https://www.guru99.com/pos-tagging-chunking-nltk.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palavras:  ['learning', 'data', 'science', 'is', 'one', 'of', 'the', 'best', 'things']\n",
      "POS Tagging:  [('learning', 'VBG'), ('data', 'NNS'), ('science', 'NN'), ('is', 'VBZ'), ('one', 'CD'), ('of', 'IN'), ('the', 'DT'), ('best', 'JJS'), ('things', 'NNS')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/cecilia/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Baixa um estimador de tags\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Cria uma frase em tokens como exemplo\n",
    "words = ['learning', 'data', 'science', 'is', 'one', 'of', 'the', 'best', 'things']\n",
    "\n",
    "# Cria as tags\n",
    "tagged = pos_tag(words)\n",
    "\n",
    "# Mostra os resultados\n",
    "print('Palavras: ', words)\n",
    "print('POS Tagging: ', tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition (NER)\n",
    "\n",
    "- Identifica nomes de entidades em textos (pessoas, locais, organizações)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NASA', 'NNP'), ('awarded', 'VBD'), ('Elon', 'NNP'), ('Musk', 'NNP'), ('’', 'NNP'), ('s', 'VBD'), ('SpaceX', 'NNP'), ('a', 'DT'), ('$', '$'), ('2.9', 'CD'), ('billion', 'CD'), ('contract', 'NN'), ('to', 'TO'), ('build', 'VB'), ('the', 'DT'), ('lunar', 'NN'), ('lander', 'NN'), ('.', '.')]\n",
      "(S\n",
      "  (ORGANIZATION NASA/NNP)\n",
      "  awarded/VBD\n",
      "  (PERSON Elon/NNP Musk/NNP)\n",
      "  ’/NNP\n",
      "  s/VBD\n",
      "  (ORGANIZATION SpaceX/NNP)\n",
      "  a/DT\n",
      "  $/$\n",
      "  2.9/CD\n",
      "  billion/CD\n",
      "  contract/NN\n",
      "  to/TO\n",
      "  build/VB\n",
      "  the/DT\n",
      "  lunar/NN\n",
      "  lander/NN\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "text = \"NASA awarded Elon Musk’s SpaceX a $2.9 billion contract to build the lunar lander.\"\n",
    "tokens = word_tokenize(text)\n",
    "tag=nltk.pos_tag(tokens)\n",
    "print(tag)\n",
    "\n",
    "ne_tree = nltk.ne_chunk(tag)\n",
    "print(ne_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YxH2uhI4A8mq"
   },
   "source": [
    "## Pipeline de Processamento de Textos\n",
    "\n",
    "Conhecendo todos os tipos de processamentos que podemos utilizar, uma forma útil e organizada para isso é construirmos uma funçãi que receba o nosso dados originais e realizada todos os processamentos que queremos nos textos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S4qLqa46BBUi"
   },
   "source": [
    "### Exercício\n",
    "\n",
    "Crie uma classe que seja capaz de:\n",
    "\n",
    "- Metodo para remover acentuação\n",
    "- Metodo de remover digitos\n",
    "- Metodo de remover caracteres especiais\n",
    "- Metodo de normalizar o texto em caixa baixa\n",
    "- Metodo para criar os tokens\n",
    "- Metodo para filtrar stopwords\n",
    "- Metodo para pegar o stemming\n",
    "- Metodo para pegar o lemma\n",
    "- Metodo pipeline responsável por chamar os preprocessamentos necessários\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcesssPhrase:\n",
    "\n",
    "  def remove_acentuacao(self, phrase: str, debug: bool = False) -> str:\n",
    "    # Utilizando a biblioteca unidecode para remover acentuação de texto\n",
    "    phrase_fmt = unidecode(phrase)\n",
    "    if debug:\n",
    "      print('`remove_acentuacao`: Frase original', phrase)\n",
    "      print('`remove_acentuacao`: Frase formatada', phrase_fmt)\n",
    "\n",
    "    # Retornando a frase formatada\n",
    "    return phrase_fmt\n",
    "\n",
    "\n",
    "  def remove_digits(self, phrase: str, debug: bool = False) -> str:\n",
    "    # utilizando expressões regulares para remoção de digitos\n",
    "    phrase_no_digits = re.sub(r'\\d', '', phrase)\n",
    "\n",
    "    # Se quisermos ligar o debug, mostre a frase original e transformada\n",
    "    if debug:\n",
    "      print('`remove_digits`: Texto original:', phrase)\n",
    "      print('`remove_digits`: Texto sem digitos:', phrase_no_digits)\n",
    "\n",
    "    # Retornando a frase sem digitos\n",
    "    return phrase_no_digits\n",
    "\n",
    "  def remove_special_char(self, phrase: str, debug: bool = False) -> str:\n",
    "    # utilizando expressões regulares para remoção de caracteres especiais\n",
    "    phrase_no_special_char = re.sub(r'[^a-zA-Z0-9]+', ' ', phrase)\n",
    "\n",
    "    # Se quisermos ligar o debug, mostre a frase original e transformada\n",
    "    if debug:\n",
    "      print('`remove_special_char`: Texto original:', phrase)\n",
    "      print('`remove_special_char`: Texto sem caracteres especiais:', phrase_no_special_char)\n",
    "\n",
    "    # Retornando a frase sem digitos\n",
    "    return phrase_no_special_char\n",
    "\n",
    "  def word_lower(self, word: str, debug: bool = False) -> str:\n",
    "    try:\n",
    "      # Formatando a palavra em caixa baixa\n",
    "      word_fmt = word.lower()\n",
    "\n",
    "      # Se o debug for True, iremos imprimir a palavra original e transformada\n",
    "      if debug:\n",
    "        print('`word_lower`: Palavra Original:', word)\n",
    "        print('`word_lower`: Palavra transformada:', word_fmt)\n",
    "\n",
    "    except:\n",
    "      # Caso a palavra não seja uma string levante um erro (TypeError) informando qual o tipo da palavra passada\n",
    "      raise TypeError(f'Esperava uma `word` no tipo str, foi passado uma {type(word)}')\n",
    "\n",
    "    # Retornando a palavra formatada\n",
    "    return word_fmt\n",
    "\n",
    "  def remove_stopwords(self, words: list[str], debug=False) -> list[str]:\n",
    "    # Carregando as stopwords (inglês)\n",
    "    stopwords_en = stopwords.words('english')\n",
    "    # Criando uma váriavel que irá armazenar elementos limpos, que não estejam dentro das stopwords\n",
    "    clean_words = []\n",
    "\n",
    "    # Percorrendo cada palavra da nossa lista de palavras\n",
    "    for word in words:\n",
    "      # Verificando se a palavra não está presente das stopwords\n",
    "      if word not in stopwords_en:\n",
    "        # Se a palavra não é uma stopword adicionamos elas a váriavel clean_words\n",
    "        clean_words.append(word)\n",
    "      else:\n",
    "        # Caso a palavra seja uma stopword e estamos no modo de debug (debug=True)\n",
    "        if debug:\n",
    "          # Imprimimos qual a palavra da lista words é uma stopword\n",
    "          print(f'`remove_stopwords`: A palavra {word} está presente nas stopwords')\n",
    "    return clean_words\n",
    "\n",
    "  def tokenizer(self, phrase: str, debug: bool) -> list[str]:\n",
    "    words = word_tokenize(phrase)\n",
    "    if debug:\n",
    "        print('`tokenizer`: Frase original:', phrase)\n",
    "        print('`tokenizer`: tokens:', words)\n",
    "    return words\n",
    "\n",
    "  def stemmer(self, words: list[str], debug: bool = False) -> list[str]:\n",
    "    # Inicializando o Porter Stemmer (inglês)\n",
    "    stemmer = PorterStemmer()\n",
    "    # Criando uma lista vazia para armazenar as palavras stem\n",
    "    stem_words = []\n",
    "    for word in words:\n",
    "      # Pegando o stem de cada palavra\n",
    "      s_word = stemmer.stem(word)\n",
    "      # Adicionando essa palavra modificada a lista stem_words\n",
    "      stem_words.append(s_word)\n",
    "\n",
    "    if debug:\n",
    "        print('`stemmer`: Tokens originais:', words)\n",
    "        print('`stemmer`: Tokens transformadods:', stem_words)\n",
    "    return stem_words\n",
    "\n",
    "  def lemmatizer(self, words: list[str], debug: bool = False) -> list[str]:\n",
    "    # Inicializando o WordNetLemmatizer (inglês)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # Criando uma lista vazia para armazenar as palavras lemma\n",
    "    lemm_words = []\n",
    "    for word in words:\n",
    "      # Pegando o lemma de cada palavra\n",
    "      l_words = lemmatizer.lemmatize(word, pos='v')\n",
    "      # Adicionando essa palavra modificada (lemma) a lista lemm_words\n",
    "      lemm_words.append(l_words)\n",
    "\n",
    "    if debug:\n",
    "        print('`stemmer`: Tokens originais:', words)\n",
    "        print('`stemmer`: Tokens transformadods:', lemm_words)\n",
    "    return stem_words\n",
    "\n",
    "\n",
    "  def pipeline(self, phrase: str, methods: list[str], debug: bool = False):\n",
    "    switcher = {\n",
    "        'remove_acentuacao': self.remove_acentuacao,\n",
    "        'remove_digits': self.remove_digits,\n",
    "        'remove_special_char': self.remove_special_char,\n",
    "        'word_lower': self.word_lower,\n",
    "        'remove_stopwords': self.remove_stopwords,\n",
    "        'tokenizer': self.tokenizer,\n",
    "        'stemmer': self.stemmer,\n",
    "        'lemmatizer': self.lemmatizer\n",
    "    }\n",
    "    for method in methods:\n",
    "      # remove_stopwords\n",
    "      if method == 'remove_stopwords':\n",
    "        phrase = switcher[method](phrase, debug=debug)\n",
    "        \n",
    "      else:\n",
    "        phrase = switcher[method](phrase, debug=debug)\n",
    "\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "brOZjdj3Cp96"
   },
   "outputs": [],
   "source": [
    "phrase = 'I grew up (b. 1965) watching and loving ç'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "CsFClqwMHbvb"
   },
   "outputs": [],
   "source": [
    "preprocess = PreProcesssPhrase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "dDWoIbbJHfKz",
    "outputId": "fab1e06c-4e69-4561-b041-53ba794f3845"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`remove_digits`: Texto original: I grew up (b. 1965) watching and loving ç\n",
      "`remove_digits`: Texto sem digitos: I grew up (b. ) watching and loving ç\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I grew up (b. ) watching and loving ç'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'remove_acentuacao'\n",
    "# 'remove_digits'\n",
    "# 'remove_special_char'\n",
    "# 'word_lower'\n",
    "# 'remove_stopwords'\n",
    "# 'tokenizer'\n",
    "# 'stemmer'\n",
    "# 'lemmatizer'\n",
    "pipeline = ['remove_digits']\n",
    "# Removendo apenas os digitos com o pipeline\n",
    "preprocess.pipeline(phrase, methods=pipeline, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "id": "eq1dJuERHw7_",
    "outputId": "103000a9-0b0f-4823-b71f-cca8bfedadb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`remove_digits`: Texto original: I grew up (b. 1965) watching and loving ç\n",
      "`remove_digits`: Texto sem digitos: I grew up (b. ) watching and loving ç\n",
      "`remove_special_char`: Texto original: I grew up (b. ) watching and loving ç\n",
      "`remove_special_char`: Texto sem caracteres especiais: I grew up b watching and loving \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I grew up b watching and loving '"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = ['remove_digits', \"remove_special_char\"]\n",
    "# Removendo digitos e caracteres especiais com o pipeline\n",
    "preprocess.pipeline(phrase, methods=pipeline, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VkU2HCMLILSn",
    "outputId": "229fe71b-ccf3-4a9d-a3ce-0a44b2170280"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`remove_digits`: Texto original: I grew up (b. 1965) watching and loving ç\n",
      "`remove_digits`: Texto sem digitos: I grew up (b. ) watching and loving ç\n",
      "`tokenizer`: Frase original: I grew up (b. ) watching and loving ç\n",
      "`tokenizer`: tokens: ['I', 'grew', 'up', '(', 'b', '.', ')', 'watching', 'and', 'loving', 'ç']\n",
      "`remove_stopwords`: A palavra up está presente nas stopwords\n",
      "`remove_stopwords`: A palavra and está presente nas stopwords\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['I', 'grew', '(', 'b', '.', ')', 'watching', 'loving', 'ç']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = ['remove_digits', 'tokenizer', \"remove_stopwords\"]\n",
    "# Removendo digitos, criando tokens e removendo stopwords com o pipeline\n",
    "preprocess.pipeline(phrase, methods=pipeline, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MhKiVPx5IiLf",
    "outputId": "b43b381c-81dd-4a19-d42e-525d5cc399bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`remove_digits`: Texto original: I grew up (b. 1965) watching and loving ç\n",
      "`remove_digits`: Texto sem digitos: I grew up (b. ) watching and loving ç\n",
      "`remove_special_char`: Texto original: I grew up (b. ) watching and loving ç\n",
      "`remove_special_char`: Texto sem caracteres especiais: I grew up b watching and loving \n",
      "`word_lower`: Palavra Original: I grew up b watching and loving \n",
      "`word_lower`: Palavra transformada: i grew up b watching and loving \n",
      "`tokenizer`: Frase original: i grew up b watching and loving \n",
      "`tokenizer`: tokens: ['i', 'grew', 'up', 'b', 'watching', 'and', 'loving']\n",
      "`remove_stopwords`: A palavra i está presente nas stopwords\n",
      "`remove_stopwords`: A palavra up está presente nas stopwords\n",
      "`remove_stopwords`: A palavra and está presente nas stopwords\n",
      "`stemmer`: Tokens originais: ['grew', 'b', 'watching', 'loving']\n",
      "`stemmer`: Tokens transformadods: ['grew', 'b', 'watch', 'love']\n"
     ]
    }
   ],
   "source": [
    "# 'remove_acentuacao'\n",
    "# 'remove_digits'\n",
    "# 'remove_special_char'\n",
    "# 'word_lower'\n",
    "# 'remove_stopwords'\n",
    "# 'tokenizer'\n",
    "# 'stemmer'\n",
    "# 'lemmatizer'\n",
    "pipeline = [\n",
    "    'remove_digits', 'remove_special_char', 'word_lower', 'tokenizer',\n",
    "    'remove_stopwords', 'stemmer'\n",
    "]\n",
    "phrase_proc = preprocess.pipeline(phrase, methods=pipeline, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5PACJIBDJJ3O",
    "outputId": "f13b35aa-5308-4f93-d453-30a144cf4167"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['grew', 'b', 'watch', 'love']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase_proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "xOfCHWPENHWc"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I grew up (b. 1965) watching and loving the Th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When I put this movie in my DVD player, and sa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Why do people who do not know what a particula...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Even though I have great interest in Biblical ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Im a die hard Dads Army fan and nothing will e...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>This is the kind of picture John Lassiter woul...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>A MUST SEE! I saw WHIPPED at a press screening...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>NBC should be ashamed. I wouldn't allow my chi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>This movie is a clumsy mishmash of various gho...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>Formula movie about the illegitimate son of a ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label\n",
       "0     I grew up (b. 1965) watching and loving the Th...      0\n",
       "1     When I put this movie in my DVD player, and sa...      0\n",
       "2     Why do people who do not know what a particula...      0\n",
       "3     Even though I have great interest in Biblical ...      0\n",
       "4     Im a die hard Dads Army fan and nothing will e...      1\n",
       "...                                                 ...    ...\n",
       "4995  This is the kind of picture John Lassiter woul...      1\n",
       "4996  A MUST SEE! I saw WHIPPED at a press screening...      1\n",
       "4997  NBC should be ashamed. I wouldn't allow my chi...      0\n",
       "4998  This movie is a clumsy mishmash of various gho...      0\n",
       "4999  Formula movie about the illegitimate son of a ...      0\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies = pd.read_csv('movies.csv', index_col=0)\n",
    "movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_sample = movies.sample(frac=0.1, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = PreProcesssPhrase()\n",
    "pipeline = [\n",
    "    'remove_digits',\n",
    "    'remove_special_char',\n",
    "    'word_lower',\n",
    "    'tokenizer',\n",
    "    'remove_stopwords',\n",
    "    'stemmer'\n",
    "]\n",
    "movies_sample[\"filtered_words\"] = movies_sample['text'].apply(preprocess.pipeline, methods=pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32214    [movi, good, concept, execut, live, br, br, co...\n",
       "21823    [film, aw, screenplay, bad, script, mediocr, e...\n",
       "35693    [ever, tri, kind, food, friend, made, said, wo...\n",
       "8705     [first, time, ever, saw, movi, jami, foxx, bet...\n",
       "14439    [dread, bore, movi, even, documentari, time, p...\n",
       "                               ...                        \n",
       "17007    [also, known, stairway, heaven, us, wwii, brit...\n",
       "12073    [rememb, view, movi, kid, recal, terrifi, imme...\n",
       "2789     [think, polic, alway, right, br, br, believ, e...\n",
       "11400    [rais, canada, saw, short, mani, time, mostli,...\n",
       "17223    [yet, quit, bad, enough, make, enjoy, fact, on...\n",
       "Name: filtered_words, Length: 5000, dtype: object"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_sample['filtered_words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalmente depois do processamento juntamos as palavras novamente em uma única string\n",
    "movies_sample['join_words'] = movies_sample['filtered_words'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32214    movi good concept execut live br br concept we...\n",
       "21823    film aw screenplay bad script mediocr even sex...\n",
       "35693    ever tri kind food friend made said wow good m...\n",
       "8705     first time ever saw movi jami foxx bet last fa...\n",
       "14439    dread bore movi even documentari time provid i...\n",
       "                               ...                        \n",
       "17007    also known stairway heaven us wwii british pet...\n",
       "12073    rememb view movi kid recal terrifi immens stay...\n",
       "2789     think polic alway right br br believ eye wit a...\n",
       "11400    rais canada saw short mani time mostli cbc see...\n",
       "17223    yet quit bad enough make enjoy fact one bore r...\n",
       "Name: join_words, Length: 5000, dtype: object"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_sample['join_words']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analisando vocabulário"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = pd.DataFrame(\n",
    "    np.concatenate(movies_sample['filtered_words'].values),\n",
    "    columns=['word']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "word\n",
       "aa              1\n",
       "lour            1\n",
       "louri           1\n",
       "lousiest        1\n",
       "loutish         1\n",
       "            ...  \n",
       "like         4527\n",
       "one          5444\n",
       "film         9718\n",
       "movi        10077\n",
       "br          19984\n",
       "Length: 25951, dtype: int64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary.groupby('word').size().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aa': 1,\n",
       " 'lour': 1,\n",
       " 'louri': 1,\n",
       " 'lousiest': 1,\n",
       " 'loutish': 1,\n",
       " 'louwyck': 1,\n",
       " 'lovel': 1,\n",
       " 'loveli': 1,\n",
       " 'loveliest': 1,\n",
       " 'lovestruck': 1,\n",
       " 'lovin': 1,\n",
       " 'lovitz': 1,\n",
       " 'louisianna': 1,\n",
       " 'lovomaniac': 1,\n",
       " 'lowlif': 1,\n",
       " 'lowood': 1,\n",
       " 'lowpoint': 1,\n",
       " 'loyalist': 1,\n",
       " 'lp': 1,\n",
       " 'lq': 1,\n",
       " 'lr': 1,\n",
       " 'ltd': 1,\n",
       " 'luana': 1,\n",
       " 'lubric': 1,\n",
       " 'lucian': 1,\n",
       " 'loweri': 1,\n",
       " 'luciano': 1,\n",
       " 'loudspeak': 1,\n",
       " 'lotta': 1,\n",
       " 'longingli': 1,\n",
       " 'longinotto': 1,\n",
       " 'longleg': 1,\n",
       " 'longshank': 1,\n",
       " 'longtim': 1,\n",
       " 'longueur': 1,\n",
       " 'longwind': 1,\n",
       " 'lonli': 1,\n",
       " 'lonni': 1,\n",
       " 'lookin': 1,\n",
       " 'loompa': 1,\n",
       " 'lotti': 1,\n",
       " 'loonivers': 1,\n",
       " 'loooong': 1,\n",
       " 'loooooov': 1,\n",
       " 'looser': 1,\n",
       " 'lop': 1,\n",
       " 'lordi': 1,\n",
       " 'lordli': 1,\n",
       " 'lorean': 1,\n",
       " 'lorri': 1,\n",
       " 'losthorizon': 1,\n",
       " 'loth': 1,\n",
       " 'lotsa': 1,\n",
       " 'looong': 1,\n",
       " 'longev': 1,\n",
       " 'luckett': 1,\n",
       " 'luddit': 1,\n",
       " 'lyndsay': 1,\n",
       " 'lynley': 1,\n",
       " 'lynx': 1,\n",
       " 'lyta': 1,\n",
       " 'maaan': 1,\n",
       " 'mab': 1,\n",
       " 'mabus': 1,\n",
       " 'macarena': 1,\n",
       " 'macbrid': 1,\n",
       " 'macca': 1,\n",
       " 'macdougal': 1,\n",
       " 'lynda': 1,\n",
       " 'mace': 1,\n",
       " 'macedonian': 1,\n",
       " 'macfadyen': 1,\n",
       " 'macfarlen': 1,\n",
       " 'macfayden': 1,\n",
       " 'macgrud': 1,\n",
       " 'macgubbin': 1,\n",
       " 'macguffin': 1,\n",
       " 'machismo': 1,\n",
       " 'macht': 1,\n",
       " 'macintosh': 1,\n",
       " 'macist': 1,\n",
       " 'macedonia': 1,\n",
       " 'luckless': 1,\n",
       " 'lynd': 1,\n",
       " 'lynchian': 1,\n",
       " 'ludwig': 1,\n",
       " 'luftwaff': 1,\n",
       " 'lug': 1,\n",
       " 'lugaci': 1,\n",
       " 'lugubri': 1,\n",
       " 'luhrman': 1,\n",
       " 'lul': 1,\n",
       " 'lullabi': 1,\n",
       " 'lulli': 1,\n",
       " 'luminari': 1,\n",
       " 'lumpen': 1,\n",
       " 'lynchpin': 1,\n",
       " 'lumpiest': 1,\n",
       " 'lupu': 1,\n",
       " 'luridli': 1,\n",
       " 'lusti': 1,\n",
       " 'luthor': 1,\n",
       " 'lutt': 1,\n",
       " 'lutz': 1,\n",
       " 'luzhini': 1,\n",
       " 'ly': 1,\n",
       " 'lycanthrop': 1,\n",
       " 'lycra': 1,\n",
       " 'lydia': 1,\n",
       " 'lundi': 1,\n",
       " 'macivor': 1,\n",
       " 'longbourn': 1,\n",
       " 'lonelyheart': 1,\n",
       " 'lilyan': 1,\n",
       " 'lim': 1,\n",
       " 'lima': 1,\n",
       " 'limber': 1,\n",
       " 'limbless': 1,\n",
       " 'limburg': 1,\n",
       " 'limey': 1,\n",
       " 'limitless': 1,\n",
       " 'limousin': 1,\n",
       " 'lindberg': 1,\n",
       " 'lindfor': 1,\n",
       " 'lilliputian': 1,\n",
       " 'lindoi': 1,\n",
       " 'lindsley': 1,\n",
       " 'lindstrom': 1,\n",
       " 'linehan': 1,\n",
       " 'linek': 1,\n",
       " 'lineman': 1,\n",
       " 'linen': 1,\n",
       " 'linesmen': 1,\n",
       " 'lineup': 1,\n",
       " 'lingo': 1,\n",
       " 'linguist': 1,\n",
       " 'linkag': 1,\n",
       " 'lindsey': 1,\n",
       " 'linnet': 1,\n",
       " 'lilith': 1,\n",
       " 'likemind': 1,\n",
       " 'liassez': 1,\n",
       " 'libatiqu': 1,\n",
       " 'liberac': 1,\n",
       " 'libido': 1,\n",
       " 'licht': 1,\n",
       " 'licoln': 1,\n",
       " 'liddl': 1,\n",
       " 'lidia': 1,\n",
       " 'lido': 1,\n",
       " 'lidsvil': 1,\n",
       " 'lieb': 1,\n",
       " 'lilian': 1,\n",
       " 'liebman': 1,\n",
       " 'lien': 1,\n",
       " 'lieutent': 1,\n",
       " 'lifeblood': 1,\n",
       " 'lifeguard': 1,\n",
       " 'lifford': 1,\n",
       " 'lightbulb': 1,\n",
       " 'lightheartedli': 1,\n",
       " 'lighthorseman': 1,\n",
       " 'lightner': 1,\n",
       " 'lightol': 1,\n",
       " 'likeli': 1,\n",
       " 'liebowitz': 1,\n",
       " 'lonett': 1,\n",
       " 'linsey': 1,\n",
       " 'linz': 1,\n",
       " 'lob': 1,\n",
       " 'lobisomen': 1,\n",
       " 'loblolli': 1,\n",
       " 'lobotom': 1,\n",
       " 'lobotomis': 1,\n",
       " 'lochari': 1,\n",
       " 'locker': 1,\n",
       " 'lockhe': 1,\n",
       " 'lockley': 1,\n",
       " 'lockwood': 1,\n",
       " 'lockyer': 1,\n",
       " 'lmao': 1,\n",
       " 'loco': 1,\n",
       " 'locust': 1,\n",
       " 'loder': 1,\n",
       " 'loesser': 1,\n",
       " 'loggia': 1,\n",
       " 'lohan': 1,\n",
       " 'lok': 1,\n",
       " 'loken': 1,\n",
       " 'loki': 1,\n",
       " 'lolol': 1,\n",
       " 'londan': 1,\n",
       " 'londo': 1,\n",
       " 'locomot': 1,\n",
       " 'linu': 1,\n",
       " 'llshit': 1,\n",
       " 'lkhubbl': 1,\n",
       " 'lioness': 1,\n",
       " 'lioney': 1,\n",
       " 'liongat': 1,\n",
       " 'lioniz': 1,\n",
       " 'lionsgat': 1,\n",
       " 'lippo': 1,\n",
       " 'lipsyt': 1,\n",
       " 'liquefi': 1,\n",
       " 'lisbon': 1,\n",
       " 'lisett': 1,\n",
       " 'literati': 1,\n",
       " 'llewellyn': 1,\n",
       " 'lith': 1,\n",
       " 'litja': 1,\n",
       " 'litman': 1,\n",
       " 'litmu': 1,\n",
       " 'livabl': 1,\n",
       " 'livelihood': 1,\n",
       " 'liver': 1,\n",
       " 'liveri': 1,\n",
       " 'liverpool': 1,\n",
       " 'liverwurst': 1,\n",
       " 'livin': 1,\n",
       " 'lk': 1,\n",
       " 'lithuanian': 1,\n",
       " 'mackendrick': 1,\n",
       " 'mackinnon': 1,\n",
       " 'macliammoir': 1,\n",
       " 'martini': 1,\n",
       " 'martita': 1,\n",
       " 'maruerit': 1,\n",
       " 'maruschka': 1,\n",
       " 'marybeth': 1,\n",
       " 'marzia': 1,\n",
       " 'masayuki': 1,\n",
       " 'mascarad': 1,\n",
       " 'masladar': 1,\n",
       " 'masoud': 1,\n",
       " 'masqu': 1,\n",
       " 'martinez': 1,\n",
       " 'massenet': 1,\n",
       " 'massimiliano': 1,\n",
       " 'mastercard': 1,\n",
       " 'masterstrok': 1,\n",
       " 'mastoraki': 1,\n",
       " 'mastrantonio': 1,\n",
       " 'mastriosimon': 1,\n",
       " 'mastroianni': 1,\n",
       " 'masuji': 1,\n",
       " 'masur': 1,\n",
       " 'mata': 1,\n",
       " 'matata': 1,\n",
       " 'masseur': 1,\n",
       " 'matchbook': 1,\n",
       " 'martindal': 1,\n",
       " 'martelli': 1,\n",
       " 'mariag': 1,\n",
       " 'mariah': 1,\n",
       " 'marianna': 1,\n",
       " 'maricarmen': 1,\n",
       " 'marich': 1,\n",
       " 'marienbad': 1,\n",
       " 'marienth': 1,\n",
       " 'marilia': 1,\n",
       " 'marionett': 1,\n",
       " 'mariposa': 1,\n",
       " 'marisol': 1,\n",
       " 'marth': 1,\n",
       " 'marita': 1,\n",
       " 'marker': 1,\n",
       " 'marksman': 1,\n",
       " 'marm': 1,\n",
       " 'marner': 1,\n",
       " 'maro': 1,\n",
       " 'marolla': 1,\n",
       " 'marrakech': 1,\n",
       " 'marrakeck': 1,\n",
       " 'marseil': 1,\n",
       " 'marsh': 1,\n",
       " 'marshi': 1,\n",
       " 'marivaux': 1,\n",
       " 'margret': 1,\n",
       " 'matchmak': 1,\n",
       " 'mateo': 1,\n",
       " 'mcarthur': 1,\n",
       " 'mcbeal': 1,\n",
       " 'mcbo': 1,\n",
       " 'mccabe': 1,\n",
       " 'mccarey': 1,\n",
       " 'mccarthyism': 1,\n",
       " 'mcclori': 1,\n",
       " 'mcclug': 1,\n",
       " 'mcclurg': 1,\n",
       " 'mccormick': 1,\n",
       " 'mccowen': 1,\n",
       " 'mcandrew': 1,\n",
       " 'mcdiarmid': 1,\n",
       " 'mcdougl': 1,\n",
       " 'mcdowal': 1,\n",
       " 'mcduck': 1,\n",
       " 'mceneri': 1,\n",
       " 'mcenro': 1,\n",
       " 'mcentir': 1,\n",
       " 'mceveeti': 1,\n",
       " 'mcfadden': 1,\n",
       " 'mcgiver': 1,\n",
       " 'mcgiveth': 1,\n",
       " 'mcgowan': 1,\n",
       " 'mcdougal': 1,\n",
       " 'matchup': 1,\n",
       " 'mcanal': 1,\n",
       " 'mba': 1,\n",
       " 'matern': 1,\n",
       " 'mathau': 1,\n",
       " 'matheron': 1,\n",
       " 'mathurin': 1,\n",
       " 'matic': 1,\n",
       " 'matlock': 1,\n",
       " 'matricul': 1,\n",
       " 'matrimoni': 1,\n",
       " 'matronli': 1,\n",
       " 'matsushima': 1,\n",
       " 'matthu': 1,\n",
       " 'mbna': 1,\n",
       " 'mattia': 1,\n",
       " 'maughan': 1,\n",
       " 'mauritania': 1,\n",
       " 'mauritiu': 1,\n",
       " 'mauser': 1,\n",
       " 'mauvai': 1,\n",
       " 'maximilian': 1,\n",
       " 'maxmillian': 1,\n",
       " 'mayberri': 1,\n",
       " 'mayflow': 1,\n",
       " 'mayi': 1,\n",
       " 'mayo': 1,\n",
       " 'matuszak': 1,\n",
       " 'margo': 1,\n",
       " 'marginalis': 1,\n",
       " 'margarin': 1,\n",
       " 'mahari': 1,\n",
       " 'mahdist': 1,\n",
       " 'mahogani': 1,\n",
       " 'maidment': 1,\n",
       " 'maili': 1,\n",
       " 'maim': 1,\n",
       " 'mainev': 1,\n",
       " 'mainspr': 1,\n",
       " 'maitlan': 1,\n",
       " 'maiz': 1,\n",
       " 'majkowski': 1,\n",
       " 'mahal': 1,\n",
       " 'majo': 1,\n",
       " 'makeout': 1,\n",
       " 'mako': 1,\n",
       " 'malabimba': 1,\n",
       " 'maladroit': 1,\n",
       " 'malam': 1,\n",
       " 'malama': 1,\n",
       " 'malay': 1,\n",
       " 'malayan': 1,\n",
       " 'maldera': 1,\n",
       " 'malebranch': 1,\n",
       " 'maledetta': 1,\n",
       " 'majumdar': 1,\n",
       " 'malenkaya': 1,\n",
       " 'magritt': 1,\n",
       " 'magnifiqu': 1,\n",
       " 'macneil': 1,\n",
       " 'macquir': 1,\n",
       " 'macreadi': 1,\n",
       " 'macro': 1,\n",
       " 'madan': 1,\n",
       " 'madchen': 1,\n",
       " 'maddeningli': 1,\n",
       " 'madder': 1,\n",
       " 'maddin': 1,\n",
       " 'maddison': 1,\n",
       " 'madelin': 1,\n",
       " 'magon': 1,\n",
       " 'madhous': 1,\n",
       " 'madoc': 1,\n",
       " 'madr': 1,\n",
       " 'madwoman': 1,\n",
       " 'magali': 1,\n",
       " 'magdalen': 1,\n",
       " 'magdalena': 1,\n",
       " 'magi': 1,\n",
       " 'magnani': 1,\n",
       " 'magnavis': 1,\n",
       " 'magnavolt': 1,\n",
       " 'magnfici': 1,\n",
       " 'madkaugh': 1,\n",
       " 'maletta': 1,\n",
       " 'malfunct': 1,\n",
       " 'malhotra': 1,\n",
       " 'manna': 1,\n",
       " 'mannen': 1,\n",
       " 'mannheim': 1,\n",
       " 'mannish': 1,\n",
       " 'manoj': 1,\n",
       " 'manojlov': 1,\n",
       " 'manserv': 1,\n",
       " 'manslaught': 1,\n",
       " 'mantan': 1,\n",
       " 'manti': 1,\n",
       " 'mantra': 1,\n",
       " 'manigot': 1,\n",
       " 'manzil': 1,\n",
       " 'maori': 1,\n",
       " 'mapl': 1,\n",
       " 'mapoth': 1,\n",
       " 'mappl': 1,\n",
       " 'maraud': 1,\n",
       " 'marcelin': 1,\n",
       " 'marchal': 1,\n",
       " 'marcia': 1,\n",
       " 'marconi': 1,\n",
       " 'marcuzzo': 1,\n",
       " 'margareta': 1,\n",
       " 'maojlov': 1,\n",
       " 'manhol': 1,\n",
       " 'manhatttan': 1,\n",
       " 'manhatten': 1,\n",
       " 'malik': 1,\n",
       " 'malleabl': 1,\n",
       " 'mallepa': 1,\n",
       " 'mallori': 1,\n",
       " 'malocchio': 1,\n",
       " 'malta': 1,\n",
       " 'mam': 1,\n",
       " 'mambo': 1,\n",
       " 'mammi': 1,\n",
       " 'mamooth': 1,\n",
       " 'manchest': 1,\n",
       " 'manchuria': 1,\n",
       " 'manchurian': 1,\n",
       " 'mancia': 1,\n",
       " 'mancini': 1,\n",
       " 'mancuso': 1,\n",
       " 'manderley': 1,\n",
       " 'mandolin': 1,\n",
       " 'mandraki': 1,\n",
       " 'manet': 1,\n",
       " 'manger': 1,\n",
       " 'mangi': 1,\n",
       " 'mangl': 1,\n",
       " 'mangler': 1,\n",
       " 'manhat': 1,\n",
       " 'liang': 1,\n",
       " 'mcgraw': 1,\n",
       " 'lia': 1,\n",
       " 'leyt': 1,\n",
       " 'kino': 1,\n",
       " 'kinsey': 1,\n",
       " 'kintaro': 1,\n",
       " 'kippei': 1,\n",
       " 'kirbyef': 1,\n",
       " 'kirkland': 1,\n",
       " 'kishikawa': 1,\n",
       " 'kishor': 1,\n",
       " 'kisser': 1,\n",
       " 'kissing': 1,\n",
       " 'kitchi': 1,\n",
       " 'kinkad': 1,\n",
       " 'kitross': 1,\n",
       " 'kitt': 1,\n",
       " 'kittiwak': 1,\n",
       " 'kkk': 1,\n",
       " 'klang': 1,\n",
       " 'klapisch': 1,\n",
       " 'klaus': 1,\n",
       " 'kleban': 1,\n",
       " 'kleber': 1,\n",
       " 'kleenex': 1,\n",
       " 'kleinman': 1,\n",
       " 'klick': 1,\n",
       " 'kitschi': 1,\n",
       " 'klimovski': 1,\n",
       " 'kinji': 1,\n",
       " 'kinglsey': 1,\n",
       " 'kiddin': 1,\n",
       " 'kiddo': 1,\n",
       " 'kierkegaard': 1,\n",
       " 'kierlaw': 1,\n",
       " 'kietel': 1,\n",
       " 'kieth': 1,\n",
       " 'kiil': 1,\n",
       " 'kiki': 1,\n",
       " 'kilcher': 1,\n",
       " 'kildar': 1,\n",
       " 'kilim': 1,\n",
       " 'kingsford': 1,\n",
       " 'killabl': 1,\n",
       " 'kilpatrick': 1,\n",
       " 'kilt': 1,\n",
       " 'kimbal': 1,\n",
       " 'kimbl': 1,\n",
       " 'kimi': 1,\n",
       " 'kimmel': 1,\n",
       " 'kincaid': 1,\n",
       " 'kindsa': 1,\n",
       " 'kinetescop': 1,\n",
       " 'kinetoscop': 1,\n",
       " 'kinfolk': 1,\n",
       " 'kilmor': 1,\n",
       " 'kidder': 1,\n",
       " 'klip': 1,\n",
       " 'klop': 1,\n",
       " 'kono': 1,\n",
       " 'koo': 1,\n",
       " 'kookoo': 1,\n",
       " 'koolhoven': 1,\n",
       " 'koop': 1,\n",
       " 'kopek': 1,\n",
       " 'koran': 1,\n",
       " 'korngold': 1,\n",
       " 'kornhaus': 1,\n",
       " 'kornman': 1,\n",
       " 'korp': 1,\n",
       " 'kommissar': 1,\n",
       " 'koslova': 1,\n",
       " 'kosugi': 1,\n",
       " 'kotto': 1,\n",
       " 'kou': 1,\n",
       " 'koun': 1,\n",
       " 'kounen': 1,\n",
       " 'kovacev': 1,\n",
       " 'kove': 1,\n",
       " 'kowtow': 1,\n",
       " 'koya': 1,\n",
       " 'koz': 1,\n",
       " 'kozasa': 1,\n",
       " 'kostic': 1,\n",
       " 'klondik': 1,\n",
       " 'komizu': 1,\n",
       " 'komatsu': 1,\n",
       " 'klugman': 1,\n",
       " 'knacker': 1,\n",
       " 'knappertsbusch': 1,\n",
       " 'knicker': 1,\n",
       " 'knickerbock': 1,\n",
       " 'knieper': 1,\n",
       " 'knievel': 1,\n",
       " 'knightli': 1,\n",
       " 'knoflikari': 1,\n",
       " 'knowabl': 1,\n",
       " 'knucklehead': 1,\n",
       " 'kombat': 1,\n",
       " 'kobayashi': 1,\n",
       " 'koboi': 1,\n",
       " 'koch': 1,\n",
       " 'kodak': 1,\n",
       " 'kodi': 1,\n",
       " 'kodokoo': 1,\n",
       " 'koenigsegg': 1,\n",
       " 'koffe': 1,\n",
       " 'kohler': 1,\n",
       " 'kolev': 1,\n",
       " 'kolkata': 1,\n",
       " 'kolker': 1,\n",
       " 'kobe': 1,\n",
       " 'kpc': 1,\n",
       " 'kickback': 1,\n",
       " 'kibbutz': 1,\n",
       " 'kasey': 1,\n",
       " 'kashmir': 1,\n",
       " 'kassandra': 1,\n",
       " 'kassir': 1,\n",
       " 'kassman': 1,\n",
       " 'kastner': 1,\n",
       " 'kasugi': 1,\n",
       " 'katarzyna': 1,\n",
       " 'katayama': 1,\n",
       " 'katey': 1,\n",
       " 'katha': 1,\n",
       " 'kasem': 1,\n",
       " 'katharina': 1,\n",
       " 'katja': 1,\n",
       " 'katkin': 1,\n",
       " 'katsuhito': 1,\n",
       " 'katya': 1,\n",
       " 'kau': 1,\n",
       " 'kawamori': 1,\n",
       " 'kaylan': 1,\n",
       " 'kaza': 1,\n",
       " 'kazakh': 1,\n",
       " 'kazooi': 1,\n",
       " 'keat': 1,\n",
       " 'kathmandu': 1,\n",
       " 'kebbel': 1,\n",
       " 'kasadya': 1,\n",
       " 'karzi': 1,\n",
       " 'kanagawa': 1,\n",
       " 'kandic': 1,\n",
       " 'kang': 1,\n",
       " 'kangaroo': 1,\n",
       " 'kangho': 1,\n",
       " 'kanh': 1,\n",
       " 'kanin': 1,\n",
       " 'kanno': 1,\n",
       " 'kant': 1,\n",
       " 'kaput': 1,\n",
       " 'karadz': 1,\n",
       " 'kasaba': 1,\n",
       " 'karamzov': 1,\n",
       " 'kardo': 1,\n",
       " 'kareen': 1,\n",
       " 'karena': 1,\n",
       " 'karim': 1,\n",
       " 'karino': 1,\n",
       " 'karisma': 1,\n",
       " 'karmic': 1,\n",
       " 'karn': 1,\n",
       " 'karnag': 1,\n",
       " 'kartalian': 1,\n",
       " 'karyo': 1,\n",
       " 'karan': 1,\n",
       " 'kibitz': 1,\n",
       " 'ked': 1,\n",
       " 'keef': 1,\n",
       " 'kesan': 1,\n",
       " 'kessler': 1,\n",
       " 'kester': 1,\n",
       " 'ketamin': 1,\n",
       " 'ketchup': 1,\n",
       " 'keuck': 1,\n",
       " 'keung': 1,\n",
       " 'keusch': 1,\n",
       " 'kewl': 1,\n",
       " 'kewpi': 1,\n",
       " 'keyboardist': 1,\n",
       " 'kerwin': 1,\n",
       " 'keziko': 1,\n",
       " 'khaleil': 1,\n",
       " 'khali': 1,\n",
       " 'khanabadosh': 1,\n",
       " 'kharbanda': 1,\n",
       " 'kheir': 1,\n",
       " 'khemu': 1,\n",
       " 'kheymeh': 1,\n",
       " 'khoua': 1,\n",
       " 'khrystyn': 1,\n",
       " 'khufu': 1,\n",
       " 'kibbe': 1,\n",
       " 'kfir': 1,\n",
       " 'kedrova': 1,\n",
       " 'kershaw': 1,\n",
       " 'kermod': 1,\n",
       " 'keeler': 1,\n",
       " 'keenan': 1,\n",
       " 'keenen': 1,\n",
       " 'keenli': 1,\n",
       " 'keerthana': 1,\n",
       " 'keifer': 1,\n",
       " 'keiko': 1,\n",
       " 'keir': 1,\n",
       " 'keisuk': 1,\n",
       " 'keita': 1,\n",
       " 'kel': 1,\n",
       " 'kerrigan': 1,\n",
       " 'keller': 1,\n",
       " 'kemo': 1,\n",
       " 'kemono': 1,\n",
       " 'kendo': 1,\n",
       " 'kenitalia': 1,\n",
       " 'kenov': 1,\n",
       " 'kenshin': 1,\n",
       " 'keoma': 1,\n",
       " 'ker': 1,\n",
       " 'kerbi': 1,\n",
       " 'kerchev': 1,\n",
       " 'keri': 1,\n",
       " 'kellerkind': 1,\n",
       " 'kraft': 1,\n",
       " 'krakowski': 1,\n",
       " 'krasker': 1,\n",
       " 'layman': 1,\n",
       " 'layton': 1,\n",
       " 'laze': 1,\n",
       " 'lazerov': 1,\n",
       " 'laziest': 1,\n",
       " 'lcd': 1,\n",
       " 'lea': 1,\n",
       " 'leaflet': 1,\n",
       " 'leander': 1,\n",
       " 'leapt': 1,\n",
       " 'learner': 1,\n",
       " 'layabout': 1,\n",
       " 'leash': 1,\n",
       " 'leatheri': 1,\n",
       " 'leaven': 1,\n",
       " 'leavin': 1,\n",
       " 'lebanes': 1,\n",
       " 'lebanon': 1,\n",
       " 'lebowski': 1,\n",
       " 'lecter': 1,\n",
       " 'ledg': 1,\n",
       " 'leech': 1,\n",
       " 'leena': 1,\n",
       " 'leet': 1,\n",
       " 'leatherfac': 1,\n",
       " 'leeway': 1,\n",
       " 'laxmi': 1,\n",
       " 'lawnmow': 1,\n",
       " 'latina': 1,\n",
       " 'latitud': 1,\n",
       " 'latrin': 1,\n",
       " 'latshaw': 1,\n",
       " 'latterli': 1,\n",
       " 'latvia': 1,\n",
       " 'laudabl': 1,\n",
       " 'laudatori': 1,\n",
       " 'lauderdal': 1,\n",
       " 'laudrup': 1,\n",
       " 'laufther': 1,\n",
       " 'lawson': 1,\n",
       " 'laugher': 1,\n",
       " 'launchpad': 1,\n",
       " 'launder': 1,\n",
       " 'laundromat': 1,\n",
       " 'laurent': 1,\n",
       " 'lauret': 1,\n",
       " 'lauter': 1,\n",
       " 'lavign': 1,\n",
       " 'lavinia': 1,\n",
       " 'lavishli': 1,\n",
       " 'lawanda': 1,\n",
       " 'lawman': 1,\n",
       " 'laughtrack': 1,\n",
       " 'latifah': 1,\n",
       " 'leez': 1,\n",
       " 'leger': 1,\n",
       " 'leprachaun': 1,\n",
       " 'leprosi': 1,\n",
       " 'leprou': 1,\n",
       " 'leskin': 1,\n",
       " 'lestat': 1,\n",
       " 'lesur': 1,\n",
       " 'lethargi': 1,\n",
       " 'letscher': 1,\n",
       " 'lettuc': 1,\n",
       " 'letyat': 1,\n",
       " 'letzt': 1,\n",
       " 'lepord': 1,\n",
       " 'leukemia': 1,\n",
       " 'leve': 1,\n",
       " 'lever': 1,\n",
       " 'leverag': 1,\n",
       " 'leverett': 1,\n",
       " 'levitt': 1,\n",
       " 'levr': 1,\n",
       " 'lew': 1,\n",
       " 'leway': 1,\n",
       " 'lexicon': 1,\n",
       " 'lexington': 1,\n",
       " 'leyland': 1,\n",
       " 'leval': 1,\n",
       " 'lefler': 1,\n",
       " 'leper': 1,\n",
       " 'lennier': 1,\n",
       " 'legionairr': 1,\n",
       " 'legisl': 1,\n",
       " 'legitimaci': 1,\n",
       " 'legizomo': 1,\n",
       " 'legless': 1,\n",
       " 'lego': 1,\n",
       " 'legre': 1,\n",
       " 'legrix': 1,\n",
       " 'leguzaimo': 1,\n",
       " 'lehrman': 1,\n",
       " 'leiberman': 1,\n",
       " 'leong': 1,\n",
       " 'leicest': 1,\n",
       " 'leighton': 1,\n",
       " 'leisin': 1,\n",
       " 'leiter': 1,\n",
       " 'leith': 1,\n",
       " 'lemk': 1,\n",
       " 'lemoin': 1,\n",
       " 'lender': 1,\n",
       " 'lenghten': 1,\n",
       " 'lengthier': 1,\n",
       " 'lengua': 1,\n",
       " 'leni': 1,\n",
       " 'leif': 1,\n",
       " 'latido': 1,\n",
       " 'latent': 1,\n",
       " 'laster': 1,\n",
       " 'kuzko': 1,\n",
       " 'kwai': 1,\n",
       " 'kwak': 1,\n",
       " 'kwik': 1,\n",
       " 'kwok': 1,\n",
       " 'kwon': 1,\n",
       " 'kyonki': 1,\n",
       " 'kyuubi': 1,\n",
       " 'labeija': 1,\n",
       " 'labl': 1,\n",
       " 'lacanian': 1,\n",
       " 'kustarica': 1,\n",
       " 'lacer': 1,\n",
       " 'lachrymos': 1,\n",
       " 'lackey': 1,\n",
       " 'lacquer': 1,\n",
       " 'lacuna': 1,\n",
       " 'ladron': 1,\n",
       " 'ladykil': 1,\n",
       " 'ladylik': 1,\n",
       " 'ladyship': 1,\n",
       " 'laert': 1,\n",
       " 'laetitia': 1,\n",
       " 'laila': 1,\n",
       " 'lacey': 1,\n",
       " 'lajo': 1,\n",
       " 'kusanagi': 1,\n",
       " 'kurtwood': 1,\n",
       " 'kraus': 1,\n",
       " 'kray': 1,\n",
       " 'krazi': 1,\n",
       " 'kreb': 1,\n",
       " 'kretschmann': 1,\n",
       " 'kringl': 1,\n",
       " 'kristian': 1,\n",
       " 'kristina': 1,\n",
       " 'kroft': 1,\n",
       " 'krull': 1,\n",
       " 'kryptonit': 1,\n",
       " 'kurupt': 1,\n",
       " 'krystal': 1,\n",
       " 'kubota': 1,\n",
       " 'kubrik': 1,\n",
       " 'kukkonen': 1,\n",
       " 'kulbhushan': 1,\n",
       " 'kum': 1,\n",
       " 'kunal': 1,\n",
       " 'kundera': 1,\n",
       " 'kungfu': 1,\n",
       " 'kunz': 1,\n",
       " 'kuprik': 1,\n",
       " 'kurdish': 1,\n",
       " 'krzysztof': 1,\n",
       " 'laka': 1,\n",
       " 'laker': 1,\n",
       " 'lakesid': 1,\n",
       " 'lanka': 1,\n",
       " 'lankan': 1,\n",
       " 'lanki': 1,\n",
       " 'lantana': 1,\n",
       " 'lao': 1,\n",
       " 'laplanch': 1,\n",
       " 'lappland': 1,\n",
       " 'laptop': 1,\n",
       " 'lar': 1,\n",
       " 'larain': 1,\n",
       " 'larami': 1,\n",
       " 'lani': 1,\n",
       " 'larceni': 1,\n",
       " 'largess': 1,\n",
       " 'larkin': 1,\n",
       " 'larrikin': 1,\n",
       " 'larroquett': 1,\n",
       " 'laru': 1,\n",
       " 'larval': 1,\n",
       " 'laryssa': 1,\n",
       " 'laserlight': 1,\n",
       " 'lashel': 1,\n",
       " 'lassen': 1,\n",
       " 'lasso': 1,\n",
       " 'larf': 1,\n",
       " 'languish': 1,\n",
       " 'languidli': 1,\n",
       " 'langrish': 1,\n",
       " 'lakin': 1,\n",
       " 'lakshmi': 1,\n",
       " 'lalo': 1,\n",
       " 'lamar': 1,\n",
       " 'lamarch': 1,\n",
       " 'lambent': 1,\n",
       " 'lamh': 1,\n",
       " 'lamour': 1,\n",
       " 'lampa': 1,\n",
       " 'lampanelli': 1,\n",
       " 'lampost': 1,\n",
       " 'lamppost': 1,\n",
       " 'lampshad': 1,\n",
       " 'lan': 1,\n",
       " 'landa': 1,\n",
       " 'landau': 1,\n",
       " 'landfil': 1,\n",
       " 'landmass': 1,\n",
       " 'landown': 1,\n",
       " 'landslid': 1,\n",
       " 'landua': 1,\n",
       " 'langa': 1,\n",
       " 'langaug': 1,\n",
       " 'langford': 1,\n",
       " 'langond': 1,\n",
       " 'lhasa': 1,\n",
       " 'kampf': 1,\n",
       " 'mcgree': 1,\n",
       " 'mchale': 1,\n",
       " 'nill': 1,\n",
       " 'nilli': 1,\n",
       " 'nilsen': 1,\n",
       " 'nilsson': 1,\n",
       " 'nim': 1,\n",
       " 'nimbl': 1,\n",
       " 'nimri': 1,\n",
       " 'niner': 1,\n",
       " 'nino': 1,\n",
       " 'ninotchka': 1,\n",
       " 'ninteen': 1,\n",
       " 'niku': 1,\n",
       " 'nintendo': 1,\n",
       " 'nip': 1,\n",
       " 'niqu': 1,\n",
       " 'nishabd': 1,\n",
       " 'nitin': 1,\n",
       " 'nitrat': 1,\n",
       " 'nitti': 1,\n",
       " 'nitu': 1,\n",
       " 'nivola': 1,\n",
       " 'nj': 1,\n",
       " 'noak': 1,\n",
       " 'nobi': 1,\n",
       " 'niob': 1,\n",
       " 'nobler': 1,\n",
       " 'nikkatsu': 1,\n",
       " 'nike': 1,\n",
       " 'niceiti': 1,\n",
       " 'nicholett': 1,\n",
       " 'nickelodean': 1,\n",
       " 'nicklebi': 1,\n",
       " 'nickola': 1,\n",
       " 'nico': 1,\n",
       " 'nicodemu': 1,\n",
       " 'nicodim': 1,\n",
       " 'nicolai': 1,\n",
       " 'nicoletta': 1,\n",
       " 'nicolson': 1,\n",
       " 'niki': 1,\n",
       " 'niei': 1,\n",
       " 'nietzschean': 1,\n",
       " 'nigeria': 1,\n",
       " 'nigger': 1,\n",
       " 'nightbe': 1,\n",
       " 'nightgown': 1,\n",
       " 'nighti': 1,\n",
       " 'nightli': 1,\n",
       " 'nightsheet': 1,\n",
       " 'nighwatch': 1,\n",
       " 'nihon': 1,\n",
       " 'nikan': 1,\n",
       " 'nieto': 1,\n",
       " 'niamh': 1,\n",
       " 'nobli': 1,\n",
       " 'noch': 1,\n",
       " 'northwestern': 1,\n",
       " 'nosed': 1,\n",
       " 'nosher': 1,\n",
       " 'nostril': 1,\n",
       " 'notar': 1,\n",
       " 'notepad': 1,\n",
       " 'nother': 1,\n",
       " 'nought': 1,\n",
       " 'nourish': 1,\n",
       " 'nova': 1,\n",
       " 'novelett': 1,\n",
       " 'northbound': 1,\n",
       " 'novello': 1,\n",
       " 'noyc': 1,\n",
       " 'nozzl': 1,\n",
       " 'npr': 1,\n",
       " 'ns': 1,\n",
       " 'nsync': 1,\n",
       " 'ntarea': 1,\n",
       " 'nuac': 1,\n",
       " 'nub': 1,\n",
       " 'nube': 1,\n",
       " 'numan': 1,\n",
       " 'numbr': 1,\n",
       " 'noy': 1,\n",
       " 'nobuo': 1,\n",
       " 'norsemen': 1,\n",
       " 'noroi': 1,\n",
       " 'nocturn': 1,\n",
       " 'noethem': 1,\n",
       " 'nofth': 1,\n",
       " 'noin': 1,\n",
       " 'noirish': 1,\n",
       " 'noisi': 1,\n",
       " 'noisiest': 1,\n",
       " 'nolti': 1,\n",
       " 'nona': 1,\n",
       " 'noncommitt': 1,\n",
       " 'nondenomin': 1,\n",
       " 'norrland': 1,\n",
       " 'nonono': 1,\n",
       " 'nonviol': 1,\n",
       " 'nook': 1,\n",
       " 'noonan': 1,\n",
       " 'noooooo': 1,\n",
       " ...}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary.groupby('word').size().sort_values().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "word\n",
       "br         9962\n",
       "film       5305\n",
       "movi       4356\n",
       "one        2807\n",
       "like       2130\n",
       "time       1688\n",
       "good       1574\n",
       "see        1512\n",
       "stori      1456\n",
       "charact    1427\n",
       "make       1411\n",
       "watch      1376\n",
       "well       1363\n",
       "get        1323\n",
       "great      1312\n",
       "love       1237\n",
       "show       1151\n",
       "would      1123\n",
       "also       1123\n",
       "realli     1069\n",
       "dtype: int64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separando críticas positivas\n",
    "review_pos = movies_sample[movies_sample['label'] == 1]\n",
    "vocabulary_pos = pd.DataFrame(np.concatenate(review_pos['filtered_words'].values), columns=['word'])\n",
    "vocabulary_pos.groupby('word').size().sort_values(ascending=False)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Aula 1 - NLP - Aux.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
