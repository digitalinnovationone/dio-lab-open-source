{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J2HFkxmHfd_R"
   },
   "source": [
    "## Aula 1 - Processamento de Linguagem Natual\n",
    "\n",
    "Na aula de hoje, vamos explorar os seguintes t√≥picos em Python:\n",
    "\n",
    "1) Dados Estruturados e N√£o Estruturados.  \n",
    "2) Introdu√ß√£o a NLP.  \n",
    "3) Processamento de Textos.  \n",
    "4) Exerc√≠cios.  \n",
    "\n",
    "<img src=\"https://i1.wp.com/thedatascientist.com/wp-content/uploads/2018/09/data_science_wordcloud.png?fit=1584%2C1008&ssl=1\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hgvh29EJh6UK"
   },
   "source": [
    "Primeiramente, precisamos entender qual a diferen√ßa enre as duas fontes de dados mais comuns, sendo elas dados **estruturados** e **n√£o estruturados**. Definimos ele como:\n",
    "<br><br>\n",
    "- **Dados Estruturados:** S√£o dados que seguem uma estrutura mais r√≠gida com um padr√£o fixo e constante. Por exemplo: Tabelas e DataFrames;<br><br>\n",
    "- **Dados N√£o estruturados:** Como j√° diz o nome, s√£o dados que n√£o tem uma estrutura bem estabelecida e necessitam de um processamento adicional para trabalharmos com eles. Exemplos: √°udios, v√≠deos, textos e etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whQI1eePiJwx"
   },
   "source": [
    "### Introdu√ß√£o ao Processamento de Linguagem Natural (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bwjo0xx9h-Pi"
   },
   "source": [
    "O Processamento de Linguagem Natural, mas conhecido como NLP, √© a abordagem onde trabalhamos com **dados n√£o estruturados** do tipo **Texto**. O objetivo de trabalharmos com textos √© extrair de informa√ß√£o e teor lingu√≠stico das nossas bases de textos e converter isso de uma forma n√∫merica, onde poderemos utilizar em nossos modelos de *Machine Learning*.<br><br>\n",
    "Temos como exemplos de aplica√ß√µes de NLP como:\n",
    "- An√°lise de Sentimentos em review de filmes e produtos ou mensagens em redes sociais;\n",
    "- Filtro de E-Mails Spams e N√£o Spams;\n",
    "- Identifica√ß√£o de textos a partir de constru√ß√µes lingu√≠sticas (descobrir se um texto foi escrito ou n√£o por Machado de assis);\n",
    "- Tradutores de Idiomas;\n",
    "- ChatBots;\n",
    "- Corretores Ortogr√°ficos;\n",
    "- Classifica√ß√£o de textos de acordo com o conte√∫do do texto (Esportes, Pol√≠tica, Economia e etc).\n",
    "<br><br>\n",
    "Nesta aula iremos aprender a partir dos nossos dados textuais a como processar, tratar e transformar os dados de uma maneira que os modelos de *Machine Learning* entendam.<br><br>\n",
    "\n",
    "A principal biblioteca de refer√™ncia para NLP chama-se [NLTK - Natural Language Toll Kit](https://www.nltk.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IjXHV5QOjWDc"
   },
   "source": [
    "### Processamento de Textos\n",
    "\n",
    "Antes de mais nada, precisamos filtrar e tratar os nossos textos, de forma a deixar apenas o conte√∫do de mais relevantes para a nossa an√°lise. Existem alguns processos importantes para trabalhar com os textos (n√£o necessariamente voc√™ precisa aplicar todos os procesos)\n",
    "\n",
    "- Remo√ß√£o de Stopwords;\n",
    "- Limpeza de Textos;\n",
    "- Tokeniza√ß√£o;\n",
    "- Normaliza√ß√£o do Texto.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9je2oXCjjrwc"
   },
   "source": [
    "### Stopwords\n",
    "\n",
    "Stopwords s√£o palavras que aparecem com uma frequ√™ncia muito alta nos textos, mas que n√£o trazem um teor de conte√∫do relevante para o nosso modelo. Vamos entender isso na pr√°tica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3IR-pN2DjwXJ",
    "outputId": "9c8b59ea-653b-457e-b82e-d36bc3f07b67"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/cecilia/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/cecilia/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package rslp to /Users/cecilia/nltk_data...\n",
      "[nltk_data]   Package rslp is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')  # https://www.nltk.org/_modules/nltk/tokenize/punkt.html\n",
    "nltk.download('rslp')   # Stemmer em portugu√™s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jR2RHfCOn83i"
   },
   "source": [
    "Baixada a fun√ß√£o de Stopwords, vamos definir um set de stopwords onde teremos uma lista com todas as stopwords em ingl√™s j√° identificadas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "-t93Bq2FncDC"
   },
   "outputs": [],
   "source": [
    "stopwords_en = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LllQ0D8sol9G",
    "outputId": "0a61c196-e808-44da-90f1-76e584b5ce1a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ClhJwZvOoqI-",
    "outputId": "37956347-1525-4132-8e86-5f457bfe9f1d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(stopwords_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3OEsva3po-FN",
    "outputId": "3c1edc99-4044-4632-8f22-3bbe9d92deaa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "vcQKI7ZdolbV"
   },
   "outputs": [],
   "source": [
    "stopwords_port = nltk.corpus.stopwords.words('portuguese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-16r_klQolCC",
    "outputId": "2debcb43-b617-4c22-8a74-f045d415e88c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "207"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords_port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y6gP4YowoyYk",
    "outputId": "c957bddd-31db-4403-c6f0-cc154ecb74cf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "207"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(stopwords_port))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TyHAf5Uko0ZQ",
    "outputId": "8ffe9e50-8250-4686-88d6-93e5855775d6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " '√†',\n",
       " 'ao',\n",
       " 'aos',\n",
       " 'aquela',\n",
       " 'aquelas',\n",
       " 'aquele',\n",
       " 'aqueles',\n",
       " 'aquilo',\n",
       " 'as',\n",
       " '√†s',\n",
       " 'at√©',\n",
       " 'com',\n",
       " 'como',\n",
       " 'da',\n",
       " 'das',\n",
       " 'de',\n",
       " 'dela',\n",
       " 'delas',\n",
       " 'dele',\n",
       " 'deles',\n",
       " 'depois',\n",
       " 'do',\n",
       " 'dos',\n",
       " 'e',\n",
       " '√©',\n",
       " 'ela',\n",
       " 'elas',\n",
       " 'ele',\n",
       " 'eles',\n",
       " 'em',\n",
       " 'entre',\n",
       " 'era',\n",
       " 'eram',\n",
       " '√©ramos',\n",
       " 'essa',\n",
       " 'essas',\n",
       " 'esse',\n",
       " 'esses',\n",
       " 'esta',\n",
       " 'est√°',\n",
       " 'estamos',\n",
       " 'est√£o',\n",
       " 'estar',\n",
       " 'estas',\n",
       " 'estava',\n",
       " 'estavam',\n",
       " 'est√°vamos',\n",
       " 'este',\n",
       " 'esteja',\n",
       " 'estejam',\n",
       " 'estejamos',\n",
       " 'estes',\n",
       " 'esteve',\n",
       " 'estive',\n",
       " 'estivemos',\n",
       " 'estiver',\n",
       " 'estivera',\n",
       " 'estiveram',\n",
       " 'estiv√©ramos',\n",
       " 'estiverem',\n",
       " 'estivermos',\n",
       " 'estivesse',\n",
       " 'estivessem',\n",
       " 'estiv√©ssemos',\n",
       " 'estou',\n",
       " 'eu',\n",
       " 'foi',\n",
       " 'fomos',\n",
       " 'for',\n",
       " 'fora',\n",
       " 'foram',\n",
       " 'f√¥ramos',\n",
       " 'forem',\n",
       " 'formos',\n",
       " 'fosse',\n",
       " 'fossem',\n",
       " 'f√¥ssemos',\n",
       " 'fui',\n",
       " 'h√°',\n",
       " 'haja',\n",
       " 'hajam',\n",
       " 'hajamos',\n",
       " 'h√£o',\n",
       " 'havemos',\n",
       " 'haver',\n",
       " 'hei',\n",
       " 'houve',\n",
       " 'houvemos',\n",
       " 'houver',\n",
       " 'houvera',\n",
       " 'houver√°',\n",
       " 'houveram',\n",
       " 'houv√©ramos',\n",
       " 'houver√£o',\n",
       " 'houverei',\n",
       " 'houverem',\n",
       " 'houveremos',\n",
       " 'houveria',\n",
       " 'houveriam',\n",
       " 'houver√≠amos',\n",
       " 'houvermos',\n",
       " 'houvesse',\n",
       " 'houvessem',\n",
       " 'houv√©ssemos',\n",
       " 'isso',\n",
       " 'isto',\n",
       " 'j√°',\n",
       " 'lhe',\n",
       " 'lhes',\n",
       " 'mais',\n",
       " 'mas',\n",
       " 'me',\n",
       " 'mesmo',\n",
       " 'meu',\n",
       " 'meus',\n",
       " 'minha',\n",
       " 'minhas',\n",
       " 'muito',\n",
       " 'na',\n",
       " 'n√£o',\n",
       " 'nas',\n",
       " 'nem',\n",
       " 'no',\n",
       " 'nos',\n",
       " 'n√≥s',\n",
       " 'nossa',\n",
       " 'nossas',\n",
       " 'nosso',\n",
       " 'nossos',\n",
       " 'num',\n",
       " 'numa',\n",
       " 'o',\n",
       " 'os',\n",
       " 'ou',\n",
       " 'para',\n",
       " 'pela',\n",
       " 'pelas',\n",
       " 'pelo',\n",
       " 'pelos',\n",
       " 'por',\n",
       " 'qual',\n",
       " 'quando',\n",
       " 'que',\n",
       " 'quem',\n",
       " 's√£o',\n",
       " 'se',\n",
       " 'seja',\n",
       " 'sejam',\n",
       " 'sejamos',\n",
       " 'sem',\n",
       " 'ser',\n",
       " 'ser√°',\n",
       " 'ser√£o',\n",
       " 'serei',\n",
       " 'seremos',\n",
       " 'seria',\n",
       " 'seriam',\n",
       " 'ser√≠amos',\n",
       " 'seu',\n",
       " 'seus',\n",
       " 's√≥',\n",
       " 'somos',\n",
       " 'sou',\n",
       " 'sua',\n",
       " 'suas',\n",
       " 'tamb√©m',\n",
       " 'te',\n",
       " 'tem',\n",
       " 't√©m',\n",
       " 'temos',\n",
       " 'tenha',\n",
       " 'tenham',\n",
       " 'tenhamos',\n",
       " 'tenho',\n",
       " 'ter√°',\n",
       " 'ter√£o',\n",
       " 'terei',\n",
       " 'teremos',\n",
       " 'teria',\n",
       " 'teriam',\n",
       " 'ter√≠amos',\n",
       " 'teu',\n",
       " 'teus',\n",
       " 'teve',\n",
       " 'tinha',\n",
       " 'tinham',\n",
       " 't√≠nhamos',\n",
       " 'tive',\n",
       " 'tivemos',\n",
       " 'tiver',\n",
       " 'tivera',\n",
       " 'tiveram',\n",
       " 'tiv√©ramos',\n",
       " 'tiverem',\n",
       " 'tivermos',\n",
       " 'tivesse',\n",
       " 'tivessem',\n",
       " 'tiv√©ssemos',\n",
       " 'tu',\n",
       " 'tua',\n",
       " 'tuas',\n",
       " 'um',\n",
       " 'uma',\n",
       " 'voc√™',\n",
       " 'voc√™s',\n",
       " 'vos']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords_port"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A9Dpg-8ApIxa"
   },
   "source": [
    "Vamos agora aplicar a remo√ß√£o de Stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "CvrcEJsQo_tf"
   },
   "outputs": [],
   "source": [
    "example = [\"my\", \"house\", \"is\", \"black\", \"and\", \"white\", \"but\", \"isn't\", \"big\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tUaavlQhtf8g",
    "outputId": "ae132438-d8f4-43bc-9180-1f4904656d34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print('my' in stopwords_en)\n",
    "print('house' in stopwords_en)\n",
    "print('is' in stopwords_en)\n",
    "print('black' in stopwords_en)\n",
    "print('and' in stopwords_en)\n",
    "print('white' in stopwords_en)\n",
    "print('but' in stopwords_en)\n",
    "print(\"isn't\" in stopwords_en)\n",
    "print(\"big\" in stopwords_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wnYz481KOjkM"
   },
   "source": [
    "### Limpeza do Texto\n",
    "\n",
    "Existem alguns cuidados com rela√ß√£o a grafia das palavras e elementos em um texto que devemos tomar bastante cuidado antes de fazer qualquer outra coisa. Esses pontos s√£o:<br><br>\n",
    "- Transformar todas as palavras para MAI√öSCULAS ou min√∫sculas;\n",
    "- Remover caracteres especiais;\n",
    "- Remover d√≠gitos (quando n√£o forem relevantes);\n",
    "- Remover acentua√ß√£o (caso t√≠pico de quando trabalhamos com textos em Portugu√™s);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "farmac√™utico\n",
      "relogio\n",
      "parada\n"
     ]
    }
   ],
   "source": [
    "words = ['FaRMac√™uTIco', 'relogio', 'Parada']\n",
    "for word in words:\n",
    "    print(word.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HYRTjdWiTKWu"
   },
   "source": [
    "### Remo√ß√£o de d√≠gitos, caracteres especiais e qualquer outro item que n√£o queremos no texto\n",
    "\n",
    "Para essa etapa do processo, iremos utilizar uma biblioteca auxiliar [RegEx (Regular Expression)](https://docs.python.org/3/library/re.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "iE4HC6guTNGR"
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2n2D1Od-TVVx"
   },
   "source": [
    "Importada a biblioteca, vamos utilizar a fun√ß√£o *re.sub*, para substituir os elementos que n√£o queremos nos nossos textos:\n",
    "\n",
    "**Procure por re.sub**  \n",
    "https://docs.python.org/3/library/re.html\n",
    "\n",
    "Obs: Utilize o site https://regex101.com/ para criar o regex e o re.sub para substituir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto Original:  S√£o mais de 50 cursos em Data Science!!! #datascience #machinelearning\n",
      "Texto sem n√∫meros S√£o mais de  cursos em Data Science!!! #datascience #machinelearning\n"
     ]
    }
   ],
   "source": [
    "frase = 'S√£o mais de 50 cursos em Data Science!!! #datascience #machinelearning'\n",
    "\n",
    "# Mostra o texto original\n",
    "print('Texto Original: ', frase)\n",
    "\n",
    "# Removendo n√∫meros\n",
    "frase = re.sub(r'\\d', '', frase)\n",
    "print('Texto sem n√∫meros', frase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto sem os caracteres especiais:  S o mais de cursos em Data Science datascience machinelearning\n"
     ]
    }
   ],
   "source": [
    "# Texto agora sem caracteres especiais tamb√©m\n",
    "frase = re.sub(r\"[^a-zA-Z]+\", ' ', frase)\n",
    "\n",
    "# Mostra o texto modificado\n",
    "print('Texto sem os caracteres especiais: ', frase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zuqfqkjuabuH"
   },
   "source": [
    "Utilizem a documenta√ß√£o para descobrir mais c√≥digos para filtrar elementos ou mesmo deem uma olhada nesse artigo, que resume de uma forma bem visual as aplica√ß√µes do RegEx: [clique aqui](https://amitness.com/regex/)\n",
    "\n",
    "Tutorial de regex https://blog.geekhunter.com.br/python-regex/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HeoTPv_4agE2"
   },
   "source": [
    "Hoje os emojis fazem parte da comunica√ß√£o via mensagens, por isso iremos ver como utilizar frases contento emojis e trata-los de forma adequada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "fCZwZhJLaTnj"
   },
   "outputs": [],
   "source": [
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vb4EGxBSatqv",
    "outputId": "975e4fba-e585-4cdd-ba61-c821277f8464"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python is üëç\n"
     ]
    }
   ],
   "source": [
    "texto = 'Python is :thumbs_up:'\n",
    "print(emoji.emojize(texto))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "AcRH6cAZbONN",
    "outputId": "d94993a9-948f-4f52-c001-4cfea0fa8d71"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Python is :thumbs_up:'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texto_com_emoji = 'Python is üëç'\n",
    "emoji.demojize(texto_com_emoji)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c9HCV1QKbjXG"
   },
   "source": [
    "Para ver a lista completa:\n",
    "\n",
    "https://www.webfx.com/tools/emoji-cheat-sheet/\n",
    "\n",
    "Link biblioteca: https://github.com/carpedm20/emoji/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 1\n",
    "Utilizar Regez para validar CPFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = r\"^(\\d{3}.){2}\\d{3}-\\d{2}$\"\n",
    "bool(re.match(pattern, '097.021.074-40'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZap5orRdLUE"
   },
   "source": [
    "### Exerc√≠cio 2\n",
    "\n",
    "Utilizar Regex para valida√ß√£o de emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email = 'cflavs.7@gmail.com'\n",
    "pattern = r\"\\\"?([-a-zA-Z0-9.`?{}]+@\\w+\\.\\w+)\\\"?\"  \n",
    "patterns = re.compile(pattern)\n",
    "bool(re.match(patterns, email))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "laoYA1wYg6Yj"
   },
   "source": [
    "Podemos utilizar NLP para identificar frases que contenham uma palavra chave!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['People are fighting with covid these days', ' How will we survive covid']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example=\"People are fighting with covid these days. Economy has fallen down. How will we survive covid\" \n",
    "keyword = 'covid'\n",
    "line=re.findall(r'([^.]*'+keyword+'[^.]*)', example)\n",
    "line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K5f4aJfl7QCC"
   },
   "source": [
    "### Remo√ß√£o de Acentua√ß√£o\n",
    "Para a remo√ß√£o de acentua√ß√£o, iremos utilizar uma bibloteca chamada *Unidecode*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a7hpJwTa7SIY",
    "outputId": "066869e7-ca3f-45b3-9b60-c6598f820639"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: unidecode in /usr/local/lib/python3.7/dist-packages (1.3.4)\n"
     ]
    }
   ],
   "source": [
    "# Caso precise instalar a biblioteca, descomente o c√≥digo abaixo\n",
    "!pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8eLSKg7A7TS5"
   },
   "outputs": [],
   "source": [
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GFxZKdkJ7VHI",
    "outputId": "49354abb-e88d-4efc-fded-37da7d157852"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jo√£o SeBasti√£o Alvar√° Vov√¥ Linguin√ßa Express√£o\n",
      "Joao SeBastiao Alvara Vovo Linguinca Expressao\n"
     ]
    }
   ],
   "source": [
    "string = 'Jo√£o SeBasti√£o Alvar√° Vov√¥ Linguin√ßa Express√£o'\n",
    "print(string)\n",
    "\n",
    "string = unidecode(string)\n",
    "print(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HcoeEmpC9Kbc"
   },
   "source": [
    "## Tokeniza√ß√£o\n",
    "\n",
    "Tokeniza√ß√£o √© um processo onde transformamos um texto de uma string √∫nica em fragmentos desse texto na forma de *tokens*, que nada mais s√£o do que as pr√≥prias palavras! Para isso, vamos utilizar a fun√ß√£o *word_tokenize* do NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ucLVo_6j9L9x"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BOwuFw8Q9O2s",
    "outputId": "f0a813b6-fd08-44bd-c3e8-fb278a683f2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A frase original √© O rato roeu a roupa do rei de roma.\n",
      "Os tokens s√£o ['O', 'rato', 'roeu', 'a', 'roupa', 'do', 'rei', 'de', 'roma', '.']\n",
      "Com split ['O', 'rato', 'roeu', 'a', 'roupa', 'do', 'rei', 'de', 'roma.']\n"
     ]
    }
   ],
   "source": [
    "string = 'O rato roeu a roupa do rei de roma.'\n",
    "words = word_tokenize(string)\n",
    "\n",
    "print('A frase original √©', string)\n",
    "print('Os tokens s√£o', words)\n",
    "print('Com split', string.split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4qed6o8G9Vib"
   },
   "source": [
    "## Normaliza√ß√£o de Textos\n",
    "\n",
    "**Normaliza√ß√£o de Textos (Text Normalization)** √© o procedimento que consiste em **padronizar** o texto, de modo a evitar que varia√ß√µes tornem os modelos demasiadamente complexos. Por exemplo: tratar singular/plural como a mesma coisa, ou ent√£o eliminar conjuga√ß√£o de verbos. Outras componentes comuns da normaliza√ß√£o s√£o a de eliminar palavras que n√£o agregam muito significado, ou palavras muito raras.\n",
    "\n",
    "Abaixo alguns exemplos de a√ß√µes de Text Normalization que podem ser aplicadas no pr√©-processamento de dados textuais:\n",
    "\n",
    "**Stemming** - Redu√ß√£o de tokens √† sua raiz invariante atrav√©s da **remo√ß√£o de prefixos ou sufixos**. Baseado em heur√≠stica<br>\n",
    "**Lemmatization** - Redu√ß√£o de tokens √† sua raiz invariante atrav√©s da **an√°lise lingu√≠stica do token**. Baseado em dicion√°rio l√©xico<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ieG5enJ79aUg"
   },
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "jA-tVw5S9RGm"
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import *\n",
    "from nltk.stem import RSLPStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZSZhPhGP9cqF",
    "outputId": "7781f8e5-82b0-4df6-a32f-45558f9c6ee7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "say\n",
      "write\n",
      "run\n",
      "ate\n",
      "work\n"
     ]
    }
   ],
   "source": [
    "# Inicializando o Porter Stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Criando uma lista de palavras\n",
    "words = ['saying', 'writing', 'running', 'ate', 'worked']\n",
    "\n",
    "# Criando uma lista vazia para armazenar as palavras stem\n",
    "stem_words = []\n",
    "\n",
    "# Percorrendo a lista de palavras\n",
    "for w in words:\n",
    "    print(stemmer.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jf0ggw2I9qFx",
    "outputId": "403ce0b5-cf64-4c3a-851a-e470cd236760"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corr\n",
      "corr\n"
     ]
    }
   ],
   "source": [
    "# Criando uma lista de palavras em portugu√™s\n",
    "words_port = ['correndo', 'corra']\n",
    "\n",
    "# Inicializando o Porter Stem para portugu√™s (RSLPStemmer)\n",
    "stemmer_port = RSLPStemmer()\n",
    "\n",
    "# Percorrendo a lista de palavras\n",
    "for w in words_port:\n",
    "    print(stemmer_port.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m spacy download \"pt_core_news_sm\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"pt_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(str([palavra for palavra in words_port]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['corra']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.lemma_ for token in doc if token.pos_ == 'NOUN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/cecilia/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/cecilia/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importando o Lemmatizer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "# https://www.nltk.org/howto/wordnet.html\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# NLTK 3.6.6 release: December 2021:\n",
    "# support OMW 1.4, use Multilingual Wordnet Data from OMW with newer Wordnet versions\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializando o Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JaqVvMR-_cfF",
    "outputId": "8cde4c03-ab37-4e2a-f98e-4c12b0fecd91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rocks : rock\n",
      "corpora : corpus\n",
      "running : run\n",
      "went : go\n"
     ]
    }
   ],
   "source": [
    "# Exemplos simples\n",
    "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\")) \n",
    "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\")) \n",
    "\n",
    "# argumento \"pos\" indica a qual classe gramatical o token pertence\n",
    "print(\"running :\", lemmatizer.lemmatize(\"running\", pos = \"v\")) \n",
    "print(\"went :\", lemmatizer.lemmatize(\"went\", pos = 'v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS - Parts of Speech\n",
    "\n",
    " Part-of_Speech Tagging √© uma ferramenta auxiliar de alguns modelos mais complexos de NLP, que ajuda na identifica√ß√£o dos elementos gramaticais dentro de uma frase. Ou seja, o pos_tag identifica dentro de um texto quais elementos s√£o verbos, sujeitos, pronomes e assim por diante. A Forma de aplicar o Part-of_Speech Tagging ser√° utilizando a fun√ß√£o pos_tag do NLTK\n",
    " \n",
    " https://www.guru99.com/pos-tagging-chunking-nltk.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palavras:  ['learning', 'data', 'science', 'is', 'one', 'of', 'the', 'best', 'things']\n",
      "POS Tagging:  [('learning', 'VBG'), ('data', 'NNS'), ('science', 'NN'), ('is', 'VBZ'), ('one', 'CD'), ('of', 'IN'), ('the', 'DT'), ('best', 'JJS'), ('things', 'NNS')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/cecilia/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Baixa um estimador de tags\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Cria uma frase em tokens como exemplo\n",
    "words = ['learning', 'data', 'science', 'is', 'one', 'of', 'the', 'best', 'things']\n",
    "\n",
    "# Cria as tags\n",
    "tagged = pos_tag(words)\n",
    "\n",
    "# Mostra os resultados\n",
    "print('Palavras: ', words)\n",
    "print('POS Tagging: ', tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition (NER)\n",
    "\n",
    "- Identifica nomes de entidades em textos (pessoas, locais, organiza√ß√µes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NASA', 'NNP'), ('awarded', 'VBD'), ('Elon', 'NNP'), ('Musk', 'NNP'), ('‚Äô', 'NNP'), ('s', 'VBD'), ('SpaceX', 'NNP'), ('a', 'DT'), ('$', '$'), ('2.9', 'CD'), ('billion', 'CD'), ('contract', 'NN'), ('to', 'TO'), ('build', 'VB'), ('the', 'DT'), ('lunar', 'NN'), ('lander', 'NN'), ('.', '.')]\n",
      "(S\n",
      "  (ORGANIZATION NASA/NNP)\n",
      "  awarded/VBD\n",
      "  (PERSON Elon/NNP Musk/NNP)\n",
      "  ‚Äô/NNP\n",
      "  s/VBD\n",
      "  (ORGANIZATION SpaceX/NNP)\n",
      "  a/DT\n",
      "  $/$\n",
      "  2.9/CD\n",
      "  billion/CD\n",
      "  contract/NN\n",
      "  to/TO\n",
      "  build/VB\n",
      "  the/DT\n",
      "  lunar/NN\n",
      "  lander/NN\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "text = \"NASA awarded Elon Musk‚Äôs SpaceX a $2.9 billion contract to build the lunar lander.\"\n",
    "tokens = word_tokenize(text)\n",
    "tag=nltk.pos_tag(tokens)\n",
    "print(tag)\n",
    "\n",
    "ne_tree = nltk.ne_chunk(tag)\n",
    "print(ne_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YxH2uhI4A8mq"
   },
   "source": [
    "## Pipeline de Processamento de Textos\n",
    "\n",
    "Conhecendo todos os tipos de processamentos que podemos utilizar, uma forma √∫til e organizada para isso √© construirmos uma fun√ß√£i que receba o nosso dados originais e realizada todos os processamentos que queremos nos textos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S4qLqa46BBUi"
   },
   "source": [
    "### Exerc√≠cio\n",
    "\n",
    "Crie uma classe que seja capaz de:\n",
    "\n",
    "- Metodo para remover acentua√ß√£o\n",
    "- Metodo de remover digitos\n",
    "- Metodo de remover caracteres especiais\n",
    "- Metodo de normalizar o texto em caixa baixa\n",
    "- Metodo para criar os tokens\n",
    "- Metodo para filtrar stopwords\n",
    "- Metodo para pegar o stemming\n",
    "- Metodo para pegar o lemma\n",
    "- Metodo pipeline respons√°vel por chamar os preprocessamentos necess√°rios\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcesssPhrase:\n",
    "\n",
    "  def remove_acentuacao(self, phrase: str, debug: bool = False) -> str:\n",
    "    # Utilizando a biblioteca unidecode para remover acentua√ß√£o de texto\n",
    "    phrase_fmt = unidecode(phrase)\n",
    "    if debug:\n",
    "      print('`remove_acentuacao`: Frase original', phrase)\n",
    "      print('`remove_acentuacao`: Frase formatada', phrase_fmt)\n",
    "\n",
    "    # Retornando a frase formatada\n",
    "    return phrase_fmt\n",
    "\n",
    "\n",
    "  def remove_digits(self, phrase: str, debug: bool = False) -> str:\n",
    "    # utilizando express√µes regulares para remo√ß√£o de digitos\n",
    "    phrase_no_digits = re.sub(r'\\d', '', phrase)\n",
    "\n",
    "    # Se quisermos ligar o debug, mostre a frase original e transformada\n",
    "    if debug:\n",
    "      print('`remove_digits`: Texto original:', phrase)\n",
    "      print('`remove_digits`: Texto sem digitos:', phrase_no_digits)\n",
    "\n",
    "    # Retornando a frase sem digitos\n",
    "    return phrase_no_digits\n",
    "\n",
    "  def remove_special_char(self, phrase: str, debug: bool = False) -> str:\n",
    "    # utilizando express√µes regulares para remo√ß√£o de caracteres especiais\n",
    "    phrase_no_special_char = re.sub(r'[^a-zA-Z0-9]+', ' ', phrase)\n",
    "\n",
    "    # Se quisermos ligar o debug, mostre a frase original e transformada\n",
    "    if debug:\n",
    "      print('`remove_special_char`: Texto original:', phrase)\n",
    "      print('`remove_special_char`: Texto sem caracteres especiais:', phrase_no_special_char)\n",
    "\n",
    "    # Retornando a frase sem digitos\n",
    "    return phrase_no_special_char\n",
    "\n",
    "  def word_lower(self, word: str, debug: bool = False) -> str:\n",
    "    try:\n",
    "      # Formatando a palavra em caixa baixa\n",
    "      word_fmt = word.lower()\n",
    "\n",
    "      # Se o debug for True, iremos imprimir a palavra original e transformada\n",
    "      if debug:\n",
    "        print('`word_lower`: Palavra Original:', word)\n",
    "        print('`word_lower`: Palavra transformada:', word_fmt)\n",
    "\n",
    "    except:\n",
    "      # Caso a palavra n√£o seja uma string levante um erro (TypeError) informando qual o tipo da palavra passada\n",
    "      raise TypeError(f'Esperava uma `word` no tipo str, foi passado uma {type(word)}')\n",
    "\n",
    "    # Retornando a palavra formatada\n",
    "    return word_fmt\n",
    "\n",
    "  def remove_stopwords(self, words: list[str], debug=False) -> list[str]:\n",
    "    # Carregando as stopwords (ingl√™s)\n",
    "    stopwords_en = stopwords.words('english')\n",
    "    # Criando uma v√°riavel que ir√° armazenar elementos limpos, que n√£o estejam dentro das stopwords\n",
    "    clean_words = []\n",
    "\n",
    "    # Percorrendo cada palavra da nossa lista de palavras\n",
    "    for word in words:\n",
    "      # Verificando se a palavra n√£o est√° presente das stopwords\n",
    "      if word not in stopwords_en:\n",
    "        # Se a palavra n√£o √© uma stopword adicionamos elas a v√°riavel clean_words\n",
    "        clean_words.append(word)\n",
    "      else:\n",
    "        # Caso a palavra seja uma stopword e estamos no modo de debug (debug=True)\n",
    "        if debug:\n",
    "          # Imprimimos qual a palavra da lista words √© uma stopword\n",
    "          print(f'`remove_stopwords`: A palavra {word} est√° presente nas stopwords')\n",
    "    return clean_words\n",
    "\n",
    "  def tokenizer(self, phrase: str, debug: bool) -> list[str]:\n",
    "    words = word_tokenize(phrase)\n",
    "    if debug:\n",
    "        print('`tokenizer`: Frase original:', phrase)\n",
    "        print('`tokenizer`: tokens:', words)\n",
    "    return words\n",
    "\n",
    "  def stemmer(self, words: list[str], debug: bool = False) -> list[str]:\n",
    "    # Inicializando o Porter Stemmer (ingl√™s)\n",
    "    stemmer = PorterStemmer()\n",
    "    # Criando uma lista vazia para armazenar as palavras stem\n",
    "    stem_words = []\n",
    "    for word in words:\n",
    "      # Pegando o stem de cada palavra\n",
    "      s_word = stemmer.stem(word)\n",
    "      # Adicionando essa palavra modificada a lista stem_words\n",
    "      stem_words.append(s_word)\n",
    "\n",
    "    if debug:\n",
    "        print('`stemmer`: Tokens originais:', words)\n",
    "        print('`stemmer`: Tokens transformadods:', stem_words)\n",
    "    return stem_words\n",
    "\n",
    "  def lemmatizer(self, words: list[str], debug: bool = False) -> list[str]:\n",
    "    # Inicializando o WordNetLemmatizer (ingl√™s)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # Criando uma lista vazia para armazenar as palavras lemma\n",
    "    lemm_words = []\n",
    "    for word in words:\n",
    "      # Pegando o lemma de cada palavra\n",
    "      l_words = lemmatizer.lemmatize(word, pos='v')\n",
    "      # Adicionando essa palavra modificada (lemma) a lista lemm_words\n",
    "      lemm_words.append(l_words)\n",
    "\n",
    "    if debug:\n",
    "        print('`stemmer`: Tokens originais:', words)\n",
    "        print('`stemmer`: Tokens transformadods:', lemm_words)\n",
    "    return stem_words\n",
    "\n",
    "\n",
    "  def pipeline(self, phrase: str, methods: list[str], debug: bool = False):\n",
    "    switcher = {\n",
    "        'remove_acentuacao': self.remove_acentuacao,\n",
    "        'remove_digits': self.remove_digits,\n",
    "        'remove_special_char': self.remove_special_char,\n",
    "        'word_lower': self.word_lower,\n",
    "        'remove_stopwords': self.remove_stopwords,\n",
    "        'tokenizer': self.tokenizer,\n",
    "        'stemmer': self.stemmer,\n",
    "        'lemmatizer': self.lemmatizer\n",
    "    }\n",
    "    for method in methods:\n",
    "      # remove_stopwords\n",
    "      if method == 'remove_stopwords':\n",
    "        phrase = switcher[method](phrase, debug=debug)\n",
    "        \n",
    "      else:\n",
    "        phrase = switcher[method](phrase, debug=debug)\n",
    "\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "brOZjdj3Cp96"
   },
   "outputs": [],
   "source": [
    "phrase = 'I grew up (b. 1965) watching and loving √ß'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "CsFClqwMHbvb"
   },
   "outputs": [],
   "source": [
    "preprocess = PreProcesssPhrase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "dDWoIbbJHfKz",
    "outputId": "fab1e06c-4e69-4561-b041-53ba794f3845"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`remove_digits`: Texto original: I grew up (b. 1965) watching and loving √ß\n",
      "`remove_digits`: Texto sem digitos: I grew up (b. ) watching and loving √ß\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I grew up (b. ) watching and loving √ß'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'remove_acentuacao'\n",
    "# 'remove_digits'\n",
    "# 'remove_special_char'\n",
    "# 'word_lower'\n",
    "# 'remove_stopwords'\n",
    "# 'tokenizer'\n",
    "# 'stemmer'\n",
    "# 'lemmatizer'\n",
    "pipeline = ['remove_digits']\n",
    "# Removendo apenas os digitos com o pipeline\n",
    "preprocess.pipeline(phrase, methods=pipeline, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "id": "eq1dJuERHw7_",
    "outputId": "103000a9-0b0f-4823-b71f-cca8bfedadb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`remove_digits`: Texto original: I grew up (b. 1965) watching and loving √ß\n",
      "`remove_digits`: Texto sem digitos: I grew up (b. ) watching and loving √ß\n",
      "`remove_special_char`: Texto original: I grew up (b. ) watching and loving √ß\n",
      "`remove_special_char`: Texto sem caracteres especiais: I grew up b watching and loving \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I grew up b watching and loving '"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = ['remove_digits', \"remove_special_char\"]\n",
    "# Removendo digitos e caracteres especiais com o pipeline\n",
    "preprocess.pipeline(phrase, methods=pipeline, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VkU2HCMLILSn",
    "outputId": "229fe71b-ccf3-4a9d-a3ce-0a44b2170280"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`remove_digits`: Texto original: I grew up (b. 1965) watching and loving √ß\n",
      "`remove_digits`: Texto sem digitos: I grew up (b. ) watching and loving √ß\n",
      "`tokenizer`: Frase original: I grew up (b. ) watching and loving √ß\n",
      "`tokenizer`: tokens: ['I', 'grew', 'up', '(', 'b', '.', ')', 'watching', 'and', 'loving', '√ß']\n",
      "`remove_stopwords`: A palavra up est√° presente nas stopwords\n",
      "`remove_stopwords`: A palavra and est√° presente nas stopwords\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['I', 'grew', '(', 'b', '.', ')', 'watching', 'loving', '√ß']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = ['remove_digits', 'tokenizer', \"remove_stopwords\"]\n",
    "# Removendo digitos, criando tokens e removendo stopwords com o pipeline\n",
    "preprocess.pipeline(phrase, methods=pipeline, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MhKiVPx5IiLf",
    "outputId": "b43b381c-81dd-4a19-d42e-525d5cc399bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`remove_digits`: Texto original: I grew up (b. 1965) watching and loving √ß\n",
      "`remove_digits`: Texto sem digitos: I grew up (b. ) watching and loving √ß\n",
      "`remove_special_char`: Texto original: I grew up (b. ) watching and loving √ß\n",
      "`remove_special_char`: Texto sem caracteres especiais: I grew up b watching and loving \n",
      "`word_lower`: Palavra Original: I grew up b watching and loving \n",
      "`word_lower`: Palavra transformada: i grew up b watching and loving \n",
      "`tokenizer`: Frase original: i grew up b watching and loving \n",
      "`tokenizer`: tokens: ['i', 'grew', 'up', 'b', 'watching', 'and', 'loving']\n",
      "`remove_stopwords`: A palavra i est√° presente nas stopwords\n",
      "`remove_stopwords`: A palavra up est√° presente nas stopwords\n",
      "`remove_stopwords`: A palavra and est√° presente nas stopwords\n",
      "`stemmer`: Tokens originais: ['grew', 'b', 'watching', 'loving']\n",
      "`stemmer`: Tokens transformadods: ['grew', 'b', 'watch', 'love']\n"
     ]
    }
   ],
   "source": [
    "# 'remove_acentuacao'\n",
    "# 'remove_digits'\n",
    "# 'remove_special_char'\n",
    "# 'word_lower'\n",
    "# 'remove_stopwords'\n",
    "# 'tokenizer'\n",
    "# 'stemmer'\n",
    "# 'lemmatizer'\n",
    "pipeline = [\n",
    "    'remove_digits', 'remove_special_char', 'word_lower', 'tokenizer',\n",
    "    'remove_stopwords', 'stemmer'\n",
    "]\n",
    "phrase_proc = preprocess.pipeline(phrase, methods=pipeline, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5PACJIBDJJ3O",
    "outputId": "f13b35aa-5308-4f93-d453-30a144cf4167"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['grew', 'b', 'watch', 'love']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase_proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "xOfCHWPENHWc"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I grew up (b. 1965) watching and loving the Th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When I put this movie in my DVD player, and sa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Why do people who do not know what a particula...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Even though I have great interest in Biblical ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Im a die hard Dads Army fan and nothing will e...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>This is the kind of picture John Lassiter woul...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>A MUST SEE! I saw WHIPPED at a press screening...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>NBC should be ashamed. I wouldn't allow my chi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>This movie is a clumsy mishmash of various gho...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>Formula movie about the illegitimate son of a ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label\n",
       "0     I grew up (b. 1965) watching and loving the Th...      0\n",
       "1     When I put this movie in my DVD player, and sa...      0\n",
       "2     Why do people who do not know what a particula...      0\n",
       "3     Even though I have great interest in Biblical ...      0\n",
       "4     Im a die hard Dads Army fan and nothing will e...      1\n",
       "...                                                 ...    ...\n",
       "4995  This is the kind of picture John Lassiter woul...      1\n",
       "4996  A MUST SEE! I saw WHIPPED at a press screening...      1\n",
       "4997  NBC should be ashamed. I wouldn't allow my chi...      0\n",
       "4998  This movie is a clumsy mishmash of various gho...      0\n",
       "4999  Formula movie about the illegitimate son of a ...      0\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies = pd.read_csv('movies.csv', index_col=0)\n",
    "movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_sample = movies.sample(frac=0.1, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = PreProcesssPhrase()\n",
    "pipeline = [\n",
    "    'remove_digits',\n",
    "    'remove_special_char',\n",
    "    'word_lower',\n",
    "    'tokenizer',\n",
    "    'remove_stopwords',\n",
    "    'stemmer'\n",
    "]\n",
    "movies_sample[\"filtered_words\"] = movies_sample['text'].apply(preprocess.pipeline, methods=pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32214    [movi, good, concept, execut, live, br, br, co...\n",
       "21823    [film, aw, screenplay, bad, script, mediocr, e...\n",
       "35693    [ever, tri, kind, food, friend, made, said, wo...\n",
       "8705     [first, time, ever, saw, movi, jami, foxx, bet...\n",
       "14439    [dread, bore, movi, even, documentari, time, p...\n",
       "                               ...                        \n",
       "17007    [also, known, stairway, heaven, us, wwii, brit...\n",
       "12073    [rememb, view, movi, kid, recal, terrifi, imme...\n",
       "2789     [think, polic, alway, right, br, br, believ, e...\n",
       "11400    [rais, canada, saw, short, mani, time, mostli,...\n",
       "17223    [yet, quit, bad, enough, make, enjoy, fact, on...\n",
       "Name: filtered_words, Length: 5000, dtype: object"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_sample['filtered_words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalmente depois do processamento juntamos as palavras novamente em uma √∫nica string\n",
    "movies_sample['join_words'] = movies_sample['filtered_words'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32214    movi good concept execut live br br concept we...\n",
       "21823    film aw screenplay bad script mediocr even sex...\n",
       "35693    ever tri kind food friend made said wow good m...\n",
       "8705     first time ever saw movi jami foxx bet last fa...\n",
       "14439    dread bore movi even documentari time provid i...\n",
       "                               ...                        \n",
       "17007    also known stairway heaven us wwii british pet...\n",
       "12073    rememb view movi kid recal terrifi immens stay...\n",
       "2789     think polic alway right br br believ eye wit a...\n",
       "11400    rais canada saw short mani time mostli cbc see...\n",
       "17223    yet quit bad enough make enjoy fact one bore r...\n",
       "Name: join_words, Length: 5000, dtype: object"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_sample['join_words']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analisando vocabul√°rio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = pd.DataFrame(\n",
    "    np.concatenate(movies_sample['filtered_words'].values),\n",
    "    columns=['word']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "word\n",
       "aa              1\n",
       "lour            1\n",
       "louri           1\n",
       "lousiest        1\n",
       "loutish         1\n",
       "            ...  \n",
       "like         4527\n",
       "one          5444\n",
       "film         9718\n",
       "movi        10077\n",
       "br          19984\n",
       "Length: 25951, dtype: int64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary.groupby('word').size().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aa': 1,\n",
       " 'lour': 1,\n",
       " 'louri': 1,\n",
       " 'lousiest': 1,\n",
       " 'loutish': 1,\n",
       " 'louwyck': 1,\n",
       " 'lovel': 1,\n",
       " 'loveli': 1,\n",
       " 'loveliest': 1,\n",
       " 'lovestruck': 1,\n",
       " 'lovin': 1,\n",
       " 'lovitz': 1,\n",
       " 'louisianna': 1,\n",
       " 'lovomaniac': 1,\n",
       " 'lowlif': 1,\n",
       " 'lowood': 1,\n",
       " 'lowpoint': 1,\n",
       " 'loyalist': 1,\n",
       " 'lp': 1,\n",
       " 'lq': 1,\n",
       " 'lr': 1,\n",
       " 'ltd': 1,\n",
       " 'luana': 1,\n",
       " 'lubric': 1,\n",
       " 'lucian': 1,\n",
       " 'loweri': 1,\n",
       " 'luciano': 1,\n",
       " 'loudspeak': 1,\n",
       " 'lotta': 1,\n",
       " 'longingli': 1,\n",
       " 'longinotto': 1,\n",
       " 'longleg': 1,\n",
       " 'longshank': 1,\n",
       " 'longtim': 1,\n",
       " 'longueur': 1,\n",
       " 'longwind': 1,\n",
       " 'lonli': 1,\n",
       " 'lonni': 1,\n",
       " 'lookin': 1,\n",
       " 'loompa': 1,\n",
       " 'lotti': 1,\n",
       " 'loonivers': 1,\n",
       " 'loooong': 1,\n",
       " 'loooooov': 1,\n",
       " 'looser': 1,\n",
       " 'lop': 1,\n",
       " 'lordi': 1,\n",
       " 'lordli': 1,\n",
       " 'lorean': 1,\n",
       " 'lorri': 1,\n",
       " 'losthorizon': 1,\n",
       " 'loth': 1,\n",
       " 'lotsa': 1,\n",
       " 'looong': 1,\n",
       " 'longev': 1,\n",
       " 'luckett': 1,\n",
       " 'luddit': 1,\n",
       " 'lyndsay': 1,\n",
       " 'lynley': 1,\n",
       " 'lynx': 1,\n",
       " 'lyta': 1,\n",
       " 'maaan': 1,\n",
       " 'mab': 1,\n",
       " 'mabus': 1,\n",
       " 'macarena': 1,\n",
       " 'macbrid': 1,\n",
       " 'macca': 1,\n",
       " 'macdougal': 1,\n",
       " 'lynda': 1,\n",
       " 'mace': 1,\n",
       " 'macedonian': 1,\n",
       " 'macfadyen': 1,\n",
       " 'macfarlen': 1,\n",
       " 'macfayden': 1,\n",
       " 'macgrud': 1,\n",
       " 'macgubbin': 1,\n",
       " 'macguffin': 1,\n",
       " 'machismo': 1,\n",
       " 'macht': 1,\n",
       " 'macintosh': 1,\n",
       " 'macist': 1,\n",
       " 'macedonia': 1,\n",
       " 'luckless': 1,\n",
       " 'lynd': 1,\n",
       " 'lynchian': 1,\n",
       " 'ludwig': 1,\n",
       " 'luftwaff': 1,\n",
       " 'lug': 1,\n",
       " 'lugaci': 1,\n",
       " 'lugubri': 1,\n",
       " 'luhrman': 1,\n",
       " 'lul': 1,\n",
       " 'lullabi': 1,\n",
       " 'lulli': 1,\n",
       " 'luminari': 1,\n",
       " 'lumpen': 1,\n",
       " 'lynchpin': 1,\n",
       " 'lumpiest': 1,\n",
       " 'lupu': 1,\n",
       " 'luridli': 1,\n",
       " 'lusti': 1,\n",
       " 'luthor': 1,\n",
       " 'lutt': 1,\n",
       " 'lutz': 1,\n",
       " 'luzhini': 1,\n",
       " 'ly': 1,\n",
       " 'lycanthrop': 1,\n",
       " 'lycra': 1,\n",
       " 'lydia': 1,\n",
       " 'lundi': 1,\n",
       " 'macivor': 1,\n",
       " 'longbourn': 1,\n",
       " 'lonelyheart': 1,\n",
       " 'lilyan': 1,\n",
       " 'lim': 1,\n",
       " 'lima': 1,\n",
       " 'limber': 1,\n",
       " 'limbless': 1,\n",
       " 'limburg': 1,\n",
       " 'limey': 1,\n",
       " 'limitless': 1,\n",
       " 'limousin': 1,\n",
       " 'lindberg': 1,\n",
       " 'lindfor': 1,\n",
       " 'lilliputian': 1,\n",
       " 'lindoi': 1,\n",
       " 'lindsley': 1,\n",
       " 'lindstrom': 1,\n",
       " 'linehan': 1,\n",
       " 'linek': 1,\n",
       " 'lineman': 1,\n",
       " 'linen': 1,\n",
       " 'linesmen': 1,\n",
       " 'lineup': 1,\n",
       " 'lingo': 1,\n",
       " 'linguist': 1,\n",
       " 'linkag': 1,\n",
       " 'lindsey': 1,\n",
       " 'linnet': 1,\n",
       " 'lilith': 1,\n",
       " 'likemind': 1,\n",
       " 'liassez': 1,\n",
       " 'libatiqu': 1,\n",
       " 'liberac': 1,\n",
       " 'libido': 1,\n",
       " 'licht': 1,\n",
       " 'licoln': 1,\n",
       " 'liddl': 1,\n",
       " 'lidia': 1,\n",
       " 'lido': 1,\n",
       " 'lidsvil': 1,\n",
       " 'lieb': 1,\n",
       " 'lilian': 1,\n",
       " 'liebman': 1,\n",
       " 'lien': 1,\n",
       " 'lieutent': 1,\n",
       " 'lifeblood': 1,\n",
       " 'lifeguard': 1,\n",
       " 'lifford': 1,\n",
       " 'lightbulb': 1,\n",
       " 'lightheartedli': 1,\n",
       " 'lighthorseman': 1,\n",
       " 'lightner': 1,\n",
       " 'lightol': 1,\n",
       " 'likeli': 1,\n",
       " 'liebowitz': 1,\n",
       " 'lonett': 1,\n",
       " 'linsey': 1,\n",
       " 'linz': 1,\n",
       " 'lob': 1,\n",
       " 'lobisomen': 1,\n",
       " 'loblolli': 1,\n",
       " 'lobotom': 1,\n",
       " 'lobotomis': 1,\n",
       " 'lochari': 1,\n",
       " 'locker': 1,\n",
       " 'lockhe': 1,\n",
       " 'lockley': 1,\n",
       " 'lockwood': 1,\n",
       " 'lockyer': 1,\n",
       " 'lmao': 1,\n",
       " 'loco': 1,\n",
       " 'locust': 1,\n",
       " 'loder': 1,\n",
       " 'loesser': 1,\n",
       " 'loggia': 1,\n",
       " 'lohan': 1,\n",
       " 'lok': 1,\n",
       " 'loken': 1,\n",
       " 'loki': 1,\n",
       " 'lolol': 1,\n",
       " 'londan': 1,\n",
       " 'londo': 1,\n",
       " 'locomot': 1,\n",
       " 'linu': 1,\n",
       " 'llshit': 1,\n",
       " 'lkhubbl': 1,\n",
       " 'lioness': 1,\n",
       " 'lioney': 1,\n",
       " 'liongat': 1,\n",
       " 'lioniz': 1,\n",
       " 'lionsgat': 1,\n",
       " 'lippo': 1,\n",
       " 'lipsyt': 1,\n",
       " 'liquefi': 1,\n",
       " 'lisbon': 1,\n",
       " 'lisett': 1,\n",
       " 'literati': 1,\n",
       " 'llewellyn': 1,\n",
       " 'lith': 1,\n",
       " 'litja': 1,\n",
       " 'litman': 1,\n",
       " 'litmu': 1,\n",
       " 'livabl': 1,\n",
       " 'livelihood': 1,\n",
       " 'liver': 1,\n",
       " 'liveri': 1,\n",
       " 'liverpool': 1,\n",
       " 'liverwurst': 1,\n",
       " 'livin': 1,\n",
       " 'lk': 1,\n",
       " 'lithuanian': 1,\n",
       " 'mackendrick': 1,\n",
       " 'mackinnon': 1,\n",
       " 'macliammoir': 1,\n",
       " 'martini': 1,\n",
       " 'martita': 1,\n",
       " 'maruerit': 1,\n",
       " 'maruschka': 1,\n",
       " 'marybeth': 1,\n",
       " 'marzia': 1,\n",
       " 'masayuki': 1,\n",
       " 'mascarad': 1,\n",
       " 'masladar': 1,\n",
       " 'masoud': 1,\n",
       " 'masqu': 1,\n",
       " 'martinez': 1,\n",
       " 'massenet': 1,\n",
       " 'massimiliano': 1,\n",
       " 'mastercard': 1,\n",
       " 'masterstrok': 1,\n",
       " 'mastoraki': 1,\n",
       " 'mastrantonio': 1,\n",
       " 'mastriosimon': 1,\n",
       " 'mastroianni': 1,\n",
       " 'masuji': 1,\n",
       " 'masur': 1,\n",
       " 'mata': 1,\n",
       " 'matata': 1,\n",
       " 'masseur': 1,\n",
       " 'matchbook': 1,\n",
       " 'martindal': 1,\n",
       " 'martelli': 1,\n",
       " 'mariag': 1,\n",
       " 'mariah': 1,\n",
       " 'marianna': 1,\n",
       " 'maricarmen': 1,\n",
       " 'marich': 1,\n",
       " 'marienbad': 1,\n",
       " 'marienth': 1,\n",
       " 'marilia': 1,\n",
       " 'marionett': 1,\n",
       " 'mariposa': 1,\n",
       " 'marisol': 1,\n",
       " 'marth': 1,\n",
       " 'marita': 1,\n",
       " 'marker': 1,\n",
       " 'marksman': 1,\n",
       " 'marm': 1,\n",
       " 'marner': 1,\n",
       " 'maro': 1,\n",
       " 'marolla': 1,\n",
       " 'marrakech': 1,\n",
       " 'marrakeck': 1,\n",
       " 'marseil': 1,\n",
       " 'marsh': 1,\n",
       " 'marshi': 1,\n",
       " 'marivaux': 1,\n",
       " 'margret': 1,\n",
       " 'matchmak': 1,\n",
       " 'mateo': 1,\n",
       " 'mcarthur': 1,\n",
       " 'mcbeal': 1,\n",
       " 'mcbo': 1,\n",
       " 'mccabe': 1,\n",
       " 'mccarey': 1,\n",
       " 'mccarthyism': 1,\n",
       " 'mcclori': 1,\n",
       " 'mcclug': 1,\n",
       " 'mcclurg': 1,\n",
       " 'mccormick': 1,\n",
       " 'mccowen': 1,\n",
       " 'mcandrew': 1,\n",
       " 'mcdiarmid': 1,\n",
       " 'mcdougl': 1,\n",
       " 'mcdowal': 1,\n",
       " 'mcduck': 1,\n",
       " 'mceneri': 1,\n",
       " 'mcenro': 1,\n",
       " 'mcentir': 1,\n",
       " 'mceveeti': 1,\n",
       " 'mcfadden': 1,\n",
       " 'mcgiver': 1,\n",
       " 'mcgiveth': 1,\n",
       " 'mcgowan': 1,\n",
       " 'mcdougal': 1,\n",
       " 'matchup': 1,\n",
       " 'mcanal': 1,\n",
       " 'mba': 1,\n",
       " 'matern': 1,\n",
       " 'mathau': 1,\n",
       " 'matheron': 1,\n",
       " 'mathurin': 1,\n",
       " 'matic': 1,\n",
       " 'matlock': 1,\n",
       " 'matricul': 1,\n",
       " 'matrimoni': 1,\n",
       " 'matronli': 1,\n",
       " 'matsushima': 1,\n",
       " 'matthu': 1,\n",
       " 'mbna': 1,\n",
       " 'mattia': 1,\n",
       " 'maughan': 1,\n",
       " 'mauritania': 1,\n",
       " 'mauritiu': 1,\n",
       " 'mauser': 1,\n",
       " 'mauvai': 1,\n",
       " 'maximilian': 1,\n",
       " 'maxmillian': 1,\n",
       " 'mayberri': 1,\n",
       " 'mayflow': 1,\n",
       " 'mayi': 1,\n",
       " 'mayo': 1,\n",
       " 'matuszak': 1,\n",
       " 'margo': 1,\n",
       " 'marginalis': 1,\n",
       " 'margarin': 1,\n",
       " 'mahari': 1,\n",
       " 'mahdist': 1,\n",
       " 'mahogani': 1,\n",
       " 'maidment': 1,\n",
       " 'maili': 1,\n",
       " 'maim': 1,\n",
       " 'mainev': 1,\n",
       " 'mainspr': 1,\n",
       " 'maitlan': 1,\n",
       " 'maiz': 1,\n",
       " 'majkowski': 1,\n",
       " 'mahal': 1,\n",
       " 'majo': 1,\n",
       " 'makeout': 1,\n",
       " 'mako': 1,\n",
       " 'malabimba': 1,\n",
       " 'maladroit': 1,\n",
       " 'malam': 1,\n",
       " 'malama': 1,\n",
       " 'malay': 1,\n",
       " 'malayan': 1,\n",
       " 'maldera': 1,\n",
       " 'malebranch': 1,\n",
       " 'maledetta': 1,\n",
       " 'majumdar': 1,\n",
       " 'malenkaya': 1,\n",
       " 'magritt': 1,\n",
       " 'magnifiqu': 1,\n",
       " 'macneil': 1,\n",
       " 'macquir': 1,\n",
       " 'macreadi': 1,\n",
       " 'macro': 1,\n",
       " 'madan': 1,\n",
       " 'madchen': 1,\n",
       " 'maddeningli': 1,\n",
       " 'madder': 1,\n",
       " 'maddin': 1,\n",
       " 'maddison': 1,\n",
       " 'madelin': 1,\n",
       " 'magon': 1,\n",
       " 'madhous': 1,\n",
       " 'madoc': 1,\n",
       " 'madr': 1,\n",
       " 'madwoman': 1,\n",
       " 'magali': 1,\n",
       " 'magdalen': 1,\n",
       " 'magdalena': 1,\n",
       " 'magi': 1,\n",
       " 'magnani': 1,\n",
       " 'magnavis': 1,\n",
       " 'magnavolt': 1,\n",
       " 'magnfici': 1,\n",
       " 'madkaugh': 1,\n",
       " 'maletta': 1,\n",
       " 'malfunct': 1,\n",
       " 'malhotra': 1,\n",
       " 'manna': 1,\n",
       " 'mannen': 1,\n",
       " 'mannheim': 1,\n",
       " 'mannish': 1,\n",
       " 'manoj': 1,\n",
       " 'manojlov': 1,\n",
       " 'manserv': 1,\n",
       " 'manslaught': 1,\n",
       " 'mantan': 1,\n",
       " 'manti': 1,\n",
       " 'mantra': 1,\n",
       " 'manigot': 1,\n",
       " 'manzil': 1,\n",
       " 'maori': 1,\n",
       " 'mapl': 1,\n",
       " 'mapoth': 1,\n",
       " 'mappl': 1,\n",
       " 'maraud': 1,\n",
       " 'marcelin': 1,\n",
       " 'marchal': 1,\n",
       " 'marcia': 1,\n",
       " 'marconi': 1,\n",
       " 'marcuzzo': 1,\n",
       " 'margareta': 1,\n",
       " 'maojlov': 1,\n",
       " 'manhol': 1,\n",
       " 'manhatttan': 1,\n",
       " 'manhatten': 1,\n",
       " 'malik': 1,\n",
       " 'malleabl': 1,\n",
       " 'mallepa': 1,\n",
       " 'mallori': 1,\n",
       " 'malocchio': 1,\n",
       " 'malta': 1,\n",
       " 'mam': 1,\n",
       " 'mambo': 1,\n",
       " 'mammi': 1,\n",
       " 'mamooth': 1,\n",
       " 'manchest': 1,\n",
       " 'manchuria': 1,\n",
       " 'manchurian': 1,\n",
       " 'mancia': 1,\n",
       " 'mancini': 1,\n",
       " 'mancuso': 1,\n",
       " 'manderley': 1,\n",
       " 'mandolin': 1,\n",
       " 'mandraki': 1,\n",
       " 'manet': 1,\n",
       " 'manger': 1,\n",
       " 'mangi': 1,\n",
       " 'mangl': 1,\n",
       " 'mangler': 1,\n",
       " 'manhat': 1,\n",
       " 'liang': 1,\n",
       " 'mcgraw': 1,\n",
       " 'lia': 1,\n",
       " 'leyt': 1,\n",
       " 'kino': 1,\n",
       " 'kinsey': 1,\n",
       " 'kintaro': 1,\n",
       " 'kippei': 1,\n",
       " 'kirbyef': 1,\n",
       " 'kirkland': 1,\n",
       " 'kishikawa': 1,\n",
       " 'kishor': 1,\n",
       " 'kisser': 1,\n",
       " 'kissing': 1,\n",
       " 'kitchi': 1,\n",
       " 'kinkad': 1,\n",
       " 'kitross': 1,\n",
       " 'kitt': 1,\n",
       " 'kittiwak': 1,\n",
       " 'kkk': 1,\n",
       " 'klang': 1,\n",
       " 'klapisch': 1,\n",
       " 'klaus': 1,\n",
       " 'kleban': 1,\n",
       " 'kleber': 1,\n",
       " 'kleenex': 1,\n",
       " 'kleinman': 1,\n",
       " 'klick': 1,\n",
       " 'kitschi': 1,\n",
       " 'klimovski': 1,\n",
       " 'kinji': 1,\n",
       " 'kinglsey': 1,\n",
       " 'kiddin': 1,\n",
       " 'kiddo': 1,\n",
       " 'kierkegaard': 1,\n",
       " 'kierlaw': 1,\n",
       " 'kietel': 1,\n",
       " 'kieth': 1,\n",
       " 'kiil': 1,\n",
       " 'kiki': 1,\n",
       " 'kilcher': 1,\n",
       " 'kildar': 1,\n",
       " 'kilim': 1,\n",
       " 'kingsford': 1,\n",
       " 'killabl': 1,\n",
       " 'kilpatrick': 1,\n",
       " 'kilt': 1,\n",
       " 'kimbal': 1,\n",
       " 'kimbl': 1,\n",
       " 'kimi': 1,\n",
       " 'kimmel': 1,\n",
       " 'kincaid': 1,\n",
       " 'kindsa': 1,\n",
       " 'kinetescop': 1,\n",
       " 'kinetoscop': 1,\n",
       " 'kinfolk': 1,\n",
       " 'kilmor': 1,\n",
       " 'kidder': 1,\n",
       " 'klip': 1,\n",
       " 'klop': 1,\n",
       " 'kono': 1,\n",
       " 'koo': 1,\n",
       " 'kookoo': 1,\n",
       " 'koolhoven': 1,\n",
       " 'koop': 1,\n",
       " 'kopek': 1,\n",
       " 'koran': 1,\n",
       " 'korngold': 1,\n",
       " 'kornhaus': 1,\n",
       " 'kornman': 1,\n",
       " 'korp': 1,\n",
       " 'kommissar': 1,\n",
       " 'koslova': 1,\n",
       " 'kosugi': 1,\n",
       " 'kotto': 1,\n",
       " 'kou': 1,\n",
       " 'koun': 1,\n",
       " 'kounen': 1,\n",
       " 'kovacev': 1,\n",
       " 'kove': 1,\n",
       " 'kowtow': 1,\n",
       " 'koya': 1,\n",
       " 'koz': 1,\n",
       " 'kozasa': 1,\n",
       " 'kostic': 1,\n",
       " 'klondik': 1,\n",
       " 'komizu': 1,\n",
       " 'komatsu': 1,\n",
       " 'klugman': 1,\n",
       " 'knacker': 1,\n",
       " 'knappertsbusch': 1,\n",
       " 'knicker': 1,\n",
       " 'knickerbock': 1,\n",
       " 'knieper': 1,\n",
       " 'knievel': 1,\n",
       " 'knightli': 1,\n",
       " 'knoflikari': 1,\n",
       " 'knowabl': 1,\n",
       " 'knucklehead': 1,\n",
       " 'kombat': 1,\n",
       " 'kobayashi': 1,\n",
       " 'koboi': 1,\n",
       " 'koch': 1,\n",
       " 'kodak': 1,\n",
       " 'kodi': 1,\n",
       " 'kodokoo': 1,\n",
       " 'koenigsegg': 1,\n",
       " 'koffe': 1,\n",
       " 'kohler': 1,\n",
       " 'kolev': 1,\n",
       " 'kolkata': 1,\n",
       " 'kolker': 1,\n",
       " 'kobe': 1,\n",
       " 'kpc': 1,\n",
       " 'kickback': 1,\n",
       " 'kibbutz': 1,\n",
       " 'kasey': 1,\n",
       " 'kashmir': 1,\n",
       " 'kassandra': 1,\n",
       " 'kassir': 1,\n",
       " 'kassman': 1,\n",
       " 'kastner': 1,\n",
       " 'kasugi': 1,\n",
       " 'katarzyna': 1,\n",
       " 'katayama': 1,\n",
       " 'katey': 1,\n",
       " 'katha': 1,\n",
       " 'kasem': 1,\n",
       " 'katharina': 1,\n",
       " 'katja': 1,\n",
       " 'katkin': 1,\n",
       " 'katsuhito': 1,\n",
       " 'katya': 1,\n",
       " 'kau': 1,\n",
       " 'kawamori': 1,\n",
       " 'kaylan': 1,\n",
       " 'kaza': 1,\n",
       " 'kazakh': 1,\n",
       " 'kazooi': 1,\n",
       " 'keat': 1,\n",
       " 'kathmandu': 1,\n",
       " 'kebbel': 1,\n",
       " 'kasadya': 1,\n",
       " 'karzi': 1,\n",
       " 'kanagawa': 1,\n",
       " 'kandic': 1,\n",
       " 'kang': 1,\n",
       " 'kangaroo': 1,\n",
       " 'kangho': 1,\n",
       " 'kanh': 1,\n",
       " 'kanin': 1,\n",
       " 'kanno': 1,\n",
       " 'kant': 1,\n",
       " 'kaput': 1,\n",
       " 'karadz': 1,\n",
       " 'kasaba': 1,\n",
       " 'karamzov': 1,\n",
       " 'kardo': 1,\n",
       " 'kareen': 1,\n",
       " 'karena': 1,\n",
       " 'karim': 1,\n",
       " 'karino': 1,\n",
       " 'karisma': 1,\n",
       " 'karmic': 1,\n",
       " 'karn': 1,\n",
       " 'karnag': 1,\n",
       " 'kartalian': 1,\n",
       " 'karyo': 1,\n",
       " 'karan': 1,\n",
       " 'kibitz': 1,\n",
       " 'ked': 1,\n",
       " 'keef': 1,\n",
       " 'kesan': 1,\n",
       " 'kessler': 1,\n",
       " 'kester': 1,\n",
       " 'ketamin': 1,\n",
       " 'ketchup': 1,\n",
       " 'keuck': 1,\n",
       " 'keung': 1,\n",
       " 'keusch': 1,\n",
       " 'kewl': 1,\n",
       " 'kewpi': 1,\n",
       " 'keyboardist': 1,\n",
       " 'kerwin': 1,\n",
       " 'keziko': 1,\n",
       " 'khaleil': 1,\n",
       " 'khali': 1,\n",
       " 'khanabadosh': 1,\n",
       " 'kharbanda': 1,\n",
       " 'kheir': 1,\n",
       " 'khemu': 1,\n",
       " 'kheymeh': 1,\n",
       " 'khoua': 1,\n",
       " 'khrystyn': 1,\n",
       " 'khufu': 1,\n",
       " 'kibbe': 1,\n",
       " 'kfir': 1,\n",
       " 'kedrova': 1,\n",
       " 'kershaw': 1,\n",
       " 'kermod': 1,\n",
       " 'keeler': 1,\n",
       " 'keenan': 1,\n",
       " 'keenen': 1,\n",
       " 'keenli': 1,\n",
       " 'keerthana': 1,\n",
       " 'keifer': 1,\n",
       " 'keiko': 1,\n",
       " 'keir': 1,\n",
       " 'keisuk': 1,\n",
       " 'keita': 1,\n",
       " 'kel': 1,\n",
       " 'kerrigan': 1,\n",
       " 'keller': 1,\n",
       " 'kemo': 1,\n",
       " 'kemono': 1,\n",
       " 'kendo': 1,\n",
       " 'kenitalia': 1,\n",
       " 'kenov': 1,\n",
       " 'kenshin': 1,\n",
       " 'keoma': 1,\n",
       " 'ker': 1,\n",
       " 'kerbi': 1,\n",
       " 'kerchev': 1,\n",
       " 'keri': 1,\n",
       " 'kellerkind': 1,\n",
       " 'kraft': 1,\n",
       " 'krakowski': 1,\n",
       " 'krasker': 1,\n",
       " 'layman': 1,\n",
       " 'layton': 1,\n",
       " 'laze': 1,\n",
       " 'lazerov': 1,\n",
       " 'laziest': 1,\n",
       " 'lcd': 1,\n",
       " 'lea': 1,\n",
       " 'leaflet': 1,\n",
       " 'leander': 1,\n",
       " 'leapt': 1,\n",
       " 'learner': 1,\n",
       " 'layabout': 1,\n",
       " 'leash': 1,\n",
       " 'leatheri': 1,\n",
       " 'leaven': 1,\n",
       " 'leavin': 1,\n",
       " 'lebanes': 1,\n",
       " 'lebanon': 1,\n",
       " 'lebowski': 1,\n",
       " 'lecter': 1,\n",
       " 'ledg': 1,\n",
       " 'leech': 1,\n",
       " 'leena': 1,\n",
       " 'leet': 1,\n",
       " 'leatherfac': 1,\n",
       " 'leeway': 1,\n",
       " 'laxmi': 1,\n",
       " 'lawnmow': 1,\n",
       " 'latina': 1,\n",
       " 'latitud': 1,\n",
       " 'latrin': 1,\n",
       " 'latshaw': 1,\n",
       " 'latterli': 1,\n",
       " 'latvia': 1,\n",
       " 'laudabl': 1,\n",
       " 'laudatori': 1,\n",
       " 'lauderdal': 1,\n",
       " 'laudrup': 1,\n",
       " 'laufther': 1,\n",
       " 'lawson': 1,\n",
       " 'laugher': 1,\n",
       " 'launchpad': 1,\n",
       " 'launder': 1,\n",
       " 'laundromat': 1,\n",
       " 'laurent': 1,\n",
       " 'lauret': 1,\n",
       " 'lauter': 1,\n",
       " 'lavign': 1,\n",
       " 'lavinia': 1,\n",
       " 'lavishli': 1,\n",
       " 'lawanda': 1,\n",
       " 'lawman': 1,\n",
       " 'laughtrack': 1,\n",
       " 'latifah': 1,\n",
       " 'leez': 1,\n",
       " 'leger': 1,\n",
       " 'leprachaun': 1,\n",
       " 'leprosi': 1,\n",
       " 'leprou': 1,\n",
       " 'leskin': 1,\n",
       " 'lestat': 1,\n",
       " 'lesur': 1,\n",
       " 'lethargi': 1,\n",
       " 'letscher': 1,\n",
       " 'lettuc': 1,\n",
       " 'letyat': 1,\n",
       " 'letzt': 1,\n",
       " 'lepord': 1,\n",
       " 'leukemia': 1,\n",
       " 'leve': 1,\n",
       " 'lever': 1,\n",
       " 'leverag': 1,\n",
       " 'leverett': 1,\n",
       " 'levitt': 1,\n",
       " 'levr': 1,\n",
       " 'lew': 1,\n",
       " 'leway': 1,\n",
       " 'lexicon': 1,\n",
       " 'lexington': 1,\n",
       " 'leyland': 1,\n",
       " 'leval': 1,\n",
       " 'lefler': 1,\n",
       " 'leper': 1,\n",
       " 'lennier': 1,\n",
       " 'legionairr': 1,\n",
       " 'legisl': 1,\n",
       " 'legitimaci': 1,\n",
       " 'legizomo': 1,\n",
       " 'legless': 1,\n",
       " 'lego': 1,\n",
       " 'legre': 1,\n",
       " 'legrix': 1,\n",
       " 'leguzaimo': 1,\n",
       " 'lehrman': 1,\n",
       " 'leiberman': 1,\n",
       " 'leong': 1,\n",
       " 'leicest': 1,\n",
       " 'leighton': 1,\n",
       " 'leisin': 1,\n",
       " 'leiter': 1,\n",
       " 'leith': 1,\n",
       " 'lemk': 1,\n",
       " 'lemoin': 1,\n",
       " 'lender': 1,\n",
       " 'lenghten': 1,\n",
       " 'lengthier': 1,\n",
       " 'lengua': 1,\n",
       " 'leni': 1,\n",
       " 'leif': 1,\n",
       " 'latido': 1,\n",
       " 'latent': 1,\n",
       " 'laster': 1,\n",
       " 'kuzko': 1,\n",
       " 'kwai': 1,\n",
       " 'kwak': 1,\n",
       " 'kwik': 1,\n",
       " 'kwok': 1,\n",
       " 'kwon': 1,\n",
       " 'kyonki': 1,\n",
       " 'kyuubi': 1,\n",
       " 'labeija': 1,\n",
       " 'labl': 1,\n",
       " 'lacanian': 1,\n",
       " 'kustarica': 1,\n",
       " 'lacer': 1,\n",
       " 'lachrymos': 1,\n",
       " 'lackey': 1,\n",
       " 'lacquer': 1,\n",
       " 'lacuna': 1,\n",
       " 'ladron': 1,\n",
       " 'ladykil': 1,\n",
       " 'ladylik': 1,\n",
       " 'ladyship': 1,\n",
       " 'laert': 1,\n",
       " 'laetitia': 1,\n",
       " 'laila': 1,\n",
       " 'lacey': 1,\n",
       " 'lajo': 1,\n",
       " 'kusanagi': 1,\n",
       " 'kurtwood': 1,\n",
       " 'kraus': 1,\n",
       " 'kray': 1,\n",
       " 'krazi': 1,\n",
       " 'kreb': 1,\n",
       " 'kretschmann': 1,\n",
       " 'kringl': 1,\n",
       " 'kristian': 1,\n",
       " 'kristina': 1,\n",
       " 'kroft': 1,\n",
       " 'krull': 1,\n",
       " 'kryptonit': 1,\n",
       " 'kurupt': 1,\n",
       " 'krystal': 1,\n",
       " 'kubota': 1,\n",
       " 'kubrik': 1,\n",
       " 'kukkonen': 1,\n",
       " 'kulbhushan': 1,\n",
       " 'kum': 1,\n",
       " 'kunal': 1,\n",
       " 'kundera': 1,\n",
       " 'kungfu': 1,\n",
       " 'kunz': 1,\n",
       " 'kuprik': 1,\n",
       " 'kurdish': 1,\n",
       " 'krzysztof': 1,\n",
       " 'laka': 1,\n",
       " 'laker': 1,\n",
       " 'lakesid': 1,\n",
       " 'lanka': 1,\n",
       " 'lankan': 1,\n",
       " 'lanki': 1,\n",
       " 'lantana': 1,\n",
       " 'lao': 1,\n",
       " 'laplanch': 1,\n",
       " 'lappland': 1,\n",
       " 'laptop': 1,\n",
       " 'lar': 1,\n",
       " 'larain': 1,\n",
       " 'larami': 1,\n",
       " 'lani': 1,\n",
       " 'larceni': 1,\n",
       " 'largess': 1,\n",
       " 'larkin': 1,\n",
       " 'larrikin': 1,\n",
       " 'larroquett': 1,\n",
       " 'laru': 1,\n",
       " 'larval': 1,\n",
       " 'laryssa': 1,\n",
       " 'laserlight': 1,\n",
       " 'lashel': 1,\n",
       " 'lassen': 1,\n",
       " 'lasso': 1,\n",
       " 'larf': 1,\n",
       " 'languish': 1,\n",
       " 'languidli': 1,\n",
       " 'langrish': 1,\n",
       " 'lakin': 1,\n",
       " 'lakshmi': 1,\n",
       " 'lalo': 1,\n",
       " 'lamar': 1,\n",
       " 'lamarch': 1,\n",
       " 'lambent': 1,\n",
       " 'lamh': 1,\n",
       " 'lamour': 1,\n",
       " 'lampa': 1,\n",
       " 'lampanelli': 1,\n",
       " 'lampost': 1,\n",
       " 'lamppost': 1,\n",
       " 'lampshad': 1,\n",
       " 'lan': 1,\n",
       " 'landa': 1,\n",
       " 'landau': 1,\n",
       " 'landfil': 1,\n",
       " 'landmass': 1,\n",
       " 'landown': 1,\n",
       " 'landslid': 1,\n",
       " 'landua': 1,\n",
       " 'langa': 1,\n",
       " 'langaug': 1,\n",
       " 'langford': 1,\n",
       " 'langond': 1,\n",
       " 'lhasa': 1,\n",
       " 'kampf': 1,\n",
       " 'mcgree': 1,\n",
       " 'mchale': 1,\n",
       " 'nill': 1,\n",
       " 'nilli': 1,\n",
       " 'nilsen': 1,\n",
       " 'nilsson': 1,\n",
       " 'nim': 1,\n",
       " 'nimbl': 1,\n",
       " 'nimri': 1,\n",
       " 'niner': 1,\n",
       " 'nino': 1,\n",
       " 'ninotchka': 1,\n",
       " 'ninteen': 1,\n",
       " 'niku': 1,\n",
       " 'nintendo': 1,\n",
       " 'nip': 1,\n",
       " 'niqu': 1,\n",
       " 'nishabd': 1,\n",
       " 'nitin': 1,\n",
       " 'nitrat': 1,\n",
       " 'nitti': 1,\n",
       " 'nitu': 1,\n",
       " 'nivola': 1,\n",
       " 'nj': 1,\n",
       " 'noak': 1,\n",
       " 'nobi': 1,\n",
       " 'niob': 1,\n",
       " 'nobler': 1,\n",
       " 'nikkatsu': 1,\n",
       " 'nike': 1,\n",
       " 'niceiti': 1,\n",
       " 'nicholett': 1,\n",
       " 'nickelodean': 1,\n",
       " 'nicklebi': 1,\n",
       " 'nickola': 1,\n",
       " 'nico': 1,\n",
       " 'nicodemu': 1,\n",
       " 'nicodim': 1,\n",
       " 'nicolai': 1,\n",
       " 'nicoletta': 1,\n",
       " 'nicolson': 1,\n",
       " 'niki': 1,\n",
       " 'niei': 1,\n",
       " 'nietzschean': 1,\n",
       " 'nigeria': 1,\n",
       " 'nigger': 1,\n",
       " 'nightbe': 1,\n",
       " 'nightgown': 1,\n",
       " 'nighti': 1,\n",
       " 'nightli': 1,\n",
       " 'nightsheet': 1,\n",
       " 'nighwatch': 1,\n",
       " 'nihon': 1,\n",
       " 'nikan': 1,\n",
       " 'nieto': 1,\n",
       " 'niamh': 1,\n",
       " 'nobli': 1,\n",
       " 'noch': 1,\n",
       " 'northwestern': 1,\n",
       " 'nosed': 1,\n",
       " 'nosher': 1,\n",
       " 'nostril': 1,\n",
       " 'notar': 1,\n",
       " 'notepad': 1,\n",
       " 'nother': 1,\n",
       " 'nought': 1,\n",
       " 'nourish': 1,\n",
       " 'nova': 1,\n",
       " 'novelett': 1,\n",
       " 'northbound': 1,\n",
       " 'novello': 1,\n",
       " 'noyc': 1,\n",
       " 'nozzl': 1,\n",
       " 'npr': 1,\n",
       " 'ns': 1,\n",
       " 'nsync': 1,\n",
       " 'ntarea': 1,\n",
       " 'nuac': 1,\n",
       " 'nub': 1,\n",
       " 'nube': 1,\n",
       " 'numan': 1,\n",
       " 'numbr': 1,\n",
       " 'noy': 1,\n",
       " 'nobuo': 1,\n",
       " 'norsemen': 1,\n",
       " 'noroi': 1,\n",
       " 'nocturn': 1,\n",
       " 'noethem': 1,\n",
       " 'nofth': 1,\n",
       " 'noin': 1,\n",
       " 'noirish': 1,\n",
       " 'noisi': 1,\n",
       " 'noisiest': 1,\n",
       " 'nolti': 1,\n",
       " 'nona': 1,\n",
       " 'noncommitt': 1,\n",
       " 'nondenomin': 1,\n",
       " 'norrland': 1,\n",
       " 'nonono': 1,\n",
       " 'nonviol': 1,\n",
       " 'nook': 1,\n",
       " 'noonan': 1,\n",
       " 'noooooo': 1,\n",
       " ...}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary.groupby('word').size().sort_values().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "word\n",
       "br         9962\n",
       "film       5305\n",
       "movi       4356\n",
       "one        2807\n",
       "like       2130\n",
       "time       1688\n",
       "good       1574\n",
       "see        1512\n",
       "stori      1456\n",
       "charact    1427\n",
       "make       1411\n",
       "watch      1376\n",
       "well       1363\n",
       "get        1323\n",
       "great      1312\n",
       "love       1237\n",
       "show       1151\n",
       "would      1123\n",
       "also       1123\n",
       "realli     1069\n",
       "dtype: int64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separando cr√≠ticas positivas\n",
    "review_pos = movies_sample[movies_sample['label'] == 1]\n",
    "vocabulary_pos = pd.DataFrame(np.concatenate(review_pos['filtered_words'].values), columns=['word'])\n",
    "vocabulary_pos.groupby('word').size().sort_values(ascending=False)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Aula 1 - NLP - Aux.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
