{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J2HFkxmHfd_R"
   },
   "source": [
    "## Aula 1 - Processamento de Linguagem Natual\n",
    "\n",
    "Na aula de hoje, vamos explorar os seguintes tópicos em Python:\n",
    "\n",
    "1) Dados Estruturados e Não Estruturados.  \n",
    "2) Introdução a NLP.  \n",
    "3) Processamento de Textos.  \n",
    "4) Exercícios.  \n",
    "\n",
    "<img src=\"https://i1.wp.com/thedatascientist.com/wp-content/uploads/2018/09/data_science_wordcloud.png?fit=1584%2C1008&ssl=1\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hgvh29EJh6UK"
   },
   "source": [
    "Primeiramente, precisamos entender qual a diferença enre as duas fontes de dados mais comuns, sendo elas dados **estruturados** e **não estruturados**. Definimos ele como:\n",
    "<br><br>\n",
    "- **Dados Estruturados:** São dados que seguem uma estrutura mais rígida com um padrão fixo e constante. Por exemplo: Tabelas e DataFrames;<br><br>\n",
    "- **Dados Não estruturados:** Como já diz o nome, são dados que não tem uma estrutura bem estabelecida e necessitam de um processamento adicional para trabalharmos com eles. Exemplos: áudios, vídeos, textos e etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whQI1eePiJwx"
   },
   "source": [
    "### Introdução ao Processamento de Linguagem Natural (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bwjo0xx9h-Pi"
   },
   "source": [
    "O Processamento de Linguagem Natural, mas conhecido como NLP, é a abordagem onde trabalhamos com **dados não estruturados** do tipo **Texto**. O objetivo de trabalharmos com textos é extrair de informação e teor linguístico das nossas bases de textos e converter isso de uma forma númerica, onde poderemos utilizar em nossos modelos de *Machine Learning*.<br><br>\n",
    "Temos como exemplos de aplicações de NLP como:\n",
    "- Análise de Sentimentos em review de filmes e produtos ou mensagens em redes sociais;\n",
    "- Filtro de E-Mails Spams e Não Spams;\n",
    "- Identificação de textos a partir de construções linguísticas (descobrir se um texto foi escrito ou não por Machado de assis);\n",
    "- Tradutores de Idiomas;\n",
    "- ChatBots;\n",
    "- Corretores Ortográficos;\n",
    "- Classificação de textos de acordo com o conteúdo do texto (Esportes, Política, Economia e etc).\n",
    "<br><br>\n",
    "Nesta aula iremos aprender a partir dos nossos dados textuais a como processar, tratar e transformar os dados de uma maneira que os modelos de *Machine Learning* entendam.<br><br>\n",
    "\n",
    "A principal biblioteca de referência para NLP chama-se [NLTK - Natural Language Toll Kit](https://www.nltk.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IjXHV5QOjWDc"
   },
   "source": [
    "### Processamento de Textos\n",
    "\n",
    "Antes de mais nada, precisamos filtrar e tratar os nossos textos, de forma a deixar apenas o conteúdo de mais relevantes para a nossa análise. Existem alguns processos importantes para trabalhar com os textos (não necessariamente você precisa aplicar todos os procesos)\n",
    "\n",
    "- Remoção de Stopwords;\n",
    "- Limpeza de Textos;\n",
    "- Tokenização;\n",
    "- Normalização do Texto.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9je2oXCjjrwc"
   },
   "source": [
    "### Stopwords\n",
    "\n",
    "Stopwords são palavras que aparecem com uma frequência muito alta nos textos, mas que não trazem um teor de conteúdo relevante para o nosso modelo. Vamos entender isso na prática:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-26T00:07:59.379665Z",
     "start_time": "2022-08-26T00:07:57.979139Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\wiltd\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\wiltd\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\wiltd\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (2022.8.17)\n",
      "Requirement already satisfied: tqdm in c:\\users\\wiltd\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: click in c:\\users\\wiltd\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\wiltd\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from click->nltk) (0.4.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-26T00:07:59.395402Z",
     "start_time": "2022-08-26T00:07:59.380669Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3IR-pN2DjwXJ",
    "outputId": "9c8b59ea-653b-457e-b82e-d36bc3f07b67"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\wiltd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\wiltd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package rslp to\n",
      "[nltk_data]     C:\\Users\\wiltd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package rslp is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')  # https://www.nltk.org/_modules/nltk/tokenize/punkt.html\n",
    "nltk.download('rslp')   # Stemmer em português"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-26T00:07:59.410409Z",
     "start_time": "2022-08-26T00:07:59.396406Z"
    }
   },
   "outputs": [],
   "source": [
    "words_en = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-26T00:07:59.426416Z",
     "start_time": "2022-08-26T00:07:59.412412Z"
    }
   },
   "outputs": [],
   "source": [
    "words_pt = nltk.corpus.stopwords.words('portuguese')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jR2RHfCOn83i"
   },
   "source": [
    "Baixada a função de Stopwords, vamos definir um set de stopwords onde teremos uma lista com todas as stopwords em inglês já identificadas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-26T00:07:59.442421Z",
     "start_time": "2022-08-26T00:07:59.427418Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['my', 'house', 'is', 'black', 'and', 'white']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = ['my', 'house', 'is', 'black', 'and', 'white']\n",
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A9Dpg-8ApIxa"
   },
   "source": [
    "Vamos agora aplicar a remoção de Stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-26T00:07:59.458435Z",
     "start_time": "2022-08-26T00:07:59.443422Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my True\n",
      "house False\n",
      "is True\n",
      "black False\n",
      "and True\n",
      "white False\n"
     ]
    }
   ],
   "source": [
    "print(example[0], example[0] in words_en)\n",
    "print(example[1], example[1] in words_en)\n",
    "print(example[2], example[2] in words_en)\n",
    "print(example[3], example[3] in words_en)\n",
    "print(example[4], example[4] in words_en)\n",
    "print(example[5], example[5] in words_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-26T00:07:59.474450Z",
     "start_time": "2022-08-26T00:07:59.459437Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "house\n",
      "black\n",
      "white\n"
     ]
    }
   ],
   "source": [
    "for word in example:\n",
    "    if not word in words_en:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wnYz481KOjkM"
   },
   "source": [
    "### Limpeza do Texto\n",
    "\n",
    "Existem alguns cuidados com relação a grafia das palavras e elementos em um texto que devemos tomar bastante cuidado antes de fazer qualquer outra coisa. Esses pontos são:<br><br>\n",
    "- Transformar todas as palavras para MAIÚSCULAS ou minúsculas;\n",
    "- Remover caracteres especiais;\n",
    "- Remover dígitos (quando não forem relevantes);\n",
    "- Remover acentuação (caso típico de quando trabalhamos com textos em Português);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-26T00:07:59.489975Z",
     "start_time": "2022-08-26T00:07:59.475451Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "farmacêutico\n",
      "relogio\n",
      "parada\n"
     ]
    }
   ],
   "source": [
    "words = ['farMacêuTICO', 'relogio', 'Parada']\n",
    "for word in words:\n",
    "    print(word.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HYRTjdWiTKWu"
   },
   "source": [
    "### Remoção de dígitos, caracteres especiais e qualquer outro item que não queremos no texto\n",
    "\n",
    "Para essa etapa do processo, iremos utilizar uma biblioteca auxiliar [RegEx (Regular Expression)](https://docs.python.org/3/library/re.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-26T00:07:59.505303Z",
     "start_time": "2022-08-26T00:07:59.490978Z"
    },
    "id": "iE4HC6guTNGR"
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2n2D1Od-TVVx"
   },
   "source": [
    "Importada a biblioteca, vamos utilizar a função *re.sub*, para substituir os elementos que não queremos nos nossos textos:\n",
    "\n",
    "**Procure por re.sub**  \n",
    "https://docs.python.org/3/library/re.html\n",
    "\n",
    "Obs: Utilize o site https://regex101.com/ para criar o regex e o re.sub para substituir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-26T00:07:59.520992Z",
     "start_time": "2022-08-26T00:07:59.507305Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'São mais de  cursos de Data Science!!!! #datascience #machinelearning'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frase = 'São mais de 50 cursos de Data Science!!!! #datascience #machinelearning'\n",
    "re.sub(r'\\d', '', frase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-26T00:07:59.537006Z",
     "start_time": "2022-08-26T00:07:59.521993Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SomaisdecursosdeDataSciencedatasciencemachinelearning'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r'[^a-zA-Z]+','', frase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zuqfqkjuabuH"
   },
   "source": [
    "Utilizem a documentação para descobrir mais códigos para filtrar elementos ou mesmo deem uma olhada nesse artigo, que resume de uma forma bem visual as aplicações do RegEx: [clique aqui](https://amitness.com/regex/)\n",
    "\n",
    "Tutorial de regex https://blog.geekhunter.com.br/python-regex/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HeoTPv_4agE2"
   },
   "source": [
    "Hoje os emojis fazem parte da comunicação via mensagens, por isso iremos ver como utilizar frases contento emojis e trata-los de forma adequada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-26T00:08:00.903848Z",
     "start_time": "2022-08-26T00:07:59.538007Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in c:\\users\\wiltd\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-26T00:08:00.919399Z",
     "start_time": "2022-08-26T00:08:00.904849Z"
    },
    "id": "fCZwZhJLaTnj"
   },
   "outputs": [],
   "source": [
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-26T00:08:00.935406Z",
     "start_time": "2022-08-26T00:08:00.920400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Python é 👍'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texto = 'Python é :thumbs_up:'\n",
    "emoji.emojize(texto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c9HCV1QKbjXG"
   },
   "source": [
    "Para ver a lista completa:\n",
    "\n",
    "https://www.webfx.com/tools/emoji-cheat-sheet/\n",
    "\n",
    "Link biblioteca: https://github.com/carpedm20/emoji/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 1\n",
    "Utilizar Regex para validar CPFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-26T00:08:00.951436Z",
     "start_time": "2022-08-26T00:08:00.936406Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern=r\"^(\\d{3}.){2}\\d{3}-\\d{2}$\" ### qual é a expressao regular do regex neste caso\n",
    "cpf = '000.000.000-00'\n",
    "bool(re.match(pattern, cpf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZap5orRdLUE"
   },
   "source": [
    "### Exercício 2\n",
    "\n",
    "Utilizar Regex para validação de emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-26T00:08:00.967450Z",
     "start_time": "2022-08-26T00:08:00.952436Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email = 'cflavs.7@gmail.com'\n",
    "pattern = r\"([-a-zA-Z0-9.`?{}]+@\\w+\\.\\w+)\"  \n",
    "patterns = re.compile(pattern)\n",
    "bool(re.match(patterns, email))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "laoYA1wYg6Yj"
   },
   "source": [
    "Podemos utilizar NLP para identificar frases que contenham uma palavra chave!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K5f4aJfl7QCC"
   },
   "source": [
    "### Remoção de Acentuação\n",
    "Para a remoção de acentuação, iremos utilizar uma bibloteca chamada *Unidecode*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-26T00:08:02.356820Z",
     "start_time": "2022-08-26T00:08:00.968451Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a7hpJwTa7SIY",
    "outputId": "066869e7-ca3f-45b3-9b60-c6598f820639"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unidecode in c:\\users\\wiltd\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.3.4)\n"
     ]
    }
   ],
   "source": [
    "# Caso precise instalar a biblioteca, descomente o código abaixo\n",
    "!pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-26T00:08:02.372889Z",
     "start_time": "2022-08-26T00:08:02.357821Z"
    },
    "id": "8eLSKg7A7TS5"
   },
   "outputs": [],
   "source": [
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-26T00:08:02.388759Z",
     "start_time": "2022-08-26T00:08:02.373889Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cecília\n"
     ]
    }
   ],
   "source": [
    "string = 'Cecília'\n",
    "print(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-26T00:08:02.404773Z",
     "start_time": "2022-08-26T00:08:02.389760Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cecilia'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unidecode(string.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HcoeEmpC9Kbc"
   },
   "source": [
    "## Tokenização\n",
    "\n",
    "Tokenização é um processo onde transformamos um texto de uma string única em fragmentos desse texto na forma de *tokens*, que nada mais são do que as próprias palavras! Para isso, vamos utilizar a função *word_tokenize* do NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-26T00:08:02.420824Z",
     "start_time": "2022-08-26T00:08:02.405775Z"
    },
    "id": "ucLVo_6j9L9x"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-26T00:08:02.436732Z",
     "start_time": "2022-08-26T00:08:02.421825Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Python', 'é', 'legal', 'e', 'python', 'é', 'divertido']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Python', 'é', 'legal', 'e', 'python', 'é', 'divertido']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = 'Python é legal e python é divertido'\n",
    "print(word_tokenize(string))\n",
    "string.split(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4qed6o8G9Vib"
   },
   "source": [
    "## Normalização de Textos\n",
    "\n",
    "**Normalização de Textos (Text Normalization)** é o procedimento que consiste em **padronizar** o texto, de modo a evitar que variações tornem os modelos demasiadamente complexos. Por exemplo: tratar singular/plural como a mesma coisa, ou então eliminar conjugação de verbos. Outras componentes comuns da normalização são a de eliminar palavras que não agregam muito significado, ou palavras muito raras.\n",
    "\n",
    "Abaixo alguns exemplos de ações de Text Normalization que podem ser aplicadas no pré-processamento de dados textuais:\n",
    "\n",
    "**Stemming** - Redução de tokens à sua raiz invariante através da **remoção de prefixos ou sufixos**. Baseado em heurística<br>\n",
    "**Lemmatization** - Redução de tokens à sua raiz invariante através da **análise linguística do token**. Baseado em dicionário léxico<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ieG5enJ79aUg"
   },
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-26T00:08:02.452744Z",
     "start_time": "2022-08-26T00:08:02.437732Z"
    },
    "id": "jA-tVw5S9RGm"
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import *\n",
    "from nltk.stem import RSLPStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-26T00:08:02.468752Z",
     "start_time": "2022-08-26T00:08:02.453745Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "say\n",
      "write\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "words = ['saying', 'writing', 'running']\n",
    "for word in words:\n",
    "    print(stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-26T00:08:02.484757Z",
     "start_time": "2022-08-26T00:08:02.469752Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corr\n",
      "corr\n"
     ]
    }
   ],
   "source": [
    "stemmer = RSLPStemmer()\n",
    "words_pt = ['correndo', 'corra']\n",
    "for word in words_pt:\n",
    "    print(stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-26T00:08:03.934443Z",
     "start_time": "2022-08-26T00:08:02.485757Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\wiltd\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.4.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\wiltd\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\wiltd\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (1.22.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\wiltd\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\wiltd\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (1.0.3)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in c:\\users\\wiltd\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (8.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\wiltd\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in c:\\users\\wiltd\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\wiltd\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in c:\\users\\wiltd\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (1.9.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\wiltd\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\wiltd\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (1.0.8)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\wiltd\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (0.4.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\wiltd\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (2.4.4)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\wiltd\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (3.0.7)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\wiltd\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\wiltd\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (0.6.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\wiltd\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (4.64.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\wiltd\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (58.1.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\wiltd\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (2.28.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\wiltd\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from packaging>=20.0->spacy) (3.0.8)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in c:\\users\\wiltd\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\wiltd\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy) (4.3.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\wiltd\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\wiltd\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.6.15)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\wiltd\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\wiltd\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\wiltd\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.8)\n",
      "Requirement already satisfied: colorama in c:\\users\\wiltd\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\wiltd\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\wiltd\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->spacy) (2.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-26T00:22:48.931264Z",
     "start_time": "2022-08-26T00:22:43.354757Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pt-core-news-sm==3.4.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-3.4.0/pt_core_news_sm-3.4.0-py3-none-any.whl (13.0 MB)\n",
      "     --------------------------------------- 13.0/13.0 MB 50.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.5.0,>=3.4.0 in c:\\users\\wiltd\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pt-core-news-sm==3.4.0) (3.4.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\wiltd\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (0.10.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\wiltd\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (1.22.3)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\wiltd\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (1.0.3)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\wiltd\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (0.6.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\wiltd\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (2.28.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\wiltd\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (3.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\wiltd\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (2.4.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\wiltd\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (1.0.8)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\wiltd\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (4.64.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in c:\\users\\wiltd\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (3.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\wiltd\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (2.0.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\wiltd\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (58.1.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\wiltd\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\wiltd\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (3.0.7)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in c:\\users\\wiltd\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (8.1.0)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\wiltd\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (0.4.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\wiltd\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (3.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\wiltd\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (21.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in c:\\users\\wiltd\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (1.9.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\wiltd\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (3.0.8)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in c:\\users\\wiltd\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\wiltd\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (4.3.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\wiltd\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (2.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\wiltd\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\wiltd\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (2022.6.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\wiltd\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (3.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\wiltd\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (0.7.8)\n",
      "Requirement already satisfied: colorama in c:\\users\\wiltd\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\wiltd\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\wiltd\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (2.1.1)\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('pt_core_news_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-25 21:22:43.631767: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2022-08-25 21:22:43.631801: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-08-25 21:22:45.820783: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'nvcuda.dll'; dlerror: nvcuda.dll not found\n",
      "2022-08-25 21:22:45.820806: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-08-25 21:22:45.823108: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: DESKTOP-B2RGRML\n",
      "2022-08-25 21:22:45.823164: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: DESKTOP-B2RGRML\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "!spacy download \"pt_core_news_sm\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-26T00:32:45.330587Z",
     "start_time": "2022-08-26T00:32:44.976867Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"pt_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-26T00:23:14.804872Z",
     "start_time": "2022-08-26T00:23:14.792861Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['corra']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(str([palavra for palavra in words_pt]))\n",
    "[token.lemma_ for token in doc if token.pos_ == 'NOUN']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parts-of-Speech (POS)\n",
    "\n",
    "https://www.guru99.com/pos-tagging-chunking-nltk.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-26T00:31:28.990713Z",
     "start_time": "2022-08-26T00:31:28.984708Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\wiltd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('learning', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('datascience', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('cool', 'JJ')]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import pos_tag, ne_chunk\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "words = ['learning', 'is', 'datascience', 'is', 'cool']\n",
    "pos_tag(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-26T00:35:28.498646Z",
     "start_time": "2022-08-26T00:35:28.489638Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NASA', 'NNP'), ('awarded', 'VBD'), ('elon', 'NN'), ('musk', 'NN'), ('’', 'NNP'), ('s', 'VBZ'), ('SpaceX', 'NNP'), ('a', 'DT'), ('$', '$'), ('2.9', 'CD'), ('billion', 'CD'), ('contract', 'NN'), ('to', 'TO'), ('build', 'VB'), ('the', 'DT'), ('lunar', 'NN'), ('lander', 'NN'), ('.', '.')]\n",
      "(S\n",
      "  (ORGANIZATION NASA/NNP)\n",
      "  awarded/VBD\n",
      "  elon/NN\n",
      "  musk/NN\n",
      "  ’/NNP\n",
      "  s/VBZ\n",
      "  (ORGANIZATION SpaceX/NNP)\n",
      "  a/DT\n",
      "  $/$\n",
      "  2.9/CD\n",
      "  billion/CD\n",
      "  contract/NN\n",
      "  to/TO\n",
      "  build/VB\n",
      "  the/DT\n",
      "  lunar/NN\n",
      "  lander/NN\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "text = \"NASA awarded elon musk’s SpaceX a $2.9 billion contract to build the lunar lander.\"\n",
    "tokens = word_tokenize(text)\n",
    "tag = pos_tag(tokens)\n",
    "print(pos_tag(tokens))\n",
    "print(nltk.ne_chunk(tag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YxH2uhI4A8mq"
   },
   "source": [
    "## Pipeline de Processamento de Textos\n",
    "\n",
    "Conhecendo todos os tipos de processamentos que podemos utilizar, uma forma útil e organizada para isso é construirmos uma funçãi que receba o nosso dados originais e realizada todos os processamentos que queremos nos textos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S4qLqa46BBUi"
   },
   "source": [
    "Crie uma classe que seja capaz de:\n",
    "\n",
    "- Metodo para remover acentuação\n",
    "- Metodo de remover digitos\n",
    "- Metodo de remover caracteres especiais\n",
    "- Metodo de normalizar o texto em caixa baixa\n",
    "- Metodo para criar os tokens\n",
    "- Metodo para filtrar stopwords\n",
    "- Metodo para pegar o stemming\n",
    "- Metodo para pegar o lemma\n",
    "- Metodo de pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testar o pipeline em diferentes strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-26T00:15:39.878363Z",
     "start_time": "2022-08-26T00:15:39.874359Z"
    },
    "id": "xOfCHWPENHWc"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import svgling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-26T00:16:23.233380Z",
     "start_time": "2022-08-26T00:16:22.689810Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I grew up (b. 1965) watching and loving the Th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When I put this movie in my DVD player, and sa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Why do people who do not know what a particula...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Even though I have great interest in Biblical ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Im a die hard Dads Army fan and nothing will e...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>This is the kind of picture John Lassiter woul...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>A MUST SEE! I saw WHIPPED at a press screening...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>NBC should be ashamed. I wouldn't allow my chi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>This movie is a clumsy mishmash of various gho...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>Formula movie about the illegitimate son of a ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label\n",
       "0     I grew up (b. 1965) watching and loving the Th...      0\n",
       "1     When I put this movie in my DVD player, and sa...      0\n",
       "2     Why do people who do not know what a particula...      0\n",
       "3     Even though I have great interest in Biblical ...      0\n",
       "4     Im a die hard Dads Army fan and nothing will e...      1\n",
       "...                                                 ...    ...\n",
       "4995  This is the kind of picture John Lassiter woul...      1\n",
       "4996  A MUST SEE! I saw WHIPPED at a press screening...      1\n",
       "4997  NBC should be ashamed. I wouldn't allow my chi...      0\n",
       "4998  This movie is a clumsy mishmash of various gho...      0\n",
       "4999  Formula movie about the illegitimate son of a ...      0\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies = pd.read_csv('dataset/movies.csv', index_col=0)\n",
    "movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-26T00:16:25.746815Z",
     "start_time": "2022-08-26T00:16:25.737806Z"
    }
   },
   "outputs": [],
   "source": [
    "movies_sample = movies.sample(frac=0.1, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-26T00:46:25.584363Z",
     "start_time": "2022-08-26T00:46:25.568349Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PreprocessPhrase' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [123]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m preprocess \u001b[38;5;241m=\u001b[39m \u001b[43mPreprocessPhrase\u001b[49m()\n\u001b[0;32m      2\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mremove_digits\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mremove_special_char\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstemmer\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      9\u001b[0m ]\n\u001b[0;32m     10\u001b[0m movies_sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiltered_words\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m movies_sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(preprocess\u001b[38;5;241m.\u001b[39mpipeline, methods\u001b[38;5;241m=\u001b[39mpipeline)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'PreprocessPhrase' is not defined"
     ]
    }
   ],
   "source": [
    "preprocess = PreProcesssPhrase()\n",
    "pipeline = [\n",
    "    'remove_digits',\n",
    "    'remove_special_char',\n",
    "    'word_lower',\n",
    "    'tokenizer',\n",
    "    'remove_stopwords',\n",
    "    'stemmer'\n",
    "]\n",
    "movies_sample[\"filtered_words\"] = movies_sample['text'].apply(preprocess.pipeline, methods=pipeline)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Aula 1 - NLP - Aux.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
