{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bf1a7ac",
   "metadata": {},
   "source": [
    "### Como utilizar ML com Dados Não Estruturais Textuais?\n",
    "\n",
    "* Técnicas de NLP preprocessam os dados para filtrarmos o que é mais relevante para a classificação \n",
    "\n",
    "![Title](../imgs/nlp-illustration.png)\n",
    "\n",
    "\n",
    "* A tokenização nos auxilia a dividir strings em palavras\n",
    "* Essas palavras serão as \"features\" que serão utilizadas para aprendizado de um modelo de Machine Learning\n",
    "* Um algoritmo de Machine Learning só entende números (atributos sempre numéricos), sendo assim, após a tokenização, precisamos converter nossas palavras para valores numéricos\n",
    "* O processo de transformação textual para base numérico denomina-se **bag of words**\n",
    "\n",
    "![Title](../imgs/1-Bag-of-words.png)\n",
    "\n",
    "* Técnicas como TF-IDF e CountVectorizer, apesar de eficientes, transformam o dataset em dado tabular\n",
    "  * Cada coluna é uma palavra\n",
    "  * Quando a palavra existir em uma dada amostra, substituimos o valor 0 pela frequência em que aquela palavra ocorre no texto\n",
    "  \n",
    "  <img src=\"https://www.researchgate.net/profile/Haider-Al-Khateeb/publication/291950178/figure/fig1/AS:330186932408324@1455734107458/Term-Frequency-Inverse-Document-Frequency-TF-IDF.png\" width=800>\n",
    "  \n",
    "* A consequencia disso é a perda da relação semântica nas frases\n",
    "* Perda de contexto das sentenças\n",
    "* Vocabulário imenso (maldição da dimensionalidade)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9434796",
   "metadata": {},
   "source": [
    "### Word Embeddings to rescue!\n",
    "\n",
    "* Vetor multidimensional de palavras\n",
    "* Mapa de características com a frequência de palavras e considerações semânticas \n",
    "* Baseado em similaridade\n",
    "* Aprendizado por meio de redes neurais\n",
    "\n",
    "##### Como fazemos isso?\n",
    "* Dada uma palavra da frase\n",
    "* Pegar as palavras vizinhas \n",
    "* Rede Neural irá calcular a probabilidade dessa palavra vizinha ser a palavra a ser predita\n",
    "* Selecionamos as palavras vizinhas definindo uma \"janela\" ou \"kernel\"\n",
    "  * Janela = 2 considere duas palavras ao redor da palavra atual\n",
    "![Title](../imgs/training_data.png)\n",
    "\n",
    "* Criamos um corpus com as palavras organizadas de acordo com as janelas em torno do dataset.\n",
    "* Palavras que aparecerem juntas tem maior probabilidade de terem significado semântico\n",
    "  * Exemplo: \"soviética\" provavelmente terá uma semelhança com união\n",
    "  \n",
    " ![Title](../imgs/vetor_multi.png)\n",
    " \n",
    "[Leitura adicional](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/): funcionamento de uma rede neural para embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7dfe2a",
   "metadata": {},
   "source": [
    "### Arquiteturas Word Embeddings\n",
    "\n",
    "* Existem diferentes formas de gerar nossas embeddings, gerando determinadas arquiteturas previamente conhecidos como CBOW e SkipGram. Essas arquiteturas também são denominadas como tipo word2vec (palavra para vetor)\n",
    "\n",
    "* CBOW: predizer uma palavra baseado nas palavras vizinhas. Mais rápido, funciona melhor nas palavras mais frequentes.\n",
    "* Skip-Gram: predizer o contexto baseado nas palavras vizinhas. Funciona melhor com datasets menores\n",
    "<img src=\"https://leimao.github.io/images/article/2019-08-23-Word2Vec-Classic/word2vec.png\">\n",
    "* Ambas arquiteturas tem uma ambientação semelhante, modificando a forma como a saída e a entrada são organizadas\n",
    "* Exemplo: Hoje vai fazer sol pela manhã com pancadas de chuva à tarde\n",
    "* Utilizando CBOW\n",
    "![Title](../imgs/cbow.png)\n",
    "* Utilizando Skip-Gram\n",
    "![Title](../imgs/skip_gram.png)\n",
    "\n",
    "[Leitura adicional](https://www.tensorflow.org/tutorials/text/word2vec): transformando o corpus do CBOW/skip_gram em numérico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa52844",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3ac1e972",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/cecilia/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/cecilia/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Baixa as listas de stopwords e as tokenizações\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Define as stopwords em inglês\n",
    "sw_english = set(stopwords.words('english'))\n",
    "\n",
    "# Instância o PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Carrega o conjunto de dados\n",
    "movies = pd.read_csv('../datasets/movies.csv', index_col = 0)\n",
    "\n",
    "# Retira uma amostra do conjunto de dados\n",
    "movies_sample = movies.sample(frac = 0.1, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1126dc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria um pipeline\n",
    "def preprocessing(string):\n",
    "    ###\n",
    "    # Deixa apenas elementos alfanuméricos\n",
    "    string = re.sub(r\"[^a-zA-Z0-9]+\", ' ', string)\n",
    "    ###\n",
    "    # deixa todas as palavras minúsculas\n",
    "    string = string.lower()\n",
    "    ###\n",
    "    # tokenização\n",
    "    words = word_tokenize(string)\n",
    "    ###\n",
    "    # Remove Stopwords\n",
    "    filtered_words = []\n",
    "    for w in words:\n",
    "        if w not in sw_english:\n",
    "            filtered_words.append(w)\n",
    "    ###\n",
    "    # Aplica o Stemming\n",
    "    stem_words = []\n",
    "    for w in filtered_words:\n",
    "        s_words = stemmer.stem(w)\n",
    "        stem_words.append(s_words)\n",
    "    ###\n",
    "    # Retorna a lista de palavras pré-processadas\n",
    "    return stem_words\n",
    "\n",
    "# Aplica o preprocessing nas críticas de filmes\n",
    "movies_sample[\"filtered_words\"] = movies_sample['text'].apply(lambda x: preprocessing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "992697a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>filtered_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36151</th>\n",
       "      <td>Well let me just say something about these act...</td>\n",
       "      <td>1</td>\n",
       "      <td>[well, let, say, someth, actor, realli, good, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31632</th>\n",
       "      <td>Now we know where they got the idea of Snakes ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[know, got, idea, snake, plane, put, bluntli, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13602</th>\n",
       "      <td>This was a great movie, and safe for the entir...</td>\n",
       "      <td>1</td>\n",
       "      <td>[great, movi, safe, entir, famili, one, see, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22447</th>\n",
       "      <td>Myron Breckinridge (Rex Reed!!!) gets a sex ch...</td>\n",
       "      <td>0</td>\n",
       "      <td>[myron, breckinridg, rex, reed, get, sex, chan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36511</th>\n",
       "      <td>I absolutely despise this movie. No wonder Jos...</td>\n",
       "      <td>0</td>\n",
       "      <td>[absolut, despis, movi, wonder, jose, larraz, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27014</th>\n",
       "      <td>despite the occasionally stilted acting and \"s...</td>\n",
       "      <td>1</td>\n",
       "      <td>[despit, occasion, stilt, act, seen, stori, fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2802</th>\n",
       "      <td>I like a lot of Myrna Loy movies. This film wa...</td>\n",
       "      <td>0</td>\n",
       "      <td>[like, lot, myrna, loy, movi, film, produc, ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13917</th>\n",
       "      <td>This film quite literally has every single act...</td>\n",
       "      <td>1</td>\n",
       "      <td>[film, quit, liter, everi, singl, action, movi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3696</th>\n",
       "      <td>This movie is without a doubt the worst horror...</td>\n",
       "      <td>0</td>\n",
       "      <td>[movi, without, doubt, worst, horror, movi, ev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3954</th>\n",
       "      <td>Michael Bassett's film 'Solomon Kane' (based o...</td>\n",
       "      <td>0</td>\n",
       "      <td>[michael, bassett, film, solomon, kane, base, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label  \\\n",
       "36151  Well let me just say something about these act...      1   \n",
       "31632  Now we know where they got the idea of Snakes ...      0   \n",
       "13602  This was a great movie, and safe for the entir...      1   \n",
       "22447  Myron Breckinridge (Rex Reed!!!) gets a sex ch...      0   \n",
       "36511  I absolutely despise this movie. No wonder Jos...      0   \n",
       "...                                                  ...    ...   \n",
       "27014  despite the occasionally stilted acting and \"s...      1   \n",
       "2802   I like a lot of Myrna Loy movies. This film wa...      0   \n",
       "13917  This film quite literally has every single act...      1   \n",
       "3696   This movie is without a doubt the worst horror...      0   \n",
       "3954   Michael Bassett's film 'Solomon Kane' (based o...      0   \n",
       "\n",
       "                                          filtered_words  \n",
       "36151  [well, let, say, someth, actor, realli, good, ...  \n",
       "31632  [know, got, idea, snake, plane, put, bluntli, ...  \n",
       "13602  [great, movi, safe, entir, famili, one, see, m...  \n",
       "22447  [myron, breckinridg, rex, reed, get, sex, chan...  \n",
       "36511  [absolut, despis, movi, wonder, jose, larraz, ...  \n",
       "...                                                  ...  \n",
       "27014  [despit, occasion, stilt, act, seen, stori, fa...  \n",
       "2802   [like, lot, myrna, loy, movi, film, produc, ch...  \n",
       "13917  [film, quit, liter, everi, singl, action, movi...  \n",
       "3696   [movi, without, doubt, worst, horror, movi, ev...  \n",
       "3954   [michael, bassett, film, solomon, kane, base, ...  \n",
       "\n",
       "[5000 rows x 3 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a27e343",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "# Define e treina o modelo\n",
    "model = Word2Vec(movies_sample[\"filtered_words\"], min_count = 2)\n",
    "model = Word2Vec(\n",
    "    sentences=movies_sample[\"filtered_words\"], # Passando a lista de tokens (List[List[str]])\n",
    "    vector_size=100,   # Tamanho do vetor de saída\n",
    "    min_count=1,  # Filtro palavras raras\n",
    "    workers=2,  # Multicore\n",
    "    window=5, # Janela de palavras, nesse caso 5 palavras\n",
    "    sg=1,  # 1 para skip-grams, 0 para o CBOW\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c6a05a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(model.wv.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f7e0efb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeiras 10 palavras:  ['br', 'movi', 'film', 'one', 'like', 'time', 'good', 'make', 'see', 'charact'] \n",
      "\n",
      "\n",
      "Palavras mais similares a time: \n",
      " [('occas', 0.7878702878952026), ('regret', 0.7604853510856628), ('nineti', 0.7588347792625427), ('odd', 0.7531259655952454), ('harder', 0.7500264048576355), ('dozen', 0.7498865723609924), ('preciou', 0.749779462814331), ('dreck', 0.7448720932006836), ('anticip', 0.7436569929122925), ('categori', 0.7422799468040466)] \n",
      "\n",
      "\n",
      "Vetor para a palavra time: \n",
      " [-0.07086917 -0.02880108  0.10448317 -0.05086483 -0.11392558 -0.28411302\n",
      "  0.09523632  0.24049914 -0.02056965 -0.16064933 -0.00274689  0.02376291\n",
      " -0.08728712 -0.01855629  0.03074314 -0.10383829  0.05066813 -0.05032621\n",
      " -0.09702493 -0.24207158  0.00690291  0.01385544  0.18003273  0.00049998\n",
      " -0.06703884 -0.0799669   0.00422336 -0.01062266 -0.16542329  0.04170448\n",
      "  0.07149941 -0.02969232  0.02628511 -0.2074668  -0.02223976  0.1612775\n",
      "  0.00636118 -0.01654712  0.09820938 -0.09244086  0.16032499 -0.10601716\n",
      " -0.01282839 -0.10655954  0.08468147 -0.00420409 -0.08448813  0.06729619\n",
      "  0.0106548   0.16013317  0.11969192 -0.07627161  0.05013746  0.11268555\n",
      " -0.13888289  0.16180281  0.16733375 -0.03124644 -0.16067925 -0.05330991\n",
      "  0.01807184  0.01946947 -0.00877138 -0.09161434 -0.08339553  0.15334941\n",
      " -0.06180122  0.16358346 -0.1054152  -0.00070434 -0.04771571  0.08595454\n",
      "  0.19117783  0.01096092 -0.00285111  0.02300196 -0.09672484 -0.04240683\n",
      " -0.02690984 -0.07867223 -0.10955988  0.01070315 -0.04364466 -0.00171053\n",
      " -0.09576893 -0.0337754   0.06220873  0.06840274  0.12668435 -0.02779801\n",
      "  0.07482631  0.12639865  0.08201265  0.01449784  0.18752155  0.1563244\n",
      " -0.02428639 -0.04682143 -0.02055722 -0.11864328] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Mostra as primeiras 10 palavras do vocabulário\n",
    "print(\"Primeiras 10 palavras: \", words[:10], '\\n\\n')\n",
    "\n",
    "# a partir da palavra time, determinar as mais similares a ela segundo os textos\n",
    "print('Palavras mais similares a time: \\n', model.wv.most_similar('time'), '\\n\\n')\n",
    "\n",
    "# Determinando o mapa característico para a palavra time\n",
    "print('Vetor para a palavra time: \\n', model.wv.get_vector('time', norm = True), '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0301f85b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base X treinável: \n",
      " [[ 0.05439223  0.04718336  0.47860208 ... -0.25150117  0.11204524\n",
      "   0.10274327]\n",
      " [-0.30462608  0.27046406  0.34967715 ... -0.09623296 -0.15711568\n",
      "  -0.14566195]\n",
      " [-0.245333    0.1662296   0.29211605 ... -0.12276348  0.09712406\n",
      "   0.01893252]\n",
      " ...\n",
      " [-0.1259432   0.07642938  0.04828117 ... -0.10612808  0.03402569\n",
      "   0.05972762]\n",
      " [-0.12425543  0.07222645  0.04515435 ... -0.09308802  0.02037374\n",
      "   0.03676774]\n",
      " [-0.12848416  0.07137331  0.04459506 ... -0.10968777  0.02961594\n",
      "   0.05630128]]\n"
     ]
    }
   ],
   "source": [
    "# transforma os vetores do word2vec para uma base X treinável\n",
    "X = model.wv[words]\n",
    "\n",
    "# Mostra a base\n",
    "print('Base X treinável: \\n', X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f51d54d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.55063856"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('peopl', 'like')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77aeb6fe",
   "metadata": {},
   "source": [
    "### Treinamento de ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9b8cecc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega o train_test_split para separar a base em treino e teste\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separa os dados em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(movies_sample[\"filtered_words\"],\n",
    "                                                     movies_sample['label'],\n",
    "                                                     test_size = 0.3,\n",
    "                                                     random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aee7f065",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Define e treina o modelo\n",
    "model = Word2Vec(X_train, min_count = 2)\n",
    "#model = Word2Vec(\n",
    "#    sentences=movies_sample[\"filtered_words\"], # Passando a lista de tokens (List[List[str]])\n",
    "#    vector_size=100,   # Tamanho do vetor de saída\n",
    "#    min_count=1,  # Filtro palavras raras\n",
    "#    workers=2,  # Multicore\n",
    "#    window=5, # Janela de palavras, nesse caso 5 palavras\n",
    "#    sg=1,  # 1 para skip-grams, 0 para o CBOW\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6770027b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t4/7csz5lt17fs3dd7hl2vvgns00000gn/T/ipykernel_67780/990797624.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  X_train_vect = np.array([np.array([model.wv[i] for i in ls if i in words])\n",
      "/var/folders/t4/7csz5lt17fs3dd7hl2vvgns00000gn/T/ipykernel_67780/990797624.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  X_test_vect = np.array([np.array([model.wv[i] for i in ls if i in words])\n"
     ]
    }
   ],
   "source": [
    "words = set(model.wv.index_to_key )\n",
    "X_train_vect = np.array([np.array([model.wv[i] for i in ls if i in words])\n",
    "                         for ls in X_train])\n",
    "X_test_vect = np.array([np.array([model.wv[i] for i in ls if i in words])\n",
    "                         for ls in X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d3317f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciando o modelo\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# treinamento do modelo\n",
    "model.fit(X_train_vect, y_train)\n",
    "\n",
    "# Gerando as predições\n",
    "y_pred = model.predict(X_test_vect)\n",
    "\n",
    "# Print do classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9978ce84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
