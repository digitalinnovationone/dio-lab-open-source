{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aprendizagem não-supervisionada e clusterização\n",
    "\n",
    "Na aula de hoje, vamos explorar os seguintes tópicos em Python:\n",
    "\n",
    "- 1) Aprendizagem não-supervisionada\n",
    "- a) Clusterização\n",
    "- b) Redução de dimensionalidade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "____\n",
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Aprendizagem não-supervisionada\n",
    "\n",
    "Chegamos ao nosso último tópico do módulo: **aprendizagem não-supervisionada (unsurpervised learning)**.\n",
    "\n",
    "Este tipo de aprendizagem se diferencia da aprendizagem supervisionada de modo muito simples: **os targets não fazem parte da base de dados!**\n",
    "\n",
    "> Na aprendizagem não-supervisionada, temos acesso apenas ao conjunto de features, $\\{\\vec{x}_i\\}_{i=1}^N$\n",
    "\n",
    "A perda que temos com relação à aprendizagem supervisionada é gigante: sem os targets, torna-se impossível a estimação do processo teórico $\\mathcal{F}$ que gerou os dados!\n",
    "\n",
    "Assim, o máximo que podemos fazer na aprendizagem não-supervisionada é a **determinação de estrutura nos dados**:\n",
    "\n",
    "<img src=https://www.researchgate.net/profile/Zhenyu-Wen-2/publication/336642133/figure/fig3/AS:815304842170368@1571395230317/Examples-of-Supervised-Learning-Linear-Regression-and-Unsupervised-Learning.png width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em um problema de classificação, somos capazes de encontrar a fronteira de decisão dentre as classes que **são conhecidas no treino**:\n",
    "\n",
    "<img src=https://s3-sa-east-1.amazonaws.com/lcpi/f29c8ebf-dd5f-4fce-99bb-86ec8af21f51.PNG width=700>\n",
    "\n",
    "Já em com dados não-supervisionados, o máximo que podemos fazer é encontrar a estrutura presente nos dados (e com maior dificuldade!)\n",
    "\n",
    "<img src=https://s3-sa-east-1.amazonaws.com/lcpi/0c7b530d-e74b-4886-9601-740d054aa166.PNG width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para muitas aplicações, isso já é suficiente: basta saber que os dados estão estruturados (agrupados/segmentados), sendo o significado de cada grupo/segmento de menor interesse, ou facilmente estimado de outra forma; ou, então, determinar aspectos importantes das features por si só, sem qualquer preocupação com o target.\n",
    "\n",
    "Neste curso, veremos dois grandes grupos de **técnicas não-supervisionadas**:\n",
    "\n",
    "- Clusterização - forma de encontrar grupos (clusters) nos dados;\n",
    "- Redução de dimensionalidade - importante processo de pré-processamento que visa reduzir o número de dimensões (features) de um dataset.\n",
    "\n",
    "Na aula de hoje, veremos técnicas de clusterização!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______\n",
    "\n",
    "### Clusterização\n",
    "\n",
    "Este tipo de problema consiste em __agrupar__ itens semelhantes, isto é, criar __grupos__ (ou __clusters__) dos dados que são parecidos entre si.\n",
    "\n",
    "> O objetivo central é **dividir os dados em grupos distintos**, tais que **membros de cada grupo sejam similares entre si**  **diferentes a membros nos outros grupos**\n",
    "\n",
    "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSjleb9bnO4ivvONN70Fc5k4Xs48cab_4i0phSc_l0qvVndo8VFs0B5d0jCEWeNOk0pG3Y&usqp=CAU\" width=600><br><br>\n",
    "\n",
    "Problemas como estes podem aparecer em diversos contextos:\n",
    "\n",
    "- Identificação de tipos de clientes parecidos, para o direcionamento de marketing;\n",
    "- Agrupamento de cidades próximas para melhor logística de entrega de produtos;\n",
    "- Identificação de padrões climáticos;\n",
    "- Identificação de genes relacionados à determinada doença;\n",
    "- Identificação de documentos semelhantes em processos legais;\n",
    "\n",
    "...e qualquer outro problema em que você deseje **agrupar dados similares** ou ainda **encontrar alguma estrutura nos seus dados!**, mas tudo isso no que diz respeito ùnicamente **às features**!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "___\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Redução de dimensionalidade\n",
    "\n",
    "Para efetuar redução de dimensionalidade, basicamente há dois métodos: \n",
    "- extração de características (os algoritmos de extração de características criam novas características a partir de transformações ou combinações do conjunto de características original)\n",
    "- seleção de características (os algoritmos de seleção de características selecionam, segundo determinado critério, o melhor subconjunto do conjunto de características original)\n",
    "\n",
    "Em geral a seleção de características reduz o custo de medição de dados, e as características selecionadas mantém sua interpretação física original, mantendo as propriedades que possuíam quando foram criadas. Já as características transformadas geradas por extração de características podem prover uma habilidade de discriminação melhor que o melhor subconjunto das características originais, mas as novas características (combinações lineares ou não lineares das características originais) podem não possuir um significado físico.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://miro.medium.com/max/1396/1*WVFe7w1rzZWsmghdvaoXag.png' width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redução de dimensionalidade é uma técnica bastante usada em datasets com o objetivo de aumentarmos a interpretabilidade dos dados minimizando a quantidade de informação perdida no processo.<br><br>\n",
    "\n",
    "Uma das técnicas utilizadas para esse processo é chamada **Principal Component Analysis (PCA)**.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://www.analyticsvidhya.com/wp-content/uploads/2016/03/2-1-e1458494877196.png\" width=800>\n",
    "\n",
    "Focaremos aqui no processo na interpretabilidade do processo pois a base matemática por trás do PCA é bem complexa. Mas esse processo basicamente reduz a dimensionalidade do nosso dataset em um valor n, seja a quantidade de dimensões que quisermos, e o que esse processo retorna para gente são features que representam a variabilidade dos nossos dados.<br><br>\n",
    "A vantagem desse processo é que ganhamos uma **interpretação gráfica dos dados*** minimizando a perda de informação e é um processo interessante para utilizarmos para **testar modelos quando tivermos uma base de dados muito grande**, pois testamos o modelo com poucas features mas sem perder o valor dos dados originais.\n",
    "\n",
    "*Visualizamos os dados até 3 dimensões"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
