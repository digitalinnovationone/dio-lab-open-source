{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula 6 - métodos de ensemble (bagging e boosting)\n",
    "\n",
    "Na aula de hoje, vamos explorar os seguintes tópicos em Python:\n",
    "\n",
    "- 1) Métodos de ensemble\n",
    "- 2) Bagging & Random Forest\n",
    "- 3) Boosting & AdaBoost\n",
    "- 4) Gradient Boosting\n",
    "- 5) XGBoost\n",
    "- 6) LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No fim da aula de hoje, vamos conhecer duas novas bibliotecas. Vamos já instalá-las, pra adiantar:\n",
    "\n",
    "`pip install xgboost`\n",
    "\n",
    "`pip install lightgbm`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T22:08:39.890765Z",
     "start_time": "2022-06-07T22:08:39.686758Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T22:08:59.159505Z",
     "start_time": "2022-06-07T22:08:39.894760Z"
    }
   },
   "outputs": [],
   "source": [
    "from ml_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "____\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Métodos de Ensemble\n",
    "\n",
    "\n",
    "Há uma classe de algoritmos de Machine Learning, os chamados **métodos de ensemble** que têm como objetivo **combinar as predições de diversos estimadores mais simples** para gerar uma **predição final mais robusta**.\n",
    "\n",
    "Os métodos de ensemble costuman ser divididos em duas classes:\n",
    "\n",
    "- **Métodos de média**: têm como procedimento geral construir diversos estimadores independentes, e tomar a média de suas predições como a predição final. O principal objetivo do método é reduzir **variância**, de modo que o modelo final seja melhor que todos os modelos individuais. Ex.: **bagging & random forest.**\n",
    "<br>\n",
    "\n",
    "- **Métodos de boosting**: têm como procedimento geral a construção de estimadores de forma sequencial, de modo que estimadores posteriores tentam reduzir o **viés** do estimador conjunto, que leva em consideração estimadores anteriores. Ex.: **adaboost, gradient boosting**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Há, ainda, uma terceira classe de método de ensemble, o chamado [stacking ensemble](https://machinelearningmastery.com/stacking-ensemble-machine-learning-with-python/), que consiste em \"empilhar\" modelos de modo a produzir a mistura. Não veremos esta modalidade em detalhes, mas deixo como sugestão para estudos posteriores! :)\n",
    "\n",
    "Para mais detalhes sobre métodos de ensemble no contexto do sklearn, [clique aqui!](https://scikit-learn.org/stable/modules/ensemble.html)\n",
    "\n",
    "Na aula de hoje, vamos conhecer em detalhes os procedimentos de bagging e boosting, ilustrados pelos métodos Random Forest e AdaBoost/Gradient Boosting, respectivamente. Vamos lá!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________\n",
    "_______\n",
    "_________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Bagging & Random Forest\n",
    "\n",
    "Uma técnica muito interessante (e muito performática!) baseada em árvores é o **Random Forest**.\n",
    "\n",
    "Neste método, são criadas varias **árvores diferentes e independentes entre si**, através de um processo **aleatório**, e a predição final é tomada através da média das predições individuais!\n",
    "\n",
    "<img src=\"https://i.ytimg.com/vi/goPiwckWE9M/maxresdefault.jpg\" width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O Random Forest utiliza os conceitos de **bootstrapping** e **aggregation** (ou então, o procedimento composto **bagging**) para criar um modelo composto que é melhor que uma única árvore!\n",
    "\n",
    "<img src=\"https://c.mql5.com/2/33/image1__1.png\" width=800>\n",
    "\n",
    "Vamos entrender um pouco melhor cada componente do método!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrapping\n",
    "\n",
    "O procedimento de **bootstrapping** é utilizado no contexto do random forest para gerar os chamados **bootstrapped datasets**.\n",
    "\n",
    "A ideia é bem simples! Para a criação de cada bootstrapped dataset, primeiro:\n",
    "\n",
    "> Selecionamos **aleatoriamente com reposição** algumas linhas da base original. Isso gera um novo dataset (reamostrado), chamado de **bootstrapped dataset**. O número de linhas do dataset reamostrado é controlável.\n",
    "\n",
    "Logo após, fazemos uma árvore de decisão **treinada neste dataset reamostrado**. Mas, com um detalhe:\n",
    "\n",
    "> Usamos apenas um **subconjunto aleatório das features** em cada avaliação de quebras (isso equivale ao `splitter=\"random\"`). A quantidade de features a serem consideradas é controlável.\n",
    "\n",
    "Com isso, muitas árvores são geradas (a quantidade também é controlável), cada uma seguindo o procedimento de bootstrap!\n",
    "\n",
    "Note que o o procedimento de bootstrapping introduz **duas fontes de aleatoriedade**, cujo objetivo é **diminuir a variância** (tendência a overfitting) do modelo.\n",
    "\n",
    "De fato, árvores individuais são facilmente overfitadas, como discutimos em aula (lembre-se da grande flexibilidade da hipótese em encontrar condições favoráveis à aprendizagem dos ruídos!).\n",
    "\n",
    "Com esta aleatorização introduzida pelo bootstrapping, o objetivo é que as árvores construídas sejam **independentes**, de modo que **os erros cometidos por cada uma sejam independentes**. \n",
    "\n",
    "Deste modo, se considerarmos as previsões isoladas e de alguma forma **agregar** as previsões, a expectativa é que o modelo final seja **menos propenso a overfitting**! Mas, uma pergunta natural é: o que é essa \"agragação\"? Aqui entra o segundo elemento do bagging..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregation\n",
    "\n",
    "Entendemos como o bootstrap é utilizado para gerar várias árvores independentes. \n",
    "\n",
    "Então, quando temos uma nova observação para atribuir o target, passamos as features **por cada uma das árvores**, e, naturalmente, cada árvore produz **o seu target**, que pode muito bem não ser o mesmo!\n",
    "\n",
    "A **agregação** é utilizada para tomar a decisão final:\n",
    "\n",
    "> No caso de classificação, a classe final é atribuída como **a classe majoritária**, isso é, **a classe que foi o output $\\hat{y}$ mais vezes dentre todas as árvores**;\n",
    "\n",
    "> No caso de regressão, o valor final é atribuído como **a média dos valores preditos $\\hat{y}$ por cada árvore**.\n",
    "\n",
    "Note que em ambos os casos, o procedimento de agregação pode ser visto como uma **média**, e o sklearn deixa isso explícito: \"*In contrast to the original publication, the scikit-learn implementation combines classifiers by averaging their probabilistic prediction, instead of letting each classifier vote for a single class.*\"\n",
    "\n",
    "Tomando a média como procedimento de agregação, a expectativa é que **alguns erros sejam anulados**, garantindo uma previsão final **mais estável e mais generalizável**, dado que os ruídos são eliminados.\n",
    "\n",
    "Juntando o bootstrapping com o aggregation, temos então o..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging\n",
    "\n",
    "> Bagging: **b**ootstrap **agg**regat**ing**\n",
    "\n",
    "Esquematicamente:\n",
    "\n",
    "<img src=https://media.geeksforgeeks.org/wp-content/uploads/20210707140912/Bagging.png width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As classes do random forest são:\n",
    "\n",
    "- [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier)\n",
    "\n",
    "- [RandomForestRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor)\n",
    "\n",
    "Ambos os métodos têm hiperparâmetros similares aos hiperparâmetros das árvores convencionais, aplicados a cada uma das árvores independentes.\n",
    "\n",
    "Além destes, há dois hiperparâmetros bem importantes, referentes ao método de ensemble em si:\n",
    "\n",
    "- `n_estimators` : controla quantas árvores independentes serão construídas (i.e., o número de árvores na floresta). Em geral, quanto mais árvores melhor (mas mais tempo vai demorar). Além disso, depois de uma determinade quantidade de árvores, os resultados vão parar de melhorar, pois há um limite para o bootstrap: depois de uma certa quantidade, as árvores deixam de ser tão independentes assim...\n",
    "<br>\n",
    "\n",
    "- `max_features`: o número de features no subconjunto aleatório de candidata a serem utilizadas em cada quebra. Quanto menor for o valor, mais conseguimos reduzir o overfitting, mas o underfitting é favorecido. Uma boa heurística é `max_features=None` para regressão e `max_features=\"sqrt\"`para classificação, embora estratégias diferentes podem (e devem) ser testadas com o CV.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "Para uma explicação bem visual sobre o funcionamento deste método, sugiro os vídeos do canal [StatQuest](https://www.youtube.com/watch?v=J4Wdy0Wc_xQ). \n",
    "\n",
    "Obs.: toda a [playlist de machine learning](https://www.youtube.com/playlist?list=PLblh5JKOoLUICTaGLRoHQDuF_7q2GfuJF) é muitíssimo interessante, com vídeos super claros e ilustrativos! Além disso, há outros vídeos de estatística que são muito bons! Este é um dos melhores canais no youtube para se aprender de forma clara e descontraída sobre estatística e machine learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____________\n",
    "\n",
    "Agora, vamos ver o Random Forest em ação, na base de risco de crédito!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T22:08:59.779179Z",
     "start_time": "2022-06-07T22:08:59.165502Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../datasets/german_credit_data.csv\", index_col=0)\n",
    "\n",
    "X = df.select_dtypes(include=np.number)\n",
    "y = df[\"Risk\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T22:11:10.447229Z",
     "start_time": "2022-06-07T22:11:10.241366Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "675    good\n",
       "703    good\n",
       "12     good\n",
       "845    good\n",
       "795    good\n",
       "       ... \n",
       "284    good\n",
       "169     bad\n",
       "856    good\n",
       "655    good\n",
       "695    good\n",
       "Name: Risk, Length: 800, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T22:14:03.449748Z",
     "start_time": "2022-06-07T22:14:03.273980Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T22:19:23.623651Z",
     "start_time": "2022-06-07T22:19:22.851564Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Métricas de avaliação de treino - com cutoff = 0.50\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bad       1.00      1.00      1.00       240\n",
      "        good       1.00      1.00      1.00       560\n",
      "\n",
      "    accuracy                           1.00       800\n",
      "   macro avg       1.00      1.00      1.00       800\n",
      "weighted avg       1.00      1.00      1.00       800\n",
      "\n",
      "\n",
      "################################################################################\n",
      "\n",
      "Métricas de avaliação de teste - com cutoff = 0.50\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bad       0.42      0.25      0.31        60\n",
      "        good       0.73      0.85      0.78       140\n",
      "\n",
      "    accuracy                           0.67       200\n",
      "   macro avg       0.57      0.55      0.55       200\n",
      "weighted avg       0.63      0.67      0.64       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(random_state=42).fit(X_train, y_train)\n",
    "\n",
    "_ = clf_metrics_train_test(rf, X_train, y_train, X_test, y_test, cutoff=0.5, \n",
    "                           print_plot=True, plot_conf_matrix=False, print_cr=True, pos_label=\"bad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T22:17:57.677052Z",
     "start_time": "2022-06-07T22:17:57.456180Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # caso queiramos acessar as árvores que compõem a floresta\n",
    "# rf.estimators_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T22:26:35.861908Z",
     "start_time": "2022-06-07T22:26:07.328451Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Métricas de avaliação de treino - com cutoff = 0.50\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bad       1.00      1.00      1.00       240\n",
      "        good       1.00      1.00      1.00       560\n",
      "\n",
      "    accuracy                           1.00       800\n",
      "   macro avg       1.00      1.00      1.00       800\n",
      "weighted avg       1.00      1.00      1.00       800\n",
      "\n",
      "\n",
      "################################################################################\n",
      "\n",
      "Métricas de avaliação de teste - com cutoff = 0.50\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bad       0.42      0.25      0.31        60\n",
      "        good       0.73      0.85      0.78       140\n",
      "\n",
      "    accuracy                           0.67       200\n",
      "   macro avg       0.57      0.55      0.55       200\n",
      "weighted avg       0.63      0.67      0.64       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=5000, random_state=42).fit(X_train, y_train)\n",
    "\n",
    "_ = clf_metrics_train_test(rf, X_train, y_train, X_test, y_test, cutoff=0.5, \n",
    "                           print_plot=True, plot_conf_matrix=False, print_cr=True, pos_label=\"bad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **DICA**: o random forest é altamente paralelizável! (afinal, as árvores são independentes).\n",
    "> Por este motivo, vale a pena utilizar o argumento `n_jobs`, para paralelizar e acelerar os cálculos!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T22:30:39.364291Z",
     "start_time": "2022-06-07T22:30:09.995543Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Métricas de avaliação de treino - com cutoff = 0.50\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bad       1.00      1.00      1.00       240\n",
      "        good       1.00      1.00      1.00       560\n",
      "\n",
      "    accuracy                           1.00       800\n",
      "   macro avg       1.00      1.00      1.00       800\n",
      "weighted avg       1.00      1.00      1.00       800\n",
      "\n",
      "\n",
      "################################################################################\n",
      "\n",
      "Métricas de avaliação de teste - com cutoff = 0.50\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bad       0.42      0.25      0.31        60\n",
      "        good       0.73      0.85      0.78       140\n",
      "\n",
      "    accuracy                           0.67       200\n",
      "   macro avg       0.57      0.55      0.55       200\n",
      "weighted avg       0.63      0.67      0.64       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=5000, random_state=42, n_jobs=-1).fit(X_train, y_train)\n",
    "\n",
    "_ = clf_metrics_train_test(rf, X_train, y_train, X_test, y_test, cutoff=0.5, \n",
    "                           print_plot=True, plot_conf_matrix=False, print_cr=True, pos_label=\"bad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apesar da performance relativamente boa no teste, é evidente que nosso modelo ainda está overfitado!\n",
    "\n",
    "Isso é algo muito interessante do random forest: apesar de ser possível overfitá-lo, **a variância do erro de generalização vai a zero, conforme mais árvores são adicionadas**:\n",
    "\n",
    "<img src=https://i.stack.imgur.com/8GU8U.png width=500>\n",
    "\n",
    "Ou seja, um modelo de random forest **tende a ser mais estável** no que diz respeito à generalização!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para evitar o overfitting em si, podemos usar as mesmas técnicas de regularização das árvores individuais, e aumentar o número de árvores na floresta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T22:37:23.285152Z",
     "start_time": "2022-06-07T22:37:19.249532Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Métricas de avaliação de treino - com cutoff = 0.50\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bad       0.92      0.24      0.38       240\n",
      "        good       0.75      0.99      0.86       560\n",
      "\n",
      "    accuracy                           0.77       800\n",
      "   macro avg       0.84      0.61      0.62       800\n",
      "weighted avg       0.80      0.77      0.71       800\n",
      "\n",
      "\n",
      "################################################################################\n",
      "\n",
      "Métricas de avaliação de teste - com cutoff = 0.50\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bad       0.55      0.18      0.27        60\n",
      "        good       0.73      0.94      0.82       140\n",
      "\n",
      "    accuracy                           0.71       200\n",
      "   macro avg       0.64      0.56      0.55       200\n",
      "weighted avg       0.67      0.71      0.66       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=1000, max_depth=5,\n",
    "                            random_state=42, n_jobs=-1).fit(X_train, y_train)\n",
    "\n",
    "_ = clf_metrics_train_test(rf, X_train, y_train, X_test, y_test, cutoff=0.5, \n",
    "                           print_plot=True, plot_conf_matrix=False, print_cr=True, pos_label=\"bad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T22:39:22.295036Z",
     "start_time": "2022-06-07T22:39:17.963435Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Métricas de avaliação de treino - com cutoff = 0.50\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bad       0.89      0.20      0.32       240\n",
      "        good       0.74      0.99      0.85       560\n",
      "\n",
      "    accuracy                           0.75       800\n",
      "   macro avg       0.81      0.59      0.58       800\n",
      "weighted avg       0.79      0.75      0.69       800\n",
      "\n",
      "\n",
      "################################################################################\n",
      "\n",
      "Métricas de avaliação de teste - com cutoff = 0.50\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bad       0.64      0.15      0.24        60\n",
      "        good       0.73      0.96      0.83       140\n",
      "\n",
      "    accuracy                           0.72       200\n",
      "   macro avg       0.68      0.56      0.54       200\n",
      "weighted avg       0.70      0.72      0.65       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=1000, max_depth=4,\n",
    "                            random_state=42, n_jobs=-1).fit(X_train, y_train)\n",
    "\n",
    "_ = clf_metrics_train_test(rf, X_train, y_train, X_test, y_test, cutoff=0.5, \n",
    "                           print_plot=True, plot_conf_matrix=False, print_cr=True, pos_label=\"bad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É interessante também ajustar os hiperparâmetros específicos do bootstraping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T22:41:27.346731Z",
     "start_time": "2022-06-07T22:41:27.175818Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 4)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T22:42:27.297736Z",
     "start_time": "2022-06-07T22:42:23.638601Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Métricas de avaliação de treino - com cutoff = 0.50\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bad       0.89      0.30      0.45       240\n",
      "        good       0.77      0.98      0.86       560\n",
      "\n",
      "    accuracy                           0.78       800\n",
      "   macro avg       0.83      0.64      0.66       800\n",
      "weighted avg       0.80      0.78      0.74       800\n",
      "\n",
      "\n",
      "################################################################################\n",
      "\n",
      "Métricas de avaliação de teste - com cutoff = 0.50\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bad       0.54      0.22      0.31        60\n",
      "        good       0.73      0.92      0.82       140\n",
      "\n",
      "    accuracy                           0.71       200\n",
      "   macro avg       0.64      0.57      0.56       200\n",
      "weighted avg       0.68      0.71      0.66       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=1000, max_samples=100,\n",
    "                            random_state=42, n_jobs=-1).fit(X_train, y_train)\n",
    "\n",
    "_ = clf_metrics_train_test(rf, X_train, y_train, X_test, y_test, cutoff=0.5, \n",
    "                           print_plot=True, plot_conf_matrix=False, print_cr=True, pos_label=\"bad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T22:43:03.986256Z",
     "start_time": "2022-06-07T22:42:59.470642Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Métricas de avaliação de treino - com cutoff = 0.50\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bad       0.81      0.25      0.38       240\n",
      "        good       0.75      0.97      0.85       560\n",
      "\n",
      "    accuracy                           0.76       800\n",
      "   macro avg       0.78      0.61      0.62       800\n",
      "weighted avg       0.77      0.76      0.71       800\n",
      "\n",
      "\n",
      "################################################################################\n",
      "\n",
      "Métricas de avaliação de teste - com cutoff = 0.50\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bad       0.55      0.18      0.27        60\n",
      "        good       0.73      0.94      0.82       140\n",
      "\n",
      "    accuracy                           0.71       200\n",
      "   macro avg       0.64      0.56      0.55       200\n",
      "weighted avg       0.67      0.71      0.66       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=1000, max_samples=0.1,\n",
    "                            random_state=42, n_jobs=-1).fit(X_train, y_train)\n",
    "\n",
    "_ = clf_metrics_train_test(rf, X_train, y_train, X_test, y_test, cutoff=0.5, \n",
    "                           print_plot=True, plot_conf_matrix=False, print_cr=True, pos_label=\"bad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T22:44:05.218859Z",
     "start_time": "2022-06-07T22:44:01.494216Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Métricas de avaliação de treino - com cutoff = 0.50\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bad       0.81      0.25      0.38       240\n",
      "        good       0.75      0.97      0.85       560\n",
      "\n",
      "    accuracy                           0.76       800\n",
      "   macro avg       0.78      0.61      0.62       800\n",
      "weighted avg       0.77      0.76      0.71       800\n",
      "\n",
      "\n",
      "################################################################################\n",
      "\n",
      "Métricas de avaliação de teste - com cutoff = 0.50\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bad       0.55      0.18      0.27        60\n",
      "        good       0.73      0.94      0.82       140\n",
      "\n",
      "    accuracy                           0.71       200\n",
      "   macro avg       0.64      0.56      0.55       200\n",
      "weighted avg       0.67      0.71      0.66       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=1000, max_samples=0.1, max_features=2,\n",
    "                            random_state=42, n_jobs=-1).fit(X_train, y_train)\n",
    "\n",
    "_ = clf_metrics_train_test(rf, X_train, y_train, X_test, y_test, cutoff=0.5, \n",
    "                           print_plot=True, plot_conf_matrix=False, print_cr=True, pos_label=\"bad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Será que dá pra melhorar?? Podemos construir uma pipeline e fazer o grid/random search para buscar o melhor modelo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-02T19:27:33.621182Z",
     "start_time": "2022-06-02T19:27:33.609173Z"
    }
   },
   "outputs": [],
   "source": [
    "# façam o gird/random search, pipeline completa...\n",
    "# obs: cuidado com o parâmetro n_estimators! se a grade incluir muitas árvores, vai demorar!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________\n",
    "_______\n",
    "_________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Boosting & AdaBoost\n",
    "\n",
    "O AdaBoost significa **Adaptive Boosting**, e tem como procedimento geral **a criação sucessiva dos chamados weak learners**, que são modelos bem fracos de aprendizagem - geralmente, **árvores de um único nó (stumps)**:\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1744/1*nJ5VrsiS1yaOR77d4h8gyw.png\" width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O AdaBoost utiliza os **erros da árvore anterior para melhorar a próxima árvore**. As predições finais são feitas com base **nos pesos de cada stump**, cuja determinação faz parte do algoritmo!\n",
    "\n",
    "<img src=\"https://static.packt-cdn.com/products/9781788295758/graphics/image_04_046-1.png\" width=700>\n",
    "\n",
    "Vamos entender um pouco melhor..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui, o bootstrapping não é utilizado: o método começa treinando um classificador fraco **no dataset original**, e depois treina diversas cópias adicionais do classificador **no mesmo dataset**, mas dando **um peso maior às observações que foram classificadas erroneamente** (ou, no caso de regressões, a observações **com o maior erro**).\n",
    "\n",
    "Assim, após diversas iterações, classificadores/regressores vão sequencialmente \"focando nos casos mais difíceis\", e construindo um classificador encadeado que seja forte, apesar de utilizar diversos classificadores fracos em como elementos fundamentais.\n",
    "\n",
    "<img src=\"https://www.researchgate.net/profile/Zhuo_Wang8/publication/288699540/figure/fig9/AS:668373486686246@1536364065786/Illustration-of-AdaBoost-algorithm-for-creating-a-strong-classifier-based-on-multiple.png\" width=500>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De forma resumida, as principais ideias por trás deste algoritmo são:\n",
    "\n",
    "- O algoritmo cria e combina um conjunto de **modelos fracos** (em geral, stumps);\n",
    "- Cada stump é criado **levando em consideração os erros do stump anterior**;\n",
    "- Alguns dos stumps têm **maior peso de decisão** do que outros na predição final;\n",
    "\n",
    "As classes no sklearn são:\n",
    "\n",
    "- [AdaBoostClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html)\n",
    "\n",
    "- [AdaBoostRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html#sklearn.ensemble.AdaBoostRegressor)\n",
    "\n",
    "Note que não há muitos hiperparâmetros. O mais importante, que deve ser tunado com o grid/random search, é:\n",
    "\n",
    "- `n_estimators` : o número de weak learners encadeados;\n",
    "\n",
    "Além disso, pode também ser interessante tunar os hiperparâmetros dos weak learners. Isso é possível de ser feito, como veremos a seguir!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiro, vamos começar com nosso baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T23:32:26.856144Z",
     "start_time": "2022-06-07T23:32:26.603769Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T23:34:47.838406Z",
     "start_time": "2022-06-07T23:34:47.232138Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Métricas de avaliação de treino - com cutoff = 0.50\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bad       0.65      0.32      0.43       240\n",
      "        good       0.76      0.93      0.84       560\n",
      "\n",
      "    accuracy                           0.74       800\n",
      "   macro avg       0.71      0.62      0.63       800\n",
      "weighted avg       0.73      0.74      0.71       800\n",
      "\n",
      "\n",
      "################################################################################\n",
      "\n",
      "Métricas de avaliação de teste - com cutoff = 0.50\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bad       0.44      0.27      0.33        60\n",
      "        good       0.73      0.86      0.79       140\n",
      "\n",
      "    accuracy                           0.68       200\n",
      "   macro avg       0.59      0.56      0.56       200\n",
      "weighted avg       0.65      0.68      0.65       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ab = AdaBoostClassifier(random_state=42).fit(X_train, y_train)\n",
    "\n",
    "_ = clf_metrics_train_test(ab, X_train, y_train, X_test, y_test, cutoff=0.5, \n",
    "                           print_plot=True, plot_conf_matrix=False, print_cr=True, pos_label=\"bad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos deixar o `base_estimator` explícito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T23:37:05.342003Z",
     "start_time": "2022-06-07T23:37:05.143116Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T23:39:17.893635Z",
     "start_time": "2022-06-07T23:39:17.452377Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Métricas de avaliação de treino - com cutoff = 0.50\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bad       0.65      0.32      0.43       240\n",
      "        good       0.76      0.93      0.84       560\n",
      "\n",
      "    accuracy                           0.74       800\n",
      "   macro avg       0.71      0.62      0.63       800\n",
      "weighted avg       0.73      0.74      0.71       800\n",
      "\n",
      "\n",
      "################################################################################\n",
      "\n",
      "Métricas de avaliação de teste - com cutoff = 0.50\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bad       0.44      0.27      0.33        60\n",
      "        good       0.73      0.86      0.79       140\n",
      "\n",
      "    accuracy                           0.68       200\n",
      "   macro avg       0.59      0.56      0.56       200\n",
      "weighted avg       0.65      0.68      0.65       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "basal = DecisionTreeClassifier(max_depth=1, random_state=42)\n",
    "\n",
    "ab = AdaBoostClassifier(base_estimator=basal,\n",
    "                        random_state=42).fit(X_train, y_train)\n",
    "\n",
    "_ = clf_metrics_train_test(ab, X_train, y_train, X_test, y_test, cutoff=0.5, \n",
    "                           print_plot=True, plot_conf_matrix=False, print_cr=True, pos_label=\"bad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos, também, mudar o estimador basal. Por exemplo, uma regressão logística fortemente regularizada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T23:40:10.705272Z",
     "start_time": "2022-06-07T23:40:10.490399Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T23:42:20.196764Z",
     "start_time": "2022-06-07T23:42:19.380896Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Métricas de avaliação de treino - com cutoff = 0.50\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bad       0.61      0.13      0.21       240\n",
      "        good       0.72      0.96      0.83       560\n",
      "\n",
      "    accuracy                           0.71       800\n",
      "   macro avg       0.66      0.55      0.52       800\n",
      "weighted avg       0.69      0.71      0.64       800\n",
      "\n",
      "\n",
      "################################################################################\n",
      "\n",
      "Métricas de avaliação de teste - com cutoff = 0.50\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bad       0.42      0.08      0.14        60\n",
      "        good       0.71      0.95      0.81       140\n",
      "\n",
      "    accuracy                           0.69       200\n",
      "   macro avg       0.56      0.52      0.47       200\n",
      "weighted avg       0.62      0.69      0.61       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "basal = LogisticRegression(C=5000, random_state=42)\n",
    "\n",
    "ab = AdaBoostClassifier(base_estimator=basal,\n",
    "                        random_state=42).fit(X_train, y_train)\n",
    "\n",
    "_ = clf_metrics_train_test(ab, X_train, y_train, X_test, y_test, cutoff=0.5, \n",
    "                           print_plot=True, plot_conf_matrix=False, print_cr=True, pos_label=\"bad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Não ficou muito legal. Por isso que, apesar de ser possível usar outros estimadores basais, é comum usarmos stumps mesmo (árvores com uma única quebra)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos agora fazer o gridsearch para mostrar algo bem legal: é possível tunarmos os hiperparâmetros do estimador basal!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T00:04:36.335111Z",
     "start_time": "2022-06-08T00:04:04.975717Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=42, shuffle=True),\n",
       "             estimator=Pipeline(steps=[('ab',\n",
       "                                        AdaBoostClassifier(base_estimator=DecisionTreeClassifier(random_state=42),\n",
       "                                                           random_state=42))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'ab__base_estimator__max_depth': [1, 2],\n",
       "                         'ab__base_estimator__min_samples_split': [5, 10, 15],\n",
       "                         'ab__n_estimators': [25, 50, 100, 125]},\n",
       "             return_train_score=True, scoring='f1_weighted', verbose=10)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basal = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "pipe = Pipeline([(\"ab\", AdaBoostClassifier(base_estimator=basal, random_state=42))])\n",
    "\n",
    "param_grid_ab = {\"ab__n_estimators\" : [25, 50, 100, 125],\n",
    "                 \"ab__base_estimator__max_depth\" : [1, 2],\n",
    "                 \"ab__base_estimator__min_samples_split\" : [5, 10, 15]}\n",
    "\n",
    "splitter = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "grid_ab = GridSearchCV(pipe,\n",
    "                       param_grid_ab,\n",
    "                       cv=splitter,\n",
    "                       scoring=\"f1_weighted\",\n",
    "                       verbose=10,\n",
    "                       n_jobs=-1,\n",
    "                       return_train_score=True)\n",
    "\n",
    "grid_ab.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T00:04:57.826597Z",
     "start_time": "2022-06-08T00:04:57.539767Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ab__base_estimator__max_depth': 2,\n",
       " 'ab__base_estimator__min_samples_split': 15,\n",
       " 'ab__n_estimators': 25}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_ab.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T00:05:59.003043Z",
     "start_time": "2022-06-08T00:05:58.845133Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6594673023864148"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_ab.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T00:05:36.081752Z",
     "start_time": "2022-06-08T00:05:35.825271Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Métricas de avaliação de treino - com cutoff = 0.50\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bad       0.78      0.46      0.58       240\n",
      "        good       0.80      0.94      0.87       560\n",
      "\n",
      "    accuracy                           0.80       800\n",
      "   macro avg       0.79      0.70      0.72       800\n",
      "weighted avg       0.80      0.80      0.78       800\n",
      "\n",
      "\n",
      "################################################################################\n",
      "\n",
      "Métricas de avaliação de teste - com cutoff = 0.50\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bad       0.37      0.23      0.29        60\n",
      "        good       0.72      0.83      0.77       140\n",
      "\n",
      "    accuracy                           0.65       200\n",
      "   macro avg       0.54      0.53      0.53       200\n",
      "weighted avg       0.61      0.65      0.62       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_ = clf_metrics_train_test(grid_ab, X_train, y_train, X_test, y_test, cutoff=0.5, \n",
    "                           print_plot=True, plot_conf_matrix=False, print_cr=True, pos_label=\"bad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-02T19:09:02.324019Z",
     "start_time": "2022-06-02T19:09:02.310026Z"
    }
   },
   "source": [
    "Usando nossa função:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T00:06:44.743716Z",
     "start_time": "2022-06-08T00:06:44.186519Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>delta</th>\n",
       "      <th>mean_train_score_norm</th>\n",
       "      <th>mean_test_score_norm</th>\n",
       "      <th>delta_norm</th>\n",
       "      <th>metrica_criterio_final</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.697986</td>\n",
       "      <td>0.656520</td>\n",
       "      <td>0.041466</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.829934</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.864967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.697986</td>\n",
       "      <td>0.656520</td>\n",
       "      <td>0.041466</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.829934</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.864967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.697986</td>\n",
       "      <td>0.656520</td>\n",
       "      <td>0.041466</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.829934</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.864967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.712629</td>\n",
       "      <td>0.651661</td>\n",
       "      <td>0.060968</td>\n",
       "      <td>0.141137</td>\n",
       "      <td>0.714404</td>\n",
       "      <td>0.149509</td>\n",
       "      <td>0.782448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.712629</td>\n",
       "      <td>0.651661</td>\n",
       "      <td>0.060968</td>\n",
       "      <td>0.141137</td>\n",
       "      <td>0.714404</td>\n",
       "      <td>0.149509</td>\n",
       "      <td>0.782448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.712629</td>\n",
       "      <td>0.651661</td>\n",
       "      <td>0.060968</td>\n",
       "      <td>0.141137</td>\n",
       "      <td>0.714404</td>\n",
       "      <td>0.149509</td>\n",
       "      <td>0.782448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.740284</td>\n",
       "      <td>0.653865</td>\n",
       "      <td>0.086419</td>\n",
       "      <td>0.218829</td>\n",
       "      <td>0.766791</td>\n",
       "      <td>0.214121</td>\n",
       "      <td>0.776335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.740284</td>\n",
       "      <td>0.653865</td>\n",
       "      <td>0.086419</td>\n",
       "      <td>0.218829</td>\n",
       "      <td>0.766791</td>\n",
       "      <td>0.214121</td>\n",
       "      <td>0.776335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.740284</td>\n",
       "      <td>0.653865</td>\n",
       "      <td>0.086419</td>\n",
       "      <td>0.218829</td>\n",
       "      <td>0.766791</td>\n",
       "      <td>0.214121</td>\n",
       "      <td>0.776335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.747468</td>\n",
       "      <td>0.654503</td>\n",
       "      <td>0.092965</td>\n",
       "      <td>0.239014</td>\n",
       "      <td>0.781979</td>\n",
       "      <td>0.230740</td>\n",
       "      <td>0.775620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.747468</td>\n",
       "      <td>0.654503</td>\n",
       "      <td>0.092965</td>\n",
       "      <td>0.239014</td>\n",
       "      <td>0.781979</td>\n",
       "      <td>0.230740</td>\n",
       "      <td>0.775620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.747468</td>\n",
       "      <td>0.654503</td>\n",
       "      <td>0.092965</td>\n",
       "      <td>0.239014</td>\n",
       "      <td>0.781979</td>\n",
       "      <td>0.230740</td>\n",
       "      <td>0.775620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.802432</td>\n",
       "      <td>0.659467</td>\n",
       "      <td>0.142964</td>\n",
       "      <td>0.393428</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.357672</td>\n",
       "      <td>0.771164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.804781</td>\n",
       "      <td>0.657373</td>\n",
       "      <td>0.147408</td>\n",
       "      <td>0.400027</td>\n",
       "      <td>0.850199</td>\n",
       "      <td>0.368954</td>\n",
       "      <td>0.740623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.804781</td>\n",
       "      <td>0.654250</td>\n",
       "      <td>0.150531</td>\n",
       "      <td>0.400027</td>\n",
       "      <td>0.775953</td>\n",
       "      <td>0.376882</td>\n",
       "      <td>0.699536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.868343</td>\n",
       "      <td>0.656662</td>\n",
       "      <td>0.211681</td>\n",
       "      <td>0.578597</td>\n",
       "      <td>0.833309</td>\n",
       "      <td>0.532120</td>\n",
       "      <td>0.650594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.867199</td>\n",
       "      <td>0.650711</td>\n",
       "      <td>0.216487</td>\n",
       "      <td>0.575382</td>\n",
       "      <td>0.691817</td>\n",
       "      <td>0.544323</td>\n",
       "      <td>0.573747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.953215</td>\n",
       "      <td>0.656524</td>\n",
       "      <td>0.296691</td>\n",
       "      <td>0.817035</td>\n",
       "      <td>0.830015</td>\n",
       "      <td>0.747935</td>\n",
       "      <td>0.541040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.867306</td>\n",
       "      <td>0.643524</td>\n",
       "      <td>0.223782</td>\n",
       "      <td>0.575684</td>\n",
       "      <td>0.520946</td>\n",
       "      <td>0.562841</td>\n",
       "      <td>0.479053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.982746</td>\n",
       "      <td>0.648955</td>\n",
       "      <td>0.333791</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.650075</td>\n",
       "      <td>0.842119</td>\n",
       "      <td>0.403978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.949662</td>\n",
       "      <td>0.638320</td>\n",
       "      <td>0.311342</td>\n",
       "      <td>0.807052</td>\n",
       "      <td>0.397209</td>\n",
       "      <td>0.785127</td>\n",
       "      <td>0.306041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.981147</td>\n",
       "      <td>0.633045</td>\n",
       "      <td>0.348102</td>\n",
       "      <td>0.895506</td>\n",
       "      <td>0.271803</td>\n",
       "      <td>0.878449</td>\n",
       "      <td>0.196677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.951976</td>\n",
       "      <td>0.626243</td>\n",
       "      <td>0.325733</td>\n",
       "      <td>0.813554</td>\n",
       "      <td>0.110077</td>\n",
       "      <td>0.821661</td>\n",
       "      <td>0.144208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.982410</td>\n",
       "      <td>0.625819</td>\n",
       "      <td>0.356591</td>\n",
       "      <td>0.899055</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_train_score  mean_test_score     delta  mean_train_score_norm  \\\n",
       "0           0.697986         0.656520  0.041466               0.100000   \n",
       "4           0.697986         0.656520  0.041466               0.100000   \n",
       "8           0.697986         0.656520  0.041466               0.100000   \n",
       "5           0.712629         0.651661  0.060968               0.141137   \n",
       "9           0.712629         0.651661  0.060968               0.141137   \n",
       "1           0.712629         0.651661  0.060968               0.141137   \n",
       "2           0.740284         0.653865  0.086419               0.218829   \n",
       "6           0.740284         0.653865  0.086419               0.218829   \n",
       "10          0.740284         0.653865  0.086419               0.218829   \n",
       "3           0.747468         0.654503  0.092965               0.239014   \n",
       "7           0.747468         0.654503  0.092965               0.239014   \n",
       "11          0.747468         0.654503  0.092965               0.239014   \n",
       "20          0.802432         0.659467  0.142964               0.393428   \n",
       "12          0.804781         0.657373  0.147408               0.400027   \n",
       "16          0.804781         0.654250  0.150531               0.400027   \n",
       "21          0.868343         0.656662  0.211681               0.578597   \n",
       "13          0.867199         0.650711  0.216487               0.575382   \n",
       "18          0.953215         0.656524  0.296691               0.817035   \n",
       "17          0.867306         0.643524  0.223782               0.575684   \n",
       "19          0.982746         0.648955  0.333791               0.900000   \n",
       "22          0.949662         0.638320  0.311342               0.807052   \n",
       "15          0.981147         0.633045  0.348102               0.895506   \n",
       "14          0.951976         0.626243  0.325733               0.813554   \n",
       "23          0.982410         0.625819  0.356591               0.899055   \n",
       "\n",
       "    mean_test_score_norm  delta_norm  metrica_criterio_final  \n",
       "0               0.829934    0.100000                0.864967  \n",
       "4               0.829934    0.100000                0.864967  \n",
       "8               0.829934    0.100000                0.864967  \n",
       "5               0.714404    0.149509                0.782448  \n",
       "9               0.714404    0.149509                0.782448  \n",
       "1               0.714404    0.149509                0.782448  \n",
       "2               0.766791    0.214121                0.776335  \n",
       "6               0.766791    0.214121                0.776335  \n",
       "10              0.766791    0.214121                0.776335  \n",
       "3               0.781979    0.230740                0.775620  \n",
       "7               0.781979    0.230740                0.775620  \n",
       "11              0.781979    0.230740                0.775620  \n",
       "20              0.900000    0.357672                0.771164  \n",
       "12              0.850199    0.368954                0.740623  \n",
       "16              0.775953    0.376882                0.699536  \n",
       "21              0.833309    0.532120                0.650594  \n",
       "13              0.691817    0.544323                0.573747  \n",
       "18              0.830015    0.747935                0.541040  \n",
       "17              0.520946    0.562841                0.479053  \n",
       "19              0.650075    0.842119                0.403978  \n",
       "22              0.397209    0.785127                0.306041  \n",
       "15              0.271803    0.878449                0.196677  \n",
       "14              0.110077    0.821661                0.144208  \n",
       "23              0.100000    0.900000                0.100000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_params_delta = calc_best_params_delta(grid_ab, peso_delta=0.5, print_deltas=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T00:07:16.451878Z",
     "start_time": "2022-06-08T00:07:16.229931Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ab__base_estimator__max_depth': 1,\n",
       " 'ab__base_estimator__min_samples_split': 5,\n",
       " 'ab__n_estimators': 25}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T00:08:10.476020Z",
     "start_time": "2022-06-08T00:08:10.017190Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Métricas de avaliação de treino - com cutoff = 0.50\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bad       0.61      0.28      0.39       240\n",
      "        good       0.75      0.92      0.83       560\n",
      "\n",
      "    accuracy                           0.73       800\n",
      "   macro avg       0.68      0.60      0.61       800\n",
      "weighted avg       0.71      0.73      0.70       800\n",
      "\n",
      "\n",
      "################################################################################\n",
      "\n",
      "Métricas de avaliação de teste - com cutoff = 0.50\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bad       0.50      0.28      0.36        60\n",
      "        good       0.74      0.88      0.80       140\n",
      "\n",
      "    accuracy                           0.70       200\n",
      "   macro avg       0.62      0.58      0.58       200\n",
      "weighted avg       0.67      0.70      0.67       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "basal = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "pipe_best_delta = Pipeline([(\"ab\", AdaBoostClassifier(base_estimator=basal, random_state=42))]).set_params(**best_params_delta)\n",
    "\n",
    "pipe_best_delta.fit(X_train, y_train)\n",
    "\n",
    "_ = clf_metrics_train_test(pipe_best_delta, X_train, y_train, X_test, y_test, cutoff=0.5, \n",
    "                           print_plot=True, plot_conf_matrix=False, print_cr=True, pos_label=\"bad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-02T19:27:56.003589Z",
     "start_time": "2022-06-02T19:27:55.989750Z"
    }
   },
   "outputs": [],
   "source": [
    "# pra casa: continuar explorando o grid/random search!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______\n",
    "\n",
    "### Bagging vs Boosting\n",
    "\n",
    "Pra lembrar as principais diferenças entre os dois métodos de ensemble que estudamos:\n",
    "\n",
    "<img src=https://pluralsight2.imgix.net/guides/81232a78-2e99-4ccc-ba8e-8cd873625fdf_2.jpg width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________\n",
    "_______\n",
    "_________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Gradient boosting\n",
    "\n",
    "Além dos métodos que estudamos, há, ainda dentro do boosting, outras classes de métodos de ensemble!\n",
    "\n",
    "Em particular, a classe de modelos que se utilizam do procedimento de **gradient boosting**.\n",
    "\n",
    "O gradient boosting também é baseado no princípio de boosting (utilização de weak learners sequencialmente adicionados de modo a **sequencialmente minimizar os erros cometidos**).\n",
    "\n",
    "<img src=https://miro.medium.com/max/788/1*pEu2LNmxf9ttXHIALPcEBw.png width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mas este método implementa o boosting através de um **gradiente** explícito.\n",
    "\n",
    "A ideia é que caminhemos na direção do **erro mínimo** de maneira iterativa **passo a passo**.\n",
    "\n",
    "Este caminho se dá justamente pelo **gradiente** da **função de custo/perda**, que mede justamente os erros cometidos.\n",
    "\n",
    "<img src=https://upload.wikimedia.org/wikipedia/commons/a/a3/Gradient_descent.gif width=400>\n",
    "\n",
    "Este método é conhecido como:\n",
    "\n",
    "### Gradiente descendente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deixei em ênfase porque este será um método de **enorme importância** no estudo de redes neurais (e é, em geral, um método de otimização muito utilizado).\n",
    "\n",
    "O objetivo geral do método é bem simples: determinar quais são os **parâmetros** da hipótese que minimizam a função de custo/perda. Para isso, o método \"percorre\" a função de erro, indo em direção ao seu mínimo (e este \"caminho\" feito na função se dá justamente pela **determinação iterativa dos parâmetros**, isto é, **a cada passo, chegamos mais perto dos parâmetros finais da hipótese**, conforme eles são ajustados aos dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Pequeno interlúdio matemático:** o gradiente descendente implementado pelo gradient boosting é, na verdade, um **gradiente descendente funcional**, isto é, desejamos encontrar não um conjunto de parâmetros que minimiza o erro, mas sim **introduzir sequencialmente weak learners (hipótese simples) que minimizam o erro**. Desta forma, o gradient boosting minimiza a função de custo ao ecolher iterativamente hipóteses simples que apontam na direção do mínimo, neste espaço funcional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apesar do interlúdio acima, não precisamos nos preocupar muito com os detalhes matemáticos: o que importa é entender que no caso do gradient boosting, há alguns pontos importantes:\n",
    "\n",
    "- Uma **função de custo/perda (loss)** é explicitamente minimizada por um procedimento de gradiente;\n",
    "\n",
    "- O gradiente está relacionado com o procedimento de **encadeamento progressivo entre weak learners**, seguindo a ideia do boosting.\n",
    "\n",
    "Pra quem quiser saber um pouco mais de detalhes (e se aventurar na matemática), sugiro [este post](https://www.gormanalysis.com/blog/gradient-boosting-explained/) ou então [este site](https://explained.ai/gradient-boosting/), que contém vários materiais ótimos para entender o método com todos os detalhes matemáticos.\n",
    "\n",
    "Os [vídeos do StatQuest](https://www.youtube.com/playlist?list=PLblh5JKOoLUJjeXUvUE0maghNuY2_5fY6) também são uma boa referência!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As classes do sklearn são:\n",
    "\n",
    "- [GradientBoostingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)\n",
    "\n",
    "- [GradientBoostingRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor)\n",
    "\n",
    "E os principais hiperparâmetros a serem ajustados são:\n",
    "\n",
    "- `n_estimators` : novamente, o número de weak learners encadeados.\n",
    "\n",
    "- `learning_rate` : a constante que multiplica o gradiente no gradiente descendente. Essencialmente, controla o \"tamanho do passo\" a ser dado em direção ao mínimo.\n",
    "\n",
    "Segundo o próprio [User Guide](https://scikit-learn.org/stable/modules/ensemble.html#gradient-boosting): \"*Empirical evidence suggests that small values of `learning_rate` favor better test error. The lireature recommends to set the learning rate to a small constant (e.g. `learning_rate <= 0.1`) and choose `n_estimators` by early stopping.*\"\n",
    "\n",
    "Ainda sobre a learning rate, as ilustrações a seguir ajudam a entender sua importância:\n",
    "\n",
    "<img src=https://www.jeremyjordan.me/content/images/2018/02/Screen-Shot-2018-02-24-at-11.47.09-AM.png width=700>\n",
    "\n",
    "<img src=https://cdn-images-1.medium.com/max/1440/0*A351v9EkS6Ps2zIg.gif width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos treinar nosso classificador baseline de gradient boosting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T00:45:50.162140Z",
     "start_time": "2022-06-08T00:45:49.715834Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T00:44:14.364378Z",
     "start_time": "2022-06-08T00:44:13.684628Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Métricas de avaliação de treino - com cutoff = 0.50\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bad       0.94      0.48      0.64       240\n",
      "        good       0.82      0.99      0.89       560\n",
      "\n",
      "    accuracy                           0.83       800\n",
      "   macro avg       0.88      0.73      0.76       800\n",
      "weighted avg       0.85      0.83      0.82       800\n",
      "\n",
      "\n",
      "################################################################################\n",
      "\n",
      "Métricas de avaliação de teste - com cutoff = 0.50\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bad       0.45      0.25      0.32        60\n",
      "        good       0.73      0.87      0.79       140\n",
      "\n",
      "    accuracy                           0.69       200\n",
      "   macro avg       0.59      0.56      0.56       200\n",
      "weighted avg       0.65      0.69      0.65       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gb = GradientBoostingClassifier(random_state=42).fit(X_train, y_train)\n",
    "\n",
    "_ = clf_metrics_train_test(gb, X_train, y_train, X_test, y_test, cutoff=0.5, \n",
    "                           print_plot=True, plot_conf_matrix=False, print_cr=True, pos_label=\"bad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restringindo a complexidade do weak learner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T00:47:28.155660Z",
     "start_time": "2022-06-08T00:47:27.760886Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Métricas de avaliação de treino - com cutoff = 0.50\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bad       0.67      0.12      0.20       240\n",
      "        good       0.72      0.97      0.83       560\n",
      "\n",
      "    accuracy                           0.72       800\n",
      "   macro avg       0.70      0.55      0.52       800\n",
      "weighted avg       0.71      0.72      0.64       800\n",
      "\n",
      "\n",
      "################################################################################\n",
      "\n",
      "Métricas de avaliação de teste - com cutoff = 0.50\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bad       0.50      0.13      0.21        60\n",
      "        good       0.72      0.94      0.81       140\n",
      "\n",
      "    accuracy                           0.70       200\n",
      "   macro avg       0.61      0.54      0.51       200\n",
      "weighted avg       0.65      0.70      0.63       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gb = GradientBoostingClassifier(max_depth=1,\n",
    "                                random_state=42).fit(X_train, y_train)\n",
    "\n",
    "_ = clf_metrics_train_test(gb, X_train, y_train, X_test, y_test, cutoff=0.5, \n",
    "                           print_plot=True, plot_conf_matrix=False, print_cr=True, pos_label=\"bad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T00:48:24.164063Z",
     "start_time": "2022-06-08T00:48:23.470510Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Métricas de avaliação de treino - com cutoff = 0.50\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bad       0.67      0.19      0.29       240\n",
      "        good       0.73      0.96      0.83       560\n",
      "\n",
      "    accuracy                           0.73       800\n",
      "   macro avg       0.70      0.57      0.56       800\n",
      "weighted avg       0.72      0.73      0.67       800\n",
      "\n",
      "\n",
      "################################################################################\n",
      "\n",
      "Métricas de avaliação de teste - com cutoff = 0.50\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bad       0.43      0.17      0.24        60\n",
      "        good       0.72      0.91      0.80       140\n",
      "\n",
      "    accuracy                           0.69       200\n",
      "   macro avg       0.58      0.54      0.52       200\n",
      "weighted avg       0.63      0.69      0.63       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gb = GradientBoostingClassifier(max_depth=1, n_estimators=500, loss=\"exponential\",\n",
    "                                random_state=42).fit(X_train, y_train)\n",
    "\n",
    "_ = clf_metrics_train_test(gb, X_train, y_train, X_test, y_test, cutoff=0.5, \n",
    "                           print_plot=True, plot_conf_matrix=False, print_cr=True, pos_label=\"bad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pra casa: grid search para otimizar os hiperparâmetros!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T21:27:36.530379Z",
     "start_time": "2022-02-21T21:27:36.525385Z"
    }
   },
   "outputs": [],
   "source": [
    "# grid search (siga a dica do sklearn!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________\n",
    "_______\n",
    "_________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) XGBoost\n",
    "\n",
    "Vamos conhecer agora mais um método de ensemble (e de boosting), o XGBoost (e**X**treme **G**radient **Boost**ing).\n",
    "\n",
    "Este método nada mais é que um gradient boosting, mas com algumas importantes modificações que lhe conferem o título de \"extreme\"! Em particular, duas alterações merecem destaque:\n",
    "\n",
    "- A adição de procedimentos de regularização (L1 e L2!), o que melhora consideravelmente sua capacidade de generalização;\n",
    "\n",
    "- A utilização de derivadas de segunda ordem (Hessiano) para o procedimento de gradiente.\n",
    "\n",
    "Para quem quiser se aventurar mais, sugiro algumas boas leituras:\n",
    "\n",
    "- [Este](https://shirinsplayground.netlify.app/2018/11/ml_basics_gbm/), explica bem as particularidades do XGBoost, além de dar uma boa introdução ao gradient boosting (o código é em R, então pode ignorar essa parte hehe);\n",
    "\n",
    "- [Este](https://medium.com/analytics-vidhya/what-makes-xgboost-so-extreme-e1544a4433bb), introduz bem o método, enquanto enfativa suas particularidades, com alguns detalhes matemáticos;\n",
    "\n",
    "- [Este](https://xgboost.readthedocs.io/en/latest/tutorials/model.html), da própria documentação da biblioteca, traz uma explicação legal, e com alguns detalhes matemáticos;\n",
    "\n",
    "- [Este](https://towardsdatascience.com/https-medium-com-vishalmorde-xgboost-algorithm-long-she-may-rein-edd9f99be63d), com uma discussão mais alto-nível (sem tantos detalhes) sobre o XGBoost e os motivos de seu sucesso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Infelizmente, o sklearn não tem o XGBoost implementado :(\n",
    "\n",
    "Mas, felizmente, existe uma biblioteca que o implementou, de maneira totalmente integrada ao sklearn!!\n",
    "\n",
    "A biblioteca é a [XGBoost](https://xgboost.readthedocs.io/en/latest/).\n",
    "\n",
    "Além das vantagens metodológicas supracitadas, o algoritmo de aprendizagem implementado nesta biblioteca conta com diversas medidas de otimização computacional (e permite [o uso de GPU](https://xgboost.readthedocs.io/en/stable/gpu/index.html), algo que não é nativamente possível com o sklearn!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T01:01:14.831618Z",
     "start_time": "2022-06-08T01:01:10.491795Z"
    }
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T01:03:07.435312Z",
     "start_time": "2022-06-08T01:03:06.364903Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:03:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Métricas de avaliação de treino - com cutoff = 0.50\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bad       1.00      0.96      0.98       240\n",
      "        good       0.98      1.00      0.99       560\n",
      "\n",
      "    accuracy                           0.99       800\n",
      "   macro avg       0.99      0.98      0.99       800\n",
      "weighted avg       0.99      0.99      0.99       800\n",
      "\n",
      "\n",
      "################################################################################\n",
      "\n",
      "Métricas de avaliação de teste - com cutoff = 0.50\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bad       0.39      0.28      0.33        60\n",
      "        good       0.72      0.81      0.76       140\n",
      "\n",
      "    accuracy                           0.65       200\n",
      "   macro avg       0.56      0.55      0.55       200\n",
      "weighted avg       0.62      0.65      0.63       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier(random_state=42).fit(X_train, y_train)\n",
    "\n",
    "_ = clf_metrics_train_test(xgb, X_train, y_train, X_test, y_test, cutoff=0.5, \n",
    "                           print_plot=True, plot_conf_matrix=False, print_cr=True, pos_label=\"bad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T01:03:37.250617Z",
     "start_time": "2022-06-08T01:03:37.031744Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
       "              gamma=0, gpu_id=-1, importance_type=None,\n",
       "              interaction_constraints='', learning_rate=0.300000012,\n",
       "              max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
       "              monotone_constraints='()', n_estimators=100, n_jobs=4,\n",
       "              num_parallel_tree=1, predictor='auto', random_state=42,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aumentando um pouco a regularização:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T01:05:07.546938Z",
     "start_time": "2022-06-08T01:05:07.027996Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:05:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Métricas de avaliação de treino - com cutoff = 0.50\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bad       0.76      0.35      0.48       240\n",
      "        good       0.77      0.95      0.85       560\n",
      "\n",
      "    accuracy                           0.77       800\n",
      "   macro avg       0.77      0.65      0.67       800\n",
      "weighted avg       0.77      0.77      0.74       800\n",
      "\n",
      "\n",
      "################################################################################\n",
      "\n",
      "Métricas de avaliação de teste - com cutoff = 0.50\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bad       0.56      0.25      0.34        60\n",
      "        good       0.74      0.91      0.82       140\n",
      "\n",
      "    accuracy                           0.71       200\n",
      "   macro avg       0.65      0.58      0.58       200\n",
      "weighted avg       0.68      0.71      0.68       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier(random_state=42, reg_alpha=5).fit(X_train, y_train)\n",
    "\n",
    "_ = clf_metrics_train_test(xgb, X_train, y_train, X_test, y_test, cutoff=0.5, \n",
    "                           print_plot=True, plot_conf_matrix=False, print_cr=True, pos_label=\"bad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T01:06:32.092489Z",
     "start_time": "2022-06-08T01:06:31.418829Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:06:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Métricas de avaliação de treino - com cutoff = 0.50\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bad       0.72      0.26      0.39       240\n",
      "        good       0.75      0.96      0.84       560\n",
      "\n",
      "    accuracy                           0.75       800\n",
      "   macro avg       0.74      0.61      0.61       800\n",
      "weighted avg       0.74      0.75      0.71       800\n",
      "\n",
      "\n",
      "################################################################################\n",
      "\n",
      "Métricas de avaliação de teste - com cutoff = 0.50\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bad       0.48      0.20      0.28        60\n",
      "        good       0.73      0.91      0.81       140\n",
      "\n",
      "    accuracy                           0.69       200\n",
      "   macro avg       0.60      0.55      0.54       200\n",
      "weighted avg       0.65      0.69      0.65       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier(random_state=42, reg_alpha=5, reg_lambda=100).fit(X_train, y_train)\n",
    "\n",
    "_ = clf_metrics_train_test(xgb, X_train, y_train, X_test, y_test, cutoff=0.5, \n",
    "                           print_plot=True, plot_conf_matrix=False, print_cr=True, pos_label=\"bad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pra casa: gridsearch completo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gridsearch do xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________\n",
    "_______\n",
    "_________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) LightGBM\n",
    "\n",
    "Por fim, o último método de boosting que vamos conhecer: o **LightGBM** (Light Gradient Boosting).\n",
    "\n",
    "Este método implementa algumas alterações na forma como o boosting é realizado, que faz com que ele seja mais eficiente em alguns aspectos, quando comparado ao XGBoost.\n",
    "\n",
    "<img src=https://i.imgur.com/VBVvOdC.png width=600>\n",
    "\n",
    "Bem como o xgboost, a lgbm disponibiliza também [suporte para GPU](https://lightgbm.readthedocs.io/en/latest/GPU-Tutorial.html). A documentação da biblioteca está [aqui](https://lightgbm.readthedocs.io/en/latest/). Vale a leitura!\n",
    "\n",
    "Para quem quiser se aventurar mais, sugiro algumas boas leituras:\n",
    "\n",
    "- [Este](https://proceedings.neurips.cc/paper/2017/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf) é o artigo original do LightGBM;\n",
    "\n",
    "- [Este artigo](https://neptune.ai/blog/xgboost-vs-lightgbm) compara o lgbm com o xgboost;\n",
    "\n",
    "- [Este post](https://towardsdatascience.com/lightgbm-vs-xgboost-which-algorithm-win-the-race-1ff7dd4917d) estende as comparações, e discute em detalhes os hiperparâmetros do lgbm.\n",
    "\n",
    "Boa notícia: o lightgbm também é [completamente integrado](https://lightgbm.readthedocs.io/en/latest/Python-API.html#scikit-learn-api) ao scikit-learn!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T01:08:23.684442Z",
     "start_time": "2022-06-08T01:08:15.291626Z"
    }
   },
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T01:09:46.501386Z",
     "start_time": "2022-06-08T01:09:46.023654Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Métricas de avaliação de treino - com cutoff = 0.50\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bad       0.79      0.40      0.53       240\n",
      "        good       0.79      0.95      0.86       560\n",
      "\n",
      "    accuracy                           0.79       800\n",
      "   macro avg       0.79      0.67      0.69       800\n",
      "weighted avg       0.79      0.79      0.76       800\n",
      "\n",
      "\n",
      "################################################################################\n",
      "\n",
      "Métricas de avaliação de teste - com cutoff = 0.50\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bad       0.53      0.27      0.36        60\n",
      "        good       0.74      0.90      0.81       140\n",
      "\n",
      "    accuracy                           0.71       200\n",
      "   macro avg       0.64      0.58      0.58       200\n",
      "weighted avg       0.68      0.71      0.68       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lgbm = LGBMClassifier(random_state=42, reg_alpha=3).fit(X_train, y_train)\n",
    "\n",
    "_ = clf_metrics_train_test(lgbm, X_train, y_train, X_test, y_test, cutoff=0.5, \n",
    "                           print_plot=True, plot_conf_matrix=False, print_cr=True, pos_label=\"bad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dentre nossos melhores resultados, e com bem \"pouco esforço\"!\n",
    "\n",
    "Esse é o grande poder dos métodos de boosting!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pra casa: gridsearch completo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gridsearch do xgboost"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
