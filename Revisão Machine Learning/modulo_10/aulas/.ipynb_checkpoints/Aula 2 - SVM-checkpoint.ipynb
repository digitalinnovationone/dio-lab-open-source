{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula 2 - SVM\n",
    "\n",
    "Na aula de hoje, vamos explorar os seguintes tópicos em Python:\n",
    "\n",
    "- 1) Introdução\n",
    "- 2) Classificadores de margem\n",
    "- 3) Support Vector Machines\n",
    "- 4) Funções de kernel\n",
    "- 5) SVM na prática com o sklearn\n",
    "- 6) SVM para regressão "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-09T15:32:22.856869Z",
     "start_time": "2022-06-09T15:32:22.780914Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-09T15:32:26.064020Z",
     "start_time": "2022-06-09T15:32:22.862865Z"
    }
   },
   "outputs": [],
   "source": [
    "from ml_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "____\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Introdução\n",
    "\n",
    "Na aula de hoje, falaremos sobre um dos mais interessantes métodos de aprendizagm supervisionada: **SVM** (**S**upport **V**ector **M**achines).\n",
    "\n",
    "Este método tem uma construção extremamente elegante e robusta, que, apesar de complexa, pode ser entendida em termos geométricos simples. Nesta aula, vamos explorar os principais aspectos desta construção, em uma apresentação mais alto-nível, sem nos preocuparmos demais com os detalhes matemáticos.\n",
    "\n",
    "Um ponto importantíssimo sobre SVMs, que é o que de fato lhes confere poder tão elevado, é sua **capacidade de produzir uma hipótese simples, a partir de um conjunto de hipóteses complexo**, o que tem consequências diretas na **capacidade de generalização** de modelos SVM: eles são capazes de generalizar muito bem, apesar das hipóteses produzidas parecerem altamente complexas, o que é algo formidável!\n",
    "\n",
    "Não entraremos nos detalhes matemáticos que justificam e provam os pontos acima. Mas, para quem tiver interesse, quase todo livro-texto de machine learning aborda estas questões. Recomendo, em particular, [este livro](https://www.google.com.br/books/edition/Learning_with_Kernels/7r34DwAAQBAJ?hl=pt-BR&gbpv=1&dq=learning+with+kernels&printsec=frontcover), que detalha extensivamente SVM e métodos relacionados; ou então [este livro](https://cs.nyu.edu/~mohri/mlbook/), que aborda formalmente a teoria de aprendizagem, eventualmente culminando no enorme sucesso de SVMs. Por fim, aproveito também pra recomendar [este curso](https://work.caltech.edu/index.html) do Caltech, onde os tópicos abordados no livro do Mohri são apresentados de maneira clara, direta, e muito ilustrativa. Esta é minha maior recomendação para quem tem interesse em iniciar os estudos em teoria de aprendizagem estatística, e então ter todo o fundamento rigoroso pra entender porque SVMs são tão interessantes!\n",
    "\n",
    "Agora, vamos iniciar nossa exposição, introduzindo um conceito fundamental para SVMs: a **margem**, e os chamados **classificadores de margem**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "____\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Classificadores de margem\n",
    "\n",
    "Considere o seguinte dataset supervisionado de um problema de classificação binário:\n",
    "\n",
    "<img src=https://s3-sa-east-1.amazonaws.com/lcpi/d8d83e07-66d8-47c6-a5e4-4a3e232481e2.PNG width=400>\n",
    "\n",
    "É visível que os dados são linearmente separáveis. De fato, existem infinitas retas possíveis que separam perfeitamente as duas classes. Alguns exemplos:\n",
    "\n",
    "<img src=https://s3-sa-east-1.amazonaws.com/lcpi/bbf3d0a6-4707-471b-9f19-7290c13e4f23.PNG width=400>\n",
    "\n",
    "Vamos olhar separadamente para cada um destes três modelos:\n",
    "\n",
    "<img src=https://s3-sa-east-1.amazonaws.com/lcpi/6c0bea51-dda9-400c-bb68-d7396e4fa606.PNG width=900>\n",
    "\n",
    "Apesar de ambos separarem perfeitamente os dados de treino (erro de treino é nulo!), podemos nos perguntar: qual deles tem potencial de apresentar **melhor generalização?**\n",
    "\n",
    "Para refletirmos sobre isso, considere que queremos classificar o ponto de teste indicado em preto:\n",
    "\n",
    "<img src=https://s3-sa-east-1.amazonaws.com/lcpi/fbed2deb-bb38-4ef5-8558-99b7aff5c7b7.PNG width=900>\n",
    "\n",
    "É super razoável que este ponto seja classificado como sendo da classe vermelha, não é mesmo? No entanto, o primeiro classificador irá classificá-lo como pertencente à classe azul."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos nos perguntar: por que isso acontece?\n",
    "\n",
    "Intuitivamente, é possível dizer que a fronteira de decisão do primeiro modelo está \"**muito próxima**\" dos pontos da classe vermelha, não é? \n",
    "\n",
    "Mas, se estamos avaliando a **fronteira de decisão**, é razoável que nos importemos, na realidade, com a proximidade entre ela **e os \"pontos mais externos\"** da respectiva classe, não é mesmo? Afinal, intuitivamente, estes são os pontos que exercem maior influência sobre a fornteira de decisão, justo?\n",
    "\n",
    "É aqui que entra o conceito de **margem**:\n",
    "\n",
    "> Chamamos de **margem** a **menor distância** entre os pontos de treino e a fronteira de decisão\n",
    "\n",
    "A seguir, visualizamos as margens associadas a cada um dos três modelos:\n",
    "\n",
    "<img src=https://s3-sa-east-1.amazonaws.com/lcpi/72f191fc-7bc0-4451-923a-ecb939e88111.PNG width=900>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dada a definição de margem, fica claro que o terceiro modelo seria nossa melhor opção, pois ele **apresenta maior margem**. De fato, intuitivamente, um classificador que apresenta maior margem, terá melhores chances de generalização!\n",
    "\n",
    "> Chamamos de **classificador de margem máxima** um classificador **linear** que é construído de modo que a margem seja maximizada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"warning\" style='padding:0.1em; background-color:#E9D8FD; color:#69337A'>\n",
    "<span>\n",
    "<h2>Conclusões</h2>\n",
    "<ul>\n",
    "<li>Um classificador de margem máxima tem alto potencial de generalização, para dados linearmente separáveis;\n",
    "</ul>\n",
    "</span>\n",
    "<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classificador de margem suave\n",
    "\n",
    "Considere o dataset a seguir:\n",
    "\n",
    "<img src=https://s3-sa-east-1.amazonaws.com/lcpi/7be60cd3-e7f8-46bd-85bc-73ec1d4b0bb5.PNG width=400>\n",
    "\n",
    "Neste caso, nós não temos mais separabilidade linear, de modo que a construção de um classificador de margem máxima não é mais possível.\n",
    "\n",
    "Como resolver este problema?\n",
    "\n",
    "Com a introdução do **classificador de margem suave (*soft margin classifier*)**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um classificador de margem suave é obtido com uma modificação extremamente simples com relação ao classificador de margem máxima: a permissão de que **erros de classificação** sejam cometidos na base de treino! Algo assim:\n",
    "\n",
    "<img src=https://s3-sa-east-1.amazonaws.com/lcpi/cfacd48d-9713-4bec-9fd7-ea1b4ae380f5.PNG width=400>\n",
    "\n",
    "\n",
    "Neste caso, temos duas observações incorretas dentro da margem - e está tudo bem, pois dados que estas observações são outliers, não precisamos mos preocupar em ajustar a margem a elas!\n",
    "\n",
    "Com isso, temos um modelo com um viés um pouco maior, mas com variância bem menor - caminhamos na direção do **sweet spot** de generalização!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, portanto, que a introdução de uma margem suave pode ser visto como **um procedimento de regularização** aplicado ao classificador de margem! E é exatamente assim que veremos este procedimento quando chegarmos em SVM - um procedimento de regularização!\n",
    "\n",
    "Um classificador de margem suave é também chamado de **classificador de vetores de suporte (*support vector classifier*)**, sendo que os pontos que \"apoiam\" a margem são justamente os chamados **vetores de suporte (*support vectors*)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"warning\" style='padding:0.1em; background-color:#E9D8FD; color:#69337A'>\n",
    "<span>\n",
    "<h2>Conclusões</h2>\n",
    "<ul>\n",
    "<li>Um classificador de margem máxima tem alto potencial de generalização, para dados linearmente separáveis;<br><br>\n",
    "<li>Um classificador de margem suave (vetores de suporte) introduz um pouco mais de viés ao permitir erros de treino, mas, com isso, aumenta o potencial de generalização, e pode ser aplicável a dados que não são exatamente linearmente separáveis;\n",
    "</ul>\n",
    "</span>\n",
    "<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estamos chegando perto das SVMs!\n",
    "\n",
    "Mas, antes de chegarmos lá, é importante frisarmos um ponto fundamental:\n",
    "\n",
    "> A **fronteira de decisão** de um classificador de margem suave (de vetores de suporte) é **linear no espaço de features** em que o classificador é treinado, ou seja, a hipótese treinada (isto é, a superfície de decisão) será **um hiperplano** de dimensão $D-1$, onde $D$ é a dimensão do espaço de features\n",
    "\n",
    "Vamos entender melhor o comentário acima com alguns exemplos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $D=2$\n",
    "\n",
    "Se temos uma duas features$(X_1, X_2)$, teremos um **hiperplano $1-$dimensional**, que nada mais é que **uma reta** como fronteira de decisão:\n",
    "\n",
    "<img src=https://s3-sa-east-1.amazonaws.com/lcpi/6ca9b6d4-90fb-4d92-8a73-42ecec09d562.PNG width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $D=3$\n",
    "\n",
    "Se temos uma três features$(X_1, X_2, X_3)$, teremos um **hiperplano $2-$dimensional**, que nada mais é que **um plano** como fronteira de decisão:\n",
    "\n",
    "<img src=https://s3-sa-east-1.amazonaws.com/lcpi/b91951b3-9256-4a38-abd6-287811332db8.PNG width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $D=1$\n",
    "\n",
    "Se temos uma única feature $(X)$, teremos um **hiperplano $0-$dimensional**, que nada mais é que **um ponto** como fronteira de decisão:\n",
    "\n",
    "<img src=https://s3-sa-east-1.amazonaws.com/lcpi/000d5eb5-25e7-485b-bf36-6ad944970e27.PNG width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para dimensões maiores, $D > 3$, não conseguimos mais visualizar a fronteira de decisão, mas, se qualquer forma, ela será linear (por isso, um hiper**plano**!).\n",
    "\n",
    "Agora estamos prontos para introduzir as tão aguardadas **Support Vector Machines!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"warning\" style='padding:0.1em; background-color:#E9D8FD; color:#69337A'>\n",
    "<span>\n",
    "<h2>Conclusões</h2>\n",
    "<ul>\n",
    "<li>Um classificador de margem máxima tem alto potencial de generalização, para dados linearmente separáveis;<br><br>\n",
    "<li>Um classificador de margem suave (vetores de suporte) introduz um pouco mais de viés ao permitir erros de treino, mas, com isso, aumenta o potencial de generalização, e pode ser aplicável a dados que não são exatamente linearmente separáveis;<br><br>\n",
    "<li>A fronteira de decisão de um classificador de margem sempre será linear no espaço de features;\n",
    "</ul>\n",
    "</span>\n",
    "<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "____\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Support Vector Machines\n",
    "\n",
    "Imagine que temos o seguinte dataset (com uma única dimensão):\n",
    "\n",
    "<img src=https://s3-sa-east-1.amazonaws.com/lcpi/e00c77e0-14fb-472f-b9ae-f6af7b229ad5.PNG width=400>\n",
    "\n",
    "É evidente que este dataset **não é linearmente separável**! Portanto, não conseguimos produzir um classificador de vetores de suporte para este dataset **no espaço de features original**.\n",
    "\n",
    "Mas, aí entra uma ideia muito interessante: e se nós **levarmos os dados para um ou outro espaço?**\n",
    "\n",
    "Seria possível que no espaço original os dados não sejam linearmente separáveis, mas **o sejam** em algum outro espaço?\n",
    "\n",
    "Bom, a priori, vamos tentar algo bem simples... Que tal introduzirmos **uma nova feature** $X_2 = X_1^2$? O que aconteceria neste caso?\n",
    "\n",
    "De fato, ao **introduzirmos** uma nova feature, estamos fazendo com que **cada observação passe a ser caracterizada por duas features ao invés de uma única**!\n",
    "\n",
    "Ou seja, nosso espaço de features efetivamente muda de $\\mathbb{R}^1$ para $\\mathbb{R}^2$! Veja:\n",
    "\n",
    "<img src=https://s3-sa-east-1.amazonaws.com/lcpi/8d5e6199-8a33-45cf-9e4f-c6507024fb36.PNG width=800>\n",
    "\n",
    "O procedimento que fizemos é chamado de **feature map**, e ele é matematicamente representado pelo mapa (função) $\\Phi$. \n",
    "\n",
    "> Como $\\Phi$ leva observações do espaço original ($1$D, uma única feature $X_1$, para vetores do novo espaço ($2$D, duas features, $X_1$ e $X_2$), o denotamos como: \n",
    "$$\\begin{align*}\n",
    "\\Phi \\ \\colon \\ & \\mathbb{R}^1 \\longrightarrow \\mathbb{R}^2 \\\\\n",
    "& \\vec{x} = X_1 \\longmapsto \\Phi(\\vec{x}) = (X_1, X_2)\n",
    "\\end{align*}$$\n",
    "\n",
    "Note que no caso ilustrado acima, temos $X_2 = X_1^2$, isto é, $\\Phi(\\vec{x}) = (X_1, X_1^2)$.\n",
    "\n",
    "Então, apesar do nome assustador, o feature map é algo que já estávamos acostumados a fazer, quando usamos o [PolynomialFeatures](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html), lembra? :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um pouco mais de terminologia:\n",
    "\n",
    "> O \"espaço original\" é comumente chamado de **espaço de input** (representaremos por $\\mathcal{X}$); enquanto o espaço após a aplicação do feature map é chamado de **espaço de features (representaremos por $\\mathcal{Z}$)**\n",
    "\n",
    "<img src=https://miro.medium.com/max/872/1*zWzeMGyCc7KvGD9X8lwlnQ.png width=400>\n",
    "\n",
    "Também é comum se referir ao espaço de features como **espaço z**, devido à comum notação $\\Phi(\\vec{x}) \\equiv \\vec{z}$. Neste caso, teríamos:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\Phi \\ \\colon \\ & \\mathbb{R}^1 \\longrightarrow \\mathbb{R}^2 \\\\\n",
    "& \\vec{x} = X_1 \\longmapsto \\Phi(\\vec{x}) = \\vec{z} = (Z_1, Z_2)\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma vez que temos estas definições, podemos perceber a real utilidade do feature map: **os dados não eram linearmente separáveis no espaço de input, mas passaram a ser no espaço de features!**\n",
    "\n",
    "Isso é realmente formidável, pois, se temos dados linearmente separáveis, podemos **treinar um classificador de margem suave** no espaço de features!\n",
    "\n",
    "Isso pode parecer estranho, pois, afinal, gostaríamos de separar os dados no espaço original, não é mesmo?\n",
    "\n",
    "Na verdade, nosso objetivo é que os dados sejam separados, **não importa em que espaço**! Se conseguirmos encontrar um espaço onde há separabilidade através da aplicação de um feature map, bastaria **aplicar o mesmo feature map** aos dados de treino e de teste, e a separabilidade sempre estará garantida!\n",
    "\n",
    "Muito legal, não é mesmo?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos gerar alguns dados pra ver isto funcionando na prática:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E claro, embora tenhamos visto um feature map do tipo $\\Phi : \\mathbb{R}^1 \\rightarrow \\mathbb{R}^2$, eles podem assumir as mais diferentes formas! Em geral, podemos definir um feature map genérico como $\\Phi : \\mathcal{X} \\rightarrow \\mathcal{Z}$ (de forma concreta, $\\Phi : \\mathbb{R}^m \\rightarrow \\mathbb{R}^n$, para $m, n$ dimensões arbitrárias!)\n",
    "\n",
    "Mais um exemplo, $\\Phi : \\mathbb{R}^2 \\rightarrow \\mathbb{R}^3$:\n",
    "\n",
    "<img src=https://s3-sa-east-1.amazonaws.com/lcpi/105e0718-f938-4d09-919a-b37f79b410f7.PNG width=400>\n",
    "\n",
    "<img src=https://s3-sa-east-1.amazonaws.com/lcpi/526d476e-614b-4acf-b6c5-41cad5567b98.gif width=400>\n",
    "\n",
    "Finalmente, agora temos todos os elementos necessários para entender o que são as SVMs:\n",
    "\n",
    "> Uma **Support Vector Machine** nada mais é que **um classificador de margem suave** treinado **no espaço de features**. Portanto, este classificador pressupõe a aplicação prévia de um **feature map** aos dados no espaço de input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"warning\" style='padding:0.1em; background-color:#E9D8FD; color:#69337A'>\n",
    "<span>\n",
    "<h2>Conclusões</h2>\n",
    "<ul>\n",
    "<li>Um classificador de margem máxima tem alto potencial de generalização, para dados linearmente separáveis;<br><br>\n",
    "<li>Um classificador de margem suave (vetores de suporte) introduz um pouco mais de viés ao permitir erros de treino, mas, com isso, aumenta o potencial de generalização, e pode ser aplicável a dados que não são exatamente linearmente separáveis;<br><br>\n",
    "<li>A fronteira de decisão de um classificador de margem sempre será linear no espaço de features;<br><br>\n",
    "<li>Se os dados não forem linearmente separáveis no espaço original (de input), podemos fazer um feature map para levá-los para um espaço de features onde haja separabilidade linear;\n",
    "</ul>\n",
    "</span>\n",
    "<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da discussão acima, ficou claro que é justamente o feature map que dá grande poder às SVMs. De fato, a possibilidade de conseguirmos separabilidade linear é algo formidável!\n",
    "\n",
    "Neste contexto, uma pergunta natural é: como escolher um bom feature map? Formalmente, existem infinitos feature maps possíveis! Como escolher, dentre infinitas opções, exatamente o mapa exato que nos garante separabilidade linear no espaço de features? Embora esta pergunta não seja fácil de responder, existem algumas técnicas para nos ajudar a escolher bons feature maps (vamos discutir sobre isso mais a diante)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Além disso, existe um segundo problema, ainda maior: suponha que queiramos introduzir um kernel que leva os pontos para um espaço de features de altíssima dimensionalidade (algo como $\\Phi : \\mathbb{R}^2 \\mapsto \\mathbb{R}^{10000}$).\n",
    "\n",
    "É de se esperar que este seja um feature map **operacionalmente custoso** de ser calculado, não é mesmo? Imagina, ter que aplicar esta transformação para todos os pontos de treino, e depois de teste! De fato, isso rapidamente se torna computacionalmente impraticável...\n",
    "\n",
    "Pra solucionar este problema, foi introduzido o uso de **funções de kernel** para capturar um aspecto importante dos feature maps! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mas, pra entendermos a importância das funções de kernel, é necessário entendermos uma coisa:\n",
    "\n",
    "> A hipótese do SVM depende apenas do **produto interno** entre as observações no espaço de features!!\n",
    "\n",
    "A hipótese é a seguinte:\n",
    "\n",
    "$$f_{H, \\alpha}(\\vec{x}) = \\text{sign} \\left ( \\sum_{i=1}^N y_i \\alpha_i \\langle \\Phi(\\vec{x}) , \\Phi(\\vec{x}_i) \\rangle + b\\right ) $$\n",
    "\n",
    "Entendo os termos:\n",
    "\n",
    "- Cada $\\vec{x}_i$, $i = 1, 2, \\cdots, N$ é uma das $N$ **observações de treino**; e cada $y_i$ é o respectivo target;\n",
    "- Cada $\\alpha_i$ é um [multiplicador de lagrange](https://en.wikipedia.org/wiki/Lagrange_multiplier). Podemos entendê-los simplesmente como **coeficientes numéricos associados a cada observação de treino**, sendo que $\\alpha_i > 0$. Podemos entender estes coeficientes como sendo substitutos ao $\\vec{w}$ (uma representação [dual](https://en.wikipedia.org/wiki/Duality_(optimization)) do hiperplano);\n",
    "- $\\langle \\Phi(\\vec{x}) , \\Phi(\\vec{x}_i) \\rangle$ é o produto interno entre a observação de teste $\\vec{x}$ e cada observação de treino $\\vec{x}_i$, **no espaço de features**, ou seja, após a aplicação do feature map!\n",
    "\n",
    "> **Obs.:** O produto interno (na forma de produto escalar) é dado por: $\\langle \\vec{a} , \\vec{b} \\rangle \\equiv \\vec{a} \\cdot \\vec{b} = a_1 b_1 + a_2 b_2 + \\cdots + a_d b_d = \\sum_{i=1}^d a_i b_i$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que, de fato, apenas o produto interno aparece! E é isso que permite o uso das funções de kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"warning\" style='padding:0.1em; background-color:#E9D8FD; color:#69337A'>\n",
    "<span>\n",
    "<h2>Conclusões</h2>\n",
    "<ul>\n",
    "<li>Um classificador de margem máxima tem alto potencial de generalização, para dados linearmente separáveis;<br><br>\n",
    "<li>Um classificador de margem suave (vetores de suporte) introduz um pouco mais de viés ao permitir erros de treino, mas, com isso, aumenta o potencial de generalização, e pode ser aplicável a dados que não são exatamente linearmente separáveis;<br><br>\n",
    "<li>A fronteira de decisão de um classificador de margem sempre será linear no espaço de features;<br><br>\n",
    "<li>Se os dados não forem linearmente separáveis no espaço original (de input), podemos fazer um feature map para levá-los para um espaço de features onde haja separabilidade linear;<br><br>\n",
    "<li>A única coisa que importa pro SVM é o produto interno entre as observações no espaço de features;\n",
    "</ul>\n",
    "</span>\n",
    "<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "____\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Funções de kernel\n",
    "\n",
    "Uma função de kernel $\\kappa$ nada mais é que uma **medida de similaridade** entre dois vetores $\\vec{x}$ e $\\vec{x}'$ (que no nosso caso, são observações). Definimos como:\n",
    "\n",
    "$$\\boxed{\\begin{align*}\n",
    "\\kappa \\ \\colon \\ & \\mathcal{X} \\times \\mathcal{X} \\longrightarrow \\mathbb{R} \\\\\n",
    "& (\\vec{x}, \\vec{x}') \\longmapsto \\kappa(\\vec{x}, \\vec{x}')\n",
    "\\end{align*}}$$\n",
    "\n",
    "Ou seja, um kernel é uma função que, dadas duas observações $\\vec{x}$ e $\\vec{x}'$, retorna um número real que caracteriza o quão similar as duas observações são entre si."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ponto fundamental**: uma função de kernel permite que **o produto escalar** entre duas observações seja calculado **no espaço de features**, sem que precisemos **explicitamente levar as observações pro espaço de features**.\n",
    "\n",
    "Ou seja, conseguimos **evitar** que o feautre map, que costuma ser custoso computacionalmente, seja explicitamente aplicado!\n",
    "\n",
    "E uma vez que pro SVM apenas o produto interno interessa, podemos usar diretamente a função de kernel, que é muito mais computacionalmente simples que o feature map explícito!\n",
    "\n",
    "Esse é o chamado **kernel trick**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importantíssimo:\n",
    "\n",
    "Assim, definimos **o produto interno no espaço de features** como sendo as medidas de similaridade entre os pontos neste espaço:\n",
    "\n",
    "$$ \\kappa_\\Phi(\\vec{x}, \\vec{x}') = \\langle \\Phi(\\vec{x}) , \\Phi(\\vec{x}') \\rangle \\ ,$$\n",
    "\n",
    "onde o subscrito $\\Phi$ deixa explícito que a dependência funcional explícita da função de kernel, depende da escolha do feature map considerado!\n",
    "\n",
    "Agora o ponto muitíssimo importante:\n",
    "\n",
    "> Note que $\\kappa_\\Phi$ nos dá **uma forma de calcular $\\langle \\Phi(\\vec{x}) , \\Phi(\\vec{x}') \\rangle$ diretamente a partir de $\\vec{x}$ e $\\vec{x}'$!**\n",
    "<br><br>\n",
    "Ou seja, apesar de **considerarmos implicitamente o feature map $\\Phi$ na dependêcia funcional de $\\kappa_\\Phi$**, nós não precisamos efetivamente calcular $\\Phi(\\vec{x})$ e $\\Phi(\\vec{x}')$!\n",
    "<br><br>\n",
    "Com isso, reduzimos drasticamente os custos computacionais associados à hipótese!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____________\n",
    "\n",
    "#### Exemplo de aplicação do kernel trick\n",
    "\n",
    "Considere que temos $\\mathcal{X} = \\mathbb{R}^2$, isto é, $\\vec{x} = (X_1, X_2)$, um espaço de input de duas dimensões.\n",
    "\n",
    "Vamos considerar um feature map $\\Phi : \\mathbb{R}^2 \\rightarrow \\mathbb{R}^6$, ou seja, teremos $\\mathcal{Z} = \\mathbb{R}^6$ como espaço de features. Explicitamente, a aplicação do feature map é:\n",
    "\n",
    "$$\\Phi(\\vec{x}) = \\left(1, X_1^2, X_2^2, \\sqrt{2}X_1, \\sqrt{2}X_2, \\sqrt{2}X_1 X_2 \\right )$$\n",
    "\n",
    "Assim, tomando duas observações genéricas $\\vec{x}$ e  $\\vec{x}'$, temos:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\langle \\Phi(\\vec{x}) , \\Phi(\\vec{x}') \\rangle &= \\langle \\left(1, X_1^2, X_2^2, \\sqrt{2}X_1, \\sqrt{2}X_2, \\sqrt{2}X_1 X_2 \\right ), \\left(1, X'^2_1, X'^2_2, \\sqrt{2}X'_1, \\sqrt{2}X'_2, \\sqrt{2}X'_1 X'_2 \\right )\\rangle =\n",
    "\\\\\n",
    "\\\\\n",
    "&= 1 + X_1^2 X'^2_1 + X_2^2 X'^2_2 + \\left ( \\sqrt{2}X_1 \\right) \\left ( \\sqrt{2}X'_1 \\right) + \\left ( \\sqrt{2}X_2 \\right )\\left ( \\sqrt{2}X'_2  \\right )+ \\left (\\sqrt{2}X_1 X_2 \\right )\\left (\\sqrt{2}X'_1 X'_2 \\right )  =\n",
    "\\\\\n",
    "\\\\\n",
    "&= 1 + X_1^2 X'^2_1 + X_2^2 X'^2_2 + 2 \\left ( X_1 X'_1 + X_2 X'_2 + X_1 X'_1 X_2 X'_2 \\right ) \n",
    "\\end{align*}$$\n",
    "\n",
    "Vamos rearranjar os termos acima da seguinte maneira:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\langle \\Phi(\\vec{x}) , \\Phi(\\vec{x}') \\rangle &= 1 + \\left ( X_1^2 X'^2_1 + X_2^2 X'^2_2 + 2  X_1 X'_1 X_2 X'_2  \\right ) + 2 \\left ( X_1 X'_1 + X_2 X'_2\\right ) =\n",
    "\\\\\n",
    "\\\\\n",
    "&= 1 + \\left ( X_1 X'_1 + X_2 X'_2\\right )^2 + 2 \\left ( X_1 X'_1 + X_2 X'_2\\right ) = \n",
    "\\\\ \n",
    "\\\\\n",
    "&= \\Big ( 1 + \\left ( X_1 X'_1 + X_2 X'_2 \\right ) \\Big)^2\n",
    "\\end{align*}$$\n",
    "\n",
    "Agora, note que: $\\langle \\vec{x} , \\vec{x}' \\rangle = \\langle (X_1, X_2), (X'_1, X'_2) \\rangle = X_1 X'_1 + X_2 X'_2$, exatamente como aparece no resultado acima! Sendo assim, temos: \n",
    "\n",
    "$$\\langle \\Phi(\\vec{x}) , \\Phi(\\vec{x}') \\rangle = \\left ( 1 + \\langle \\vec{x} , \\vec{x}' \\rangle \\right)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ou seja, mostramos que é possível escrever o produto interno entre os vetores **no espaço de features** em termos (do produto interno) dos vetores **no espaço de input!**. Oras, isso é justamente o kernel:\n",
    "\n",
    "$$ \\kappa_\\Phi(\\vec{x}, \\vec{x}') = \\left ( 1 + \\langle \\vec{x} , \\vec{x}' \\rangle \\right)^2 = \\langle \\Phi(\\vec{x}) , \\Phi(\\vec{x}') \\rangle$$\n",
    "\n",
    "Este é o ponto! Fizemos o exemplo a seguir para ver de fato como a utilização do kernel é correspondente ao produto interno das observações no espaço de features! Isto é, de fato, $\\kappa_\\Phi(\\vec{x}, \\vec{x}') = \\langle \\Phi(\\vec{x}) , \\Phi(\\vec{x}') \\rangle$!\n",
    "\n",
    "É por isso que dizemos que o kernel nos permite **calcular o produto interno entre dois vetores no espaço de features** sem explicitamente \"**ter que visitar**\" o espaço de features! Este é o ganho de eficiência que os kernels proporcionam!\n",
    "_____________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assim, definimos **o produto interno no espaço de features** como sendo as medidas de similaridade entre os pontos neste espaço:\n",
    "\n",
    "$$ \\kappa_\\Phi(\\vec{x}, \\vec{x}') = \\langle \\Phi(\\vec{x}) , \\Phi(\\vec{x}') \\rangle \\ ,$$\n",
    "\n",
    "E a hipótese é reescrita como:\n",
    "\n",
    "$$\\boxed{f_{H, \\alpha}(\\vec{x}) = \\text{sign} \\left ( \\sum_{i=1}^N y_i \\alpha_i\\kappa_\\Phi(\\vec{x}, \\vec{x}_i) + b\\right )} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"warning\" style='padding:0.1em; background-color:#E9D8FD; color:#69337A'>\n",
    "<span>\n",
    "<h2>Conclusões</h2>\n",
    "<ul>\n",
    "<li>Um classificador de margem máxima tem alto potencial de generalização, para dados linearmente separáveis;<br><br>\n",
    "<li>Um classificador de margem suave (vetores de suporte) introduz um pouco mais de viés ao permitir erros de treino, mas, com isso, aumenta o potencial de generalização, e pode ser aplicável a dados que não são exatamente linearmente separáveis;<br><br>\n",
    "<li>A fronteira de decisão de um classificador de margem sempre será linear no espaço de features;<br><br>\n",
    "<li>Se os dados não forem linearmente separáveis no espaço original (de input), podemos fazer um feature map para levá-los para um espaço de features onde haja separabilidade linear;<br><br>\n",
    "<li>A única coisa que importa pro SVM é o produto interno entre as observações no espaço de features;<br><br>\n",
    "<li>Uma função de kernel permite que calculemos o produto interno no espaço de features sem a necessidade de explicitamente aplicar o feature map.\n",
    "</ul>\n",
    "</span>\n",
    "<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da mesma forma que existem infinitos feature maps possíveis, a variedade de kernels também é imensa! Apesar dos kernels oferecerem uma vantagem operacional absurda com relação à aplicação explícita do feature map, o problema de escolha de um kernel adequado para um determinado problema ainda existe.\n",
    "\n",
    "Na prática, existem algumas formas de propor kernels, mas este não é um tema fácil. Existe todo um conjunto de métodos e técnicas que se utilizam de kerneles para tarefas de aprendizagem, os chamados [métodos de kernel](https://en.wikipedia.org/wiki/Kernel_method).\n",
    "\n",
    "Apesar da enorme liberdade no design de kernels, há algumas classes particulares de kernels que são comumente utilizados:\n",
    "\n",
    "- Linear kernel: $\\kappa_\\Phi(\\vec{x}, \\vec{x}') = \\langle \\vec{x}, \\vec{x}'\\rangle $\n",
    "<br><br>\n",
    "- Polynomial kernel: $\\kappa_\\Phi(\\vec{x}, \\vec{x}') = (\\gamma \\langle \\vec{x}, \\vec{x}'\\rangle + r)^d$\n",
    "<br><br>\n",
    "- Radial Basis Function (RBF) kernel: $\\kappa_\\Phi(\\vec{x}, \\vec{x}') = \\exp(-\\gamma \\|\\vec{x}-\\vec{x}'\\|^2)$\n",
    "<br><br>\n",
    "- Sigmoid kernel: $\\kappa_\\Phi(\\vec{x}, \\vec{x}') = \\tanh(\\gamma \\langle \\vec{x},\\vec{x}'\\rangle + r)$\n",
    "\n",
    "> No exemplo explícito que fizemos acima, usamos justamente um kernel polinomial com $r=\\gamma=1$ e $d=2$!\n",
    "\n",
    "Note que a dependência funcional dos kernels muda, dependendo exatamente do feature map específico que eles representam. No entanto, em todos os casos, as features no espaço de input são utilizadas, o que garante a eficiência!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "\n",
    "Agora que entendemos o SVM, vamos ver sua aplicação com o sklearn a um problema de classificação!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "____\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 5) SVM na prática com o sklearn\n",
    "\n",
    "Para construir um classificador SVM com o sklearn, basta usar a classe [SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html).\n",
    "\n",
    "Vamos utilizar o dataset German Credit Risk!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para a construção de um modelo SVM, é muinto importante que os dados sejam normalizados!\n",
    "\n",
    "O motivo é bem simples: como vimos acima, o SVM é completamente baseado no kernel, que por sua vez é dado por um produto interno. Já o produto interno, é altamente dependente da **escala das features** (lembre-se, $\\langle \\Phi(\\vec{x}) , \\Phi(\\vec{x}') \\rangle = \\left | \\Phi(\\vec{x}) \\right | \\left | \\Phi(\\vec{x}') \\right | \\cos \\left ({\\theta_{\\vec{x}, \\vec{x}'}} \\right )$, isto é, a norma dos vetores influencia o produto interno!).\n",
    "\n",
    "Portanto, para evitar que efeitos de escala influenciem a classificação, a normalização é um passo extremamente importante!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-09T15:32:27.268080Z",
     "start_time": "2022-06-09T15:32:27.114166Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../datasets/german_credit_data.csv\", index_col=0)\n",
    "\n",
    "X = df.select_dtypes(include=np.number)\n",
    "y = df[\"Risk\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, vamos instanciar a classe do [SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)!\n",
    "\n",
    "Perceba que há muitos hiperparâmetros. Vale a pena estudar um pouco mais o fundo o funcionamento de cada um, e sua influência. Para algumas dicas práticas do uso de SVMs com o sklearn, [clique aqui!](https://scikit-learn.org/stable/modules/svm.html#tips-on-practical-use)\n",
    "\n",
    "Na prática, os principais hiperparâmetros serão `C` e o `gamma` (se o kernel escolhido utilizar este parâmetro, como, por exemplo, o kernel rbf: por isso, o hiperparâmetro `kernel` também é importante!):\n",
    "\n",
    "- `C`: é um parâmetro de regularização, relacionado com a \"suavidade\" da margem. Ele controla o tradeoff entre a complexidade da fronteira de decisão, e erros de classificação que são permitidos. Quanto **menor** o C, mais suave será a fronteira de decisão, pois mais erros de classificação são permitidos (isto é, a margem fica **mais larga**); quanto **maior** C, a tolerância a erros de classificação é menor (e a margem fica menos suave, mais complexa);\n",
    "\n",
    "<img src=\"https://learnopencv.com/wp-content/uploads/2018/07/svm-parameter-c-example.png\" width=500>\n",
    "\n",
    "- `gamma`: define a influência que cada ponto tem na fronteira de decisão. É a \"abertura\" do kernel: quanto **maior** o gamma, a influência é de mais curto alcance, e vice-versa;\n",
    "\n",
    "<img src=\"https://sgao323.gitbooks.io/artificial-intelligence-projects/content/assets/svm_gamma.png\" width=400>\n",
    "\n",
    "- `kernel`: as opções disponíveis são `linear`, `poly`, `rbf` e `sigmoid`, que apresentamos acima. Também é possível utilizar um kernel personalizado pré-calculado (neste caso, usamos a opção `precomputed`).\n",
    "\n",
    "\n",
    "No que diz respeito a `C` e `gamma`, é importante que valores adequados sejam encontrados com o uso de Grid/Random search, usando **valores exponencialmente espaçados**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um hiperparâmetro importante, sobretudo quando temos classes desbalanceadas, é o `class_weight=\"balanced\"`.\n",
    "\n",
    "O que isso faz é atribuir um **peso maior na função de custo** a erros cometidos em observações da classe **menos frequente** na base, e vice-versa. \n",
    "\n",
    "Isso corresponde a implicitamente \"focar mais\" nas observações menos frequentes, com o objetivo de compensar o desbalanceamento das classes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, podemos montar um gridsearch para otimizar o modelo:\n",
    "\n",
    "**Obs importante: cuidado com o `max_iter`!** É prudente limitá-lo para evitar que o SVM fique em loop infinito sem convergir!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E esse é o SVM para classificação! :)\n",
    "\n",
    "O SVM també pode ser utilizado para regressão, vamos ver a seguir!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-09T15:32:44.689744Z",
     "start_time": "2022-06-09T15:32:44.535833Z"
    }
   },
   "outputs": [],
   "source": [
    "# lição de casa: continuar o grid search, ver se consegue mais que 0.67 no treino!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "____\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) SVM para regressão\n",
    "\n",
    "Embora fizemos nossa apresentação do SVM como um classificador, também é possível utilizar este método para regressão!\n",
    "\n",
    "Todos os elementos do classificador SVM (margem, kernel, etc.) também são relevantes aqui.\n",
    "\n",
    "A ideia é bem simples: utilização de um kernel para que um modelo de **regressão linear seja treinado no espaço de features**. No espaço de inputs, este modelo é refletido como uma regressão não-linear (da mesma forma que, no caso de classificação, fronteiras de decisão lineares no espaço de features são refletidas como fronteiras não-lineares no espaço de input).\n",
    "\n",
    "A principal diferença é que o conceito de margem também está presente, de modo que **apenas alguns pontos efetivamente vão contriuir para a regressão**. Neste caso, são os pontos **dentro da margem** (região conhecida como $\\epsilon-$tubo) que serão estes vetores de suporte. Ou seja, pontos que estão fora da margem não contribuem para a função de custo.\n",
    "\n",
    "<img src=https://www.saedsayad.com/images/SVR_5.png width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hiperparâmetro `epsilon`**\n",
    "\n",
    "Este hiperparâmetro determina o epsilon-tubo, dentro do qual nenhuma penalidade é aplicada aos pontos.\n",
    "\n",
    "Se recomenda testar valores entre 1e-3 e 1 ([veja aqui](http://adrem.uantwerpen.be/bibrem/pubs/IJCNN2007.pdf) maiores detalhes!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma comparação entre classificadores e regressores SVM:\n",
    "\n",
    "<img src=https://miro.medium.com/max/1100/1*XE9jt0r1yAW8LnliQ3mllQ.png width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A classe de regressores SVM no sklearn é a [SVR](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html). Vamos vê-la em ação!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-09T15:32:44.847653Z",
     "start_time": "2022-06-09T15:32:44.692743Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../datasets/house_prices.csv\")\n",
    "\n",
    "X = df.drop(columns=[\"Id\", \"SalePrice\"])\n",
    "y = df[\"SalePrice\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = X_train.select_dtypes(include=np.number).dropna(axis=\"columns\")\n",
    "X_test = X_test.loc[:, X_train.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O modelo tá super underfitado!\n",
    "\n",
    "Vamos ver se conseguimos melhorar com uma busca aleatória, inicialmente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Já melhorou bastante!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-09T15:33:32.441797Z",
     "start_time": "2022-06-09T15:33:32.321219Z"
    }
   },
   "outputs": [],
   "source": [
    "# pra casa: continue o tuning de hiperparametros!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
