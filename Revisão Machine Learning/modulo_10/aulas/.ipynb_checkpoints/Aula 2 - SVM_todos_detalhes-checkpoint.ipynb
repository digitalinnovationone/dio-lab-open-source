{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula 2 - SVM - detalhes do método\n",
    "\n",
    "Na aula de hoje, vamos explorar os seguintes tópicos em Python:\n",
    "\n",
    "- 1) Introdução\n",
    "- 2) Classificadores de margem\n",
    "- 3) Support Vector Machines\n",
    "- 4) Funções de kernel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-23T21:12:01.218161Z",
     "start_time": "2022-02-23T21:11:57.508617Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "____\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Introdução\n",
    "\n",
    "Na aula de hoje, falaremos sobre um dos mais interessantes métodos de aprendizagm supervisionada: **SVM** (**S**upport **V**ector **M**achines).\n",
    "\n",
    "Este método tem uma construção extremamente elegante e robusta, que, apesar de complexa, pode ser entendida em termos geométricos simples. Nesta aula, vamos explorar os principais aspectos desta construção, em uma apresentação mais alto-nível, sem nos preocuparmos demais com os detalhes matemáticos.\n",
    "\n",
    "Um ponto importantíssimo sobre SVMs, que é o que de fato lhes confere poder tão elevado, é sua **capacidade de produzir uma hipótese simples, a partir de um conjunto de hipóteses complexo**, o que tem consequências diretas na **capacidade de generalização** de modelos SVM: eles são capazes de generalizar muito bem, apesar das hipóteses produzidas parecerem altamente complexas, o que é algo formidável!\n",
    "\n",
    "Não entraremos nos detalhes matemáticos que justificam e provam os pontos acima. Mas, para quem tiver interesse, quase todo livro-texto de machine learning aborda estas questões. Recomendo, em particular, [este livro](https://www.google.com.br/books/edition/Learning_with_Kernels/7r34DwAAQBAJ?hl=pt-BR&gbpv=1&dq=learning+with+kernels&printsec=frontcover), que detalha extensivamente SVM e métodos relacionados; ou então [este livro](https://cs.nyu.edu/~mohri/mlbook/), que aborda formalmente a teoria de aprendizagem, eventualmente culminando no enorme sucesso de SVMs. Por fim, aproveito também pra recomendar [este curso](https://work.caltech.edu/index.html) do Caltech, onde os tópicos abordados no livro do Mohri são apresentados de maneira clara, direta, e muito ilustrativa. Esta é minha maior recomendação para quem tem interesse em iniciar os estudos em teoria de aprendizagem estatística, e então ter todo o fundamento rigoroso pra entender porque SVMs são tão interessantes!\n",
    "\n",
    "Agora, vamos iniciar nossa exposição, introduzindo um conceito fundamental para SVMs: a **margem**, e os chamados **classificadores de margem**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "____\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Classificadores de margem\n",
    "\n",
    "Considere o seguinte dataset supervisionado de um problema de classificação binário:\n",
    "\n",
    "<img src=https://s3-sa-east-1.amazonaws.com/lcpi/d8d83e07-66d8-47c6-a5e4-4a3e232481e2.PNG width=400>\n",
    "\n",
    "É visível que os dados são linearmente separáveis. De fato, existem infinitas retas possíveis que separam perfeitamente as duas classes. Alguns exemplos:\n",
    "\n",
    "<img src=https://s3-sa-east-1.amazonaws.com/lcpi/bbf3d0a6-4707-471b-9f19-7290c13e4f23.PNG width=400>\n",
    "\n",
    "Vamos olhar separadamente para cada um destes três modelos:\n",
    "\n",
    "<img src=https://s3-sa-east-1.amazonaws.com/lcpi/6c0bea51-dda9-400c-bb68-d7396e4fa606.PNG width=900>\n",
    "\n",
    "Apesar de ambos separarem perfeitamente os dados de treino (erro de treino é nulo!), podemos nos perguntar: qual deles tem potencial de apresentar **melhor generalização?**\n",
    "\n",
    "Para refletirmos sobre isso, considere que queremos classificar o ponto de teste indicado em preto:\n",
    "\n",
    "<img src=https://s3-sa-east-1.amazonaws.com/lcpi/fbed2deb-bb38-4ef5-8558-99b7aff5c7b7.PNG width=900>\n",
    "\n",
    "É super razoável que este ponto seja classificado como sendo da classe vermelha, não é mesmo? No entanto, o primeiro classificador irá classificá-lo como pertencente à classe azul."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos nos perguntar: por que isso acontece?\n",
    "\n",
    "Intuitivamente, é possível dizer que a fronteira de decisão do primeiro modelo está \"**muito próxima**\" dos pontos da classe vermelha, não é? \n",
    "\n",
    "Mas, se estamos avaliando a **fronteira de decisão**, é razoável que nos importemos, na realidade, com a proximidade entre ela **e os \"pontos mais externos\"** da respectiva classe, não é mesmo? Afinal, intuitivamente, estes são os pontos que exercem maior influência sobre a fornteira de decisão, justo?\n",
    "\n",
    "É aqui que entra o conceito de **margem**:\n",
    "\n",
    "> Chamamos de **margem** a **menor distância** entre os pontos de treino e a fronteira de decisão\n",
    "\n",
    "A seguir, visualizamos as margens associadas a cada um dos três modelos:\n",
    "\n",
    "<img src=https://s3-sa-east-1.amazonaws.com/lcpi/72f191fc-7bc0-4451-923a-ecb939e88111.PNG width=900>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dada a definição de margem, fica claro que o terceiro modelo seria nossa melhor opção, pois ele **apresenta maior margem**. De fato, intuitivamente, um classificador que apresenta maior margem, terá melhores chances de generalização!\n",
    "\n",
    "> Chamamos de **classificador de margem máxima** um classificador **linear** que é construído de modo que a margem seja maximizada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No entanto, a intuição de que um classificador de margem máxima sempre será nossa melhor opção não é 100% correta... Considere o dataset a seguir, bem parecido com o dataset original, mas contendo um outlier da classe vermelha:\n",
    "\n",
    "<img src=https://s3-sa-east-1.amazonaws.com/lcpi/79154075-f167-4c46-b361-e1febbaa4d83.PNG width=400>\n",
    "\n",
    "Devido a este outlier, até mesmo o classificador de margem máxima terá uma generalização ruim, pois mesmo a margem máxima é extremamente estreita, o que aumenta a variância do modelo:\n",
    "\n",
    "<img src=https://s3-sa-east-1.amazonaws.com/lcpi/da32de74-064a-43a7-98cc-366a348fe275.PNG width=400>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E pode ser que problemas ainda mais graves ocorram, caso outliers ainda mais \"extremos\" existam, como no caso do dataset a seguir:\n",
    "\n",
    "<img src=https://s3-sa-east-1.amazonaws.com/lcpi/7be60cd3-e7f8-46bd-85bc-73ec1d4b0bb5.PNG width=400>\n",
    "\n",
    "Neste caso, nós não temos mais separabilidade linear, de modo que a construção de um classificador de margem máxima não é mais possível.\n",
    "\n",
    "Como resolver este problema?\n",
    "\n",
    "Com a introdução do **classificador de margem suave (*soft margin classifier*)**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classificador de margem suave\n",
    "\n",
    "Um classificador de margem suave é obtido com uma modificação extremamente simples com relação ao classificador de margem máxima: a permissão de que **erros de classificação** sejam cometidos na base de treino! Uma possível solução seria algo assim:\n",
    "\n",
    "<img src=https://s3-sa-east-1.amazonaws.com/lcpi/cfacd48d-9713-4bec-9fd7-ea1b4ae380f5.PNG width=400>\n",
    "\n",
    "Neste caso, temos duas observações incorretas dentro da margem - e está tudo bem, pois dados que estas observações são outliers, não precisamos mos preocupar em ajustar a margem a elas!\n",
    "\n",
    "Com isso, temos um modelo com um viés um pouco maior, mas com variância bem menor - caminhamos na direção do **sweet spot** de generalização!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, portanto, que a introdução de uma margem suave pode ser visto como **um procedimento de regularização** aplicado ao classificador de margem! E é exatamente assim que veremos este procedimento quando chegarmos em SVM - um procedimento de regularização!\n",
    "\n",
    "> Um classificador de margem suave é também chamado de **classificador de vetores de suporte (*support vector classifier*)**, sendo que os pontos que \"apoiam\" a margem são justamente os chamados **vetores de suporte (*support vectors*)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estamos chegando perto das SVMs!\n",
    "\n",
    "Mas, antes de chegarmos lá, é importante frisarmos um ponto fundamental:\n",
    "\n",
    "> A **fronteira de decisão** de um classificador de margem suave (de vetores de suporte) é **linear** no espaço de features em que o classificador é treinado, ou seja, a hipótese treinada (isto é, a superfície de decisão) será **um hiperplano** de dimensão $D-1$, onde $D$ é a dimensão do espaço de features\n",
    "\n",
    "Vamos entender melhor o comentário acima com alguns exemplos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $D=2$\n",
    "\n",
    "Se temos uma duas features$(X_1, X_2)$, teremos um **hiperplano $1-$dimensional**, que nada mais é que **uma reta** como fronteira de decisão:\n",
    "\n",
    "<img src=https://s3-sa-east-1.amazonaws.com/lcpi/6ca9b6d4-90fb-4d92-8a73-42ecec09d562.PNG width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $D=3$\n",
    "\n",
    "Se temos uma três features$(X_1, X_2, X_3)$, teremos um **hiperplano $2-$dimensional**, que nada mais é que **um plano** como fronteira de decisão:\n",
    "\n",
    "<img src=https://s3-sa-east-1.amazonaws.com/lcpi/b91951b3-9256-4a38-abd6-287811332db8.PNG width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $D=1$\n",
    "\n",
    "Se temos uma única feature $(X)$, teremos um **hiperplano $0-$dimensional**, que nada mais é que **um ponto** como fronteira de decisão:\n",
    "\n",
    "<img src=https://s3-sa-east-1.amazonaws.com/lcpi/000d5eb5-25e7-485b-bf36-6ad944970e27.PNG width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para dimensões maiores, $D > 3$, não conseguimos mais visualizar a fronteira de decisão, mas, se qualquer forma, ela será linear (por isso, um hiper**plano**!).\n",
    "\n",
    "Agora estamos prontos para introduzir as tão aguardadas **Support Vector Machines!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "____\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Support Vector Machines\n",
    "\n",
    "Imagine que temos o seguinte dataset (com uma única dimensão):\n",
    "\n",
    "<img src=https://s3-sa-east-1.amazonaws.com/lcpi/e00c77e0-14fb-472f-b9ae-f6af7b229ad5.PNG width=400>\n",
    "\n",
    "É evidente que este dataset **não é linearmente separável**! Portanto, não conseguimos produzir um classificador de vetores de suporte para este dataset **no espaço de features original**.\n",
    "\n",
    "Mas, aí entra uma ideia muito interessante: e se nós **levarmos os dados para um ou outro espaço?**\n",
    "\n",
    "Seria possível que no espaço original os dados não sejam linearmente separáveis, mas **o sejam** em algum outro espaço?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bom, a priori, vamos tentar algo bem simples... Que tal introduzirmos **uma nova feature** $X_2 = X_1^2$? O que aconteceria neste caso?\n",
    "\n",
    "De fato, ao **introduzirmos** uma nova feature, estamos fazendo com que **cada observação passe a ser caracterizada por duas features ao invés de uma única**!\n",
    "\n",
    "Ou seja, nosso espaço de features efetivamente muda de $\\mathbb{R}^1$ para $\\mathbb{R}^2$! Veja:\n",
    "\n",
    "<img src=https://s3-sa-east-1.amazonaws.com/lcpi/8d5e6199-8a33-45cf-9e4f-c6507024fb36.PNG width=800>\n",
    "\n",
    "O procedimento que fizemos é chamado de **feature map**, e ele é matematicamente representado pelo mapa (função) $\\Phi$. \n",
    "\n",
    "> Como $\\Phi$ leva observações do espaço original ($1$D, uma única feature $X_1$, para vetores do novo espaço ($2$D, duas features, $X_1$ e $X_2$), o denotamos como: \n",
    "$$\\begin{align*}\n",
    "\\Phi \\ \\colon \\ & \\mathbb{R}^1 \\longrightarrow \\mathbb{R}^2 \\\\\n",
    "& \\vec{x} = X_1 \\longmapsto \\Phi(\\vec{x}) = (X_1, X_2)\n",
    "\\end{align*}$$\n",
    "\n",
    "Note que no caso ilustrado acima, temos $X_2 = X_1^2$, isto é, $\\Phi(\\vec{x}) = (X_1, X_1^2)$.\n",
    "\n",
    "Então, apesar do nome assustador, o feature map é algo que já estávamos acostumados a fazer, quando usamos o [PolynomialFeatures](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html), lembra? :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um pouco mais de terminologia:\n",
    "\n",
    "> O \"espaço original\" é comumente chamado de **espaço de input** (representaremos por $\\mathcal{X}$); enquanto o espaço após a aplicação do feature map é chamado de **espaço de features (representaremos por $\\mathcal{Z}$)**\n",
    "\n",
    "<img src=https://miro.medium.com/max/872/1*zWzeMGyCc7KvGD9X8lwlnQ.png width=400>\n",
    "\n",
    "Também é comum se referir ao espaço de features como **espaço z**, devido à comum notação $\\Phi(\\vec{x}) \\equiv \\vec{z}$. Neste caso, teríamos:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\Phi \\ \\colon \\ & \\mathbb{R}^1 \\longrightarrow \\mathbb{R}^2 \\\\\n",
    "& \\vec{x} = X_1 \\longmapsto \\Phi(\\vec{x}) = \\vec{z} = (Z_1, Z_2)\n",
    "\\end{align*}$$\n",
    "\n",
    "Onde, no nosso caso, temos simplesmente $Z_1 = X_1$ e $Z_2 = X_1^2$.\n",
    "\n",
    "De maneira genérica, definimos o feature map como:\n",
    "\n",
    "$$\\boxed{\\begin{align*}\n",
    "\\Phi \\ \\colon \\ \\mathcal{X} & \\longrightarrow \\mathcal{Z} \\\\\n",
    "\\vec{x} & \\longmapsto \\vec{z} = \\Phi(\\vec{x})\n",
    "\\end{align*}}$$\n",
    "\n",
    "Onde os espaços de input e de features são definidos adequadamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma vez que temos estas definições, podemos perceber a real utilidade do feature map: **os dados não eram linearmente separáveis no espaço de input, mas passaram a ser no espaço de features!**\n",
    "\n",
    "Isso é realmente formidável, pois, se temos dados linearmente separáveis, podemos **treinar um classificador de margem suave** no espaço de features!\n",
    "\n",
    "Isso pode parecer estranho, pois, afinal, gostaríamos de separar os dados no espaço original, não é mesmo?\n",
    "\n",
    "Na verdade, nosso objetivo é que os dados sejam separados, **não importa em que espaço**! Se conseguirmos encontrar um espaço onde há separabilidade através da aplicação de um feature map, bastaria **aplicar o mesmo feature map** aos dados de treino e de teste, e a separabilidade sempre estará garantida!\n",
    "\n",
    "Muito legal, não é mesmo?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos gerar alguns dados pra ver isto funcionando na prática:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-23T21:12:01.403535Z",
     "start_time": "2022-02-23T21:12:01.220130Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_1</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.496714</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.138264</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.647689</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.523030</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.234153</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.234137</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.579213</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.767435</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.469474</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.542560</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.463418</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.465730</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.241962</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-1.913280</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-1.724918</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.562288</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-1.012831</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.314247</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.908024</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-1.412304</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         x_1  y\n",
       "0   0.496714  0\n",
       "1  -0.138264  0\n",
       "2   0.647689  0\n",
       "3   1.523030  1\n",
       "4  -0.234153  0\n",
       "5  -0.234137  0\n",
       "6   1.579213  1\n",
       "7   0.767435  0\n",
       "8  -0.469474  0\n",
       "9   0.542560  0\n",
       "10 -0.463418  0\n",
       "11 -0.465730  0\n",
       "12  0.241962  0\n",
       "13 -1.913280  1\n",
       "14 -1.724918  1\n",
       "15 -0.562288  0\n",
       "16 -1.012831  0\n",
       "17  0.314247  0\n",
       "18 -0.908024  0\n",
       "19 -1.412304  1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "data = {\"x_1\": np.random.normal(0, 1, 20)}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df[\"y\"] = df[\"x_1\"].apply(lambda x: 1 if abs(x) > 1.1 else 0)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-23T21:12:01.926555Z",
     "start_time": "2022-02-23T21:12:01.406535Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.scatterplot(data=df, x=\"x_1\", y=np.zeros(df.shape[0]), hue=\"y\")\n",
    "\n",
    "sns.despine()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-23T21:12:02.198380Z",
     "start_time": "2022-02-23T21:12:01.930533Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df[\"x_2\"] = df[\"x_1\"]**2\n",
    "\n",
    "sns.scatterplot(data=df, x=\"x_1\", y=\"x_2\", hue=\"y\")\n",
    "\n",
    "sns.despine()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E claro, embora tenhamos visto um feature map do tipo $\\Phi : \\mathbb{R}^1 \\rightarrow \\mathbb{R}^2$, eles podem assumir as mais diferentes formas! Em geral, podemos definir um feature map genérico como $\\Phi : \\mathcal{X} \\rightarrow \\mathcal{Z}$ (de forma concreta, $\\Phi : \\mathbb{R}^m \\rightarrow \\mathbb{R}^n$, para $m, n$ dimensões arbitrárias!)\n",
    "\n",
    "Mais um exemplo, $\\Phi : \\mathbb{R}^2 \\rightarrow \\mathbb{R}^3$:\n",
    "\n",
    "<img src=https://s3-sa-east-1.amazonaws.com/lcpi/105e0718-f938-4d09-919a-b37f79b410f7.PNG width=400>\n",
    "\n",
    "<img src=https://i.pinimg.com/originals/bc/6b/75/bc6b756f5bf44b54f269b2c076cff162.gif width=400>\n",
    "\n",
    "Finalmente, agora temos todos os elementos necessários para entender o que são as SVMs:\n",
    "\n",
    "> Uma **Support Vector Machine** nada mais é que **um classificador de margem suave** treinado **no espaço de features**. Portanto, este classificador pressupõe a aplicação prévia de um **feature map** aos dados no espaço de input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da discussão acima, ficou claro que é justamente o feature map que dá grande poder às SVMs. De fato, a possibilidade de conseguirmos separabilidade linear é algo formidável!\n",
    "\n",
    "Neste contexto, uma pergunta natural é: como escolher um bom feature map? Formalmente, existem infinitos feature maps possíveis! Como escolher, dentre infinitas opções, exatamente o mapa exato que nos garante separabilidade linear no espaço de features? Embora esta pergunta não seja fácil de responder, existem algumas técnicas para nos ajudar a escolher bons feature maps (vamos discutir sobre isso mais a diante).\n",
    "\n",
    "Além disso, existe um segundo problema, ainda maior: suponha que queiramos introduzir um kernel que leva os pontos para um espaço de features de altíssima dimensionalidade (algo como $\\Phi : \\mathbb{R}^2 \\mapsto \\mathbb{R}^{10000}$).\n",
    "\n",
    "É de se esperar que este seja um feature map **operacionalmente custoso** de ser calculado, não é mesmo? Imagina, ter que aplicar esta transformação para todos os pontos de treino, e depois de teste! De fato, isso rapidamente se torna computacionalmente impraticável...\n",
    "\n",
    "Pra solucionar este problema, foi introduzido o uso de **funções de kernel** para capturar um aspecto importante dos feature maps! \n",
    "\n",
    "Para entender o que é feito, antes precisamos entender um pouco melhor qual é a hipótese associada a classificadores de margem. A próxima seção será um pouco mais matemática, mas é super importante para que possamos realmente entender como as funções de kernel entram em jogo!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hipótese dos classificadores de margem\n",
    "\n",
    "A fronteira de decisão (também chamada de **função de decisão**), que é a hipótese modelada em classificadores de margem, é definida como:\n",
    "\n",
    "$$ f_{H, \\vec{w}}(\\vec{x}) = \\text{sign} \\left ( \\langle \\vec{w} , \\vec{x} \\rangle + b\\right ) \\ ,$$\n",
    "\n",
    "Onde $\\langle \\vec{w} , \\vec{x} \\rangle \\equiv \\vec{w} \\cdot \\vec{x} = w_1 X_1 + w_2 X_2 + \\cdots + w_d X_d = \\sum_{i=1}^d w_i X_i$ é simplesmente o **produto interno** entre o **vetor de parâmetros** $\\vec{w}$ (que costumávamos chamar de $\\vec{b}$) e o vetor de features $\\vec{x}$; e $\\text{sign}(x)$ é a função sinal:\n",
    "\n",
    "$ \\text{sign}(x) = \n",
    "\\begin{cases}\n",
    "1 \\hspace{0.5cm} \\text{se } x > 0 \\\\\n",
    "0 \\hspace{0.5cm} \\text{se } x = 0 \\\\\n",
    "-1 \\hspace{0.5cm} \\text{se } x < 0\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "<img src=https://upload.wikimedia.org/wikipedia/commons/thumb/4/4f/Signum_function.svg/1200px-Signum_function.svg.png width=300>\n",
    "\n",
    "Note que, com esta definição de hipótese, teremos $\\hat{y} = \\{+1, -1\\}$ como os targets preditos, de modo que conseguimos diretamente representar as duas classes de um problema de classificação binário!\n",
    "\n",
    "<img src=https://s3-sa-east-1.amazonaws.com/lcpi/e761bb75-fa4d-45f0-9b86-3181f7edfd4e.PNG width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceba que a função de decisão **é linear**: por este motivo que os classificadores de margem produzem fronteiras de decisão que são **hiperplanos**, ou seja, lineares!\n",
    "\n",
    "Aliás, note que nossa hipótese é simplesmente a hipótese da regressão linear passada como input à função sinal! (muito parecido com o logit, que tem a hipótese da regressão linear passado à função sigmoide). Por isso, faz sentido que a fronteira de decisão seja linear!\n",
    "\n",
    "> A priori, bastaria que passássemos esta hipótese para um algoritmo de aprendizagem **com o objetivo de determinar o vetor de parâmetros $\\vec{w}$ e b**, e, pronto, teríamos nosso modelo treinado!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No entanto, existe um claro \"problema\" com a hipótese como definimos acima: ela é definida **no espaço de input**, que pode muito bem ser tal que **os dados não são linearmente separáveis**. Ora, se for esse o caso, é evidente que um classificador linear não fará um bom trabalho...\n",
    "\n",
    "Aí, entra em cena o feature map! Depois de algun passos algébricos (vide as referências para detalhes), é possível mostrar que **dada a aplicação do feature map**, o a hipótese passa a ser dada por:\n",
    "\n",
    "$$f_{H, \\alpha}(\\vec{x}) = \\text{sign} \\left ( \\sum_{i=1}^N y_i \\alpha_i \\langle \\Phi(\\vec{x}) , \\Phi(\\vec{x}_i) \\rangle + b\\right ) $$\n",
    "\n",
    "Abrindo o somatório: $f_{H, \\vec{w}}(\\vec{x}) = \\text{sign} \\left (  y_1 \\alpha_1 \\langle \\Phi(\\vec{x}) , \\Phi(\\vec{x}_1) \\rangle + y_2 \\alpha_2 \\langle \\Phi(\\vec{x}) , \\Phi(\\vec{x}_2) \\rangle + \\cdots + y_N \\alpha_N \\langle \\Phi(\\vec{x}) , \\Phi(\\vec{x}_N) \\rangle + b\\right ) $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entendendo os termos:\n",
    "\n",
    "- Cada $\\vec{x}_i$, $i = 1, 2, \\cdots, N$ é uma das $N$ **observações de treino**; e cada $y_i$ é o respectivo target;\n",
    "- Cada $\\alpha_i$ é um [multiplicador de lagrange](https://en.wikipedia.org/wiki/Lagrange_multiplier). Podemos entendê-los simplesmente como **coeficientes numéricos associados a cada observação de treino**, sendo que $\\alpha_i > 0$. Podemos entender estes coeficientes como sendo substitutos ao $\\vec{w}$ (uma representação [dual](https://en.wikipedia.org/wiki/Duality_(optimization)) do hiperplano);\n",
    "- $\\langle \\Phi(\\vec{x}) , \\Phi(\\vec{x}_i) \\rangle$ é o produto interno entre a observação de teste $\\vec{x}$ e cada observação de treino $\\vec{x}_i$, **no espaço de features**, ou seja, após a aplicação do feature map!\n",
    "\n",
    "\n",
    "> **Obs.:** na nossa notação de **espaço z**, poderíamos escrever $\\Phi(\\vec{x}) = \\vec{z}$ e $\\Phi(\\vec{x}_i) = \\vec{z}_i$.\n",
    "<br><br>\n",
    "No entanto, vamos manter a notação explicita $\\Phi(\\vec{x})$. Não parece, mas isso na verdade facilitará nossa notação!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, um ponto importantíssimo:\n",
    "\n",
    "> Na prática, após o treinamento da hipótese acima, teremos que **muitos $\\alpha_i$ serão identicamente nulos!** Isto quer dizer que a soma acima será simplificada, dado que muitos termos irão ser eliminados.\n",
    "<br><br>\n",
    "Portanto, fica evidente que a fronteira de decisão será **influenciada apenas pelos pontos cujos $\\alpha_i$ associados não são nulos!**\n",
    "<br><br>\n",
    "Estes pontos que \"sobrevivem\" são justamente os **vetores de suporte**, isto é, **no espaço de features**, estes são os pontos que definem a margem, \"apoiando\" a fronteira de decisão!\n",
    "\n",
    "<img src=https://learnopencv.com/wp-content/uploads/2018/07/support-vectors-and-maximum-margin.png width=300>\n",
    "\n",
    "No processo de treinamento da hipótese como definida acima, determinamos justamente os $\\alpha_i$, determinando, assim, quais observações de teste realmente são relevantes para a classificação (isto é, os vetores de suporte). \n",
    "\n",
    "O algoritmo de aprendizagem para este problema está inserido no paradigma de otimização conhecido como [programação quadrátrica](https://en.wikipedia.org/wiki/Quadratic_programming). Além dos textos acima, recomendo [este post](https://towardsdatascience.com/support-vector-machines-dual-formulation-quadratic-programming-sequential-minimal-optimization-57f4387ce4dd#d326) para quem estiver interessado em detalhes do procedimento!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estamos quase lá!\n",
    "\n",
    "Note que a hipótese depende apenas **do produto interno entre as observações no espaço de features**.\n",
    "\n",
    "Agora entra justamente o problema que indicamos anteriormente: para calcular $\\langle \\Phi(\\vec{x}) , \\Phi(\\vec{x}_i) \\rangle$, a priori precisaríamos antes calcular $\\Phi(\\vec{x})$ e $\\Phi(\\vec{x}_i)$, o que envolve a aplicação do feature map, algo que é computacionalmente muito custoso! O produto interno em si, não é tão custoso - difícil mesmo é \"levar\" as observações explicitamente para o espaço de features!\n",
    "\n",
    "> Seria ótimo se fosse possível calcularmos o produto interno $\\langle \\Phi(\\vec{x}) , \\Phi(\\vec{x}_i) \\rangle$ **diretamente a partir de $\\vec{x}$ e $\\vec{x}_i$**, sem que necessitemos previamente aplicar o feature map para obter  $\\Phi(\\vec{x})$ e $\\Phi(\\vec{x}_i)$, não é mesmo?\n",
    "\n",
    "A boa notícia é que isso é sim possível, e isso é **exatamente o que as funções de kernel fazem!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "____\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Funções de kernel\n",
    "\n",
    "Uma função de kernel $\\kappa$ nada mais é que uma **medida de similaridade** entre dois vetores $\\vec{x}$ e $\\vec{x}'$ (que no nosso caso, são observações). Definimos como:\n",
    "\n",
    "$$\\boxed{\\begin{align*}\n",
    "\\kappa \\ \\colon \\ & \\mathcal{X} \\times \\mathcal{X} \\longrightarrow \\mathbb{R} \\\\\n",
    "& (\\vec{x}, \\vec{x}') \\longmapsto \\kappa(\\vec{x}, \\vec{x}')\n",
    "\\end{align*}}$$\n",
    "\n",
    "Ou seja, um kernel é uma função que, dadas duas observações $\\vec{x}$ e $\\vec{x}'$, retorna um número real que caracteriza o quão similar as duas observações são entre si."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Há diversas formas possíveis de expressar a similaridade entre dois vetores. No entanto, uma forma particularmente natural de fazê-lo é justamente através do **produto interno**, dada sua relação com a [similaridade de coseno](https://en.wikipedia.org/wiki/Cosine_similarity) (lembre-se que $\\langle \\Phi(\\vec{x}) , \\Phi(\\vec{x}') \\rangle = \\left | \\Phi(\\vec{x}) \\right | \\left | \\Phi(\\vec{x}') \\right | \\cos \\left ({\\theta_{\\vec{x}, \\vec{x}'}} \\right )$):\n",
    "\n",
    "<img src=https://www.pyimagesearch.com/wp-content/uploads/2014/02/sim_metric_cosine.png width=600>\n",
    "\n",
    "<img src=https://www.dummies.com/wp-content/uploads/369549.image3.jpg width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assim, definimos **o produto interno no espaço de features** como sendo as medidas de similaridade entre os pontos neste espaço:\n",
    "\n",
    "$$ \\kappa_\\Phi(\\vec{x}, \\vec{x}') = \\langle \\Phi(\\vec{x}) , \\Phi(\\vec{x}') \\rangle \\ ,$$\n",
    "\n",
    "onde o subscrito $\\Phi$ deixa explícito que a dependência funcional explícita da função de kernel, depende da escolha do feature map considerado!\n",
    "\n",
    "Agora, vem um ponto muitíssimo importante:\n",
    "\n",
    "> Note que $\\kappa_\\Phi$ nos dá **uma forma de calcular $\\langle \\Phi(\\vec{x}) , \\Phi(\\vec{x}') \\rangle$ diretamente a partir de $\\vec{x}$ e $\\vec{x}'$!**\n",
    "<br><br>\n",
    "Ou seja, apesar de **considerarmos implicitamente o feature map $\\Phi$ na dependêcia funcional de $\\kappa_\\Phi$**, nós não precisamos efetivamente calcular $\\Phi(\\vec{x})$ e $\\Phi(\\vec{x}')$!\n",
    "<br><br>\n",
    "Com isso, reduzimos drasticamente os custos computacionais associados à hipótese!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que nossa hipótese depende **apenas do produto interno** entre os vetores no espaço de features:\n",
    "\n",
    "$$f_{H, \\alpha}(\\vec{x}) = \\text{sign} \\left ( \\sum_{i=1}^N y_i \\alpha_i \\langle \\Phi(\\vec{x}) , \\Phi(\\vec{x}_i) \\rangle + b\\right ) $$\n",
    "\n",
    "Mas, escrita da maneira acima, temos todos os problemas de custo computacional. No entanto, agora conhecemos uma forma **extremamente eficiente** de calcular $\\langle \\Phi(\\vec{x}) , \\Phi(\\vec{x}_i) \\rangle$, que é usando a definição acima de kernel, $\\langle \\Phi(\\vec{x}) , \\Phi(\\vec{x}_i) \\rangle = \\kappa_\\Phi(\\vec{x}, \\vec{x}_i)$:\n",
    "\n",
    "$$\\boxed{f_{H, \\alpha}(\\vec{x}) = \\text{sign} \\left ( \\sum_{i=1}^N y_i \\alpha_i\\kappa_\\Phi(\\vec{x}, \\vec{x}_i) + b\\right )} $$\n",
    "\n",
    "Agora sim, temos uma forma eficiente de treinarmos e avaliarmos nossa hipótese!\n",
    "\n",
    "Com isso, tornamos o principal elemento das SVMs algo muito mais eficiente! Por este motivo, a utilização de funções de kernel no contexto de SVM é conhecida como **kernel trick**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A exposição acima pode ter sido um tanto abstrata, então vamos ver um exemplo que vai deixar claro como de fato é possível calcularmos $\\langle \\Phi(\\vec{x}) , \\Phi(\\vec{x}') \\rangle$ diretamente a partir de $\\vec{x}$ e $\\vec{x}'$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____________\n",
    "\n",
    "#### Exemplo de aplicação do kernel trick\n",
    "\n",
    "Considere que temos $\\mathcal{X} = \\mathbb{R}^2$, isto é, $\\vec{x} = (X_1, X_2)$, um espaço de input de duas dimensões.\n",
    "\n",
    "Vamos considerar um feature map $\\Phi : \\mathbb{R}^2 \\rightarrow \\mathbb{R}^6$, ou seja, teremos $\\mathcal{Z} = \\mathbb{R}^6$ como espaço de features. Explicitamente, a aplicação do feature map é:\n",
    "\n",
    "$$\\Phi(\\vec{x}) = \\left(1, X_1^2, X_2^2, \\sqrt{2}X_1, \\sqrt{2}X_2, \\sqrt{2}X_1 X_2 \\right )$$\n",
    "\n",
    "Assim, tomando duas observações genéricas $\\vec{x}$ e  $\\vec{x}'$, temos:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\langle \\Phi(\\vec{x}) , \\Phi(\\vec{x}') \\rangle &= \\langle \\left(1, X_1^2, X_2^2, \\sqrt{2}X_1, \\sqrt{2}X_2, \\sqrt{2}X_1 X_2 \\right ), \\left(1, X'^2_1, X'^2_2, \\sqrt{2}X'_1, \\sqrt{2}X'_2, \\sqrt{2}X'_1 X'_2 \\right )\\rangle =\n",
    "\\\\\n",
    "\\\\\n",
    "&= 1 + X_1^2 X'^2_1 + X_2^2 X'^2_2 + \\left ( \\sqrt{2}X_1 \\right) \\left ( \\sqrt{2}X'_1 \\right) + \\left ( \\sqrt{2}X_2 \\right )\\left ( \\sqrt{2}X'_2  \\right )+ \\left (\\sqrt{2}X_1 X_2 \\right )\\left (\\sqrt{2}X'_1 X'_2 \\right )  =\n",
    "\\\\\n",
    "\\\\\n",
    "&= 1 + X_1^2 X'^2_1 + X_2^2 X'^2_2 + 2 \\left ( X_1 X'_1 + X_2 X'_2 + X_1 X'_1 X_2 X'_2 \\right ) \n",
    "\\end{align*}$$\n",
    "\n",
    "Vamos rearranjar os termos acima da seguinte maneira:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\langle \\Phi(\\vec{x}) , \\Phi(\\vec{x}') \\rangle &= 1 + \\left ( X_1^2 X'^2_1 + X_2^2 X'^2_2 + 2  X_1 X'_1 X_2 X'_2  \\right ) + 2 \\left ( X_1 X'_1 + X_2 X'_2\\right ) =\n",
    "\\\\\n",
    "\\\\\n",
    "&= 1 + \\left ( X_1 X'_1 + X_2 X'_2\\right )^2 + 2 \\left ( X_1 X'_1 + X_2 X'_2\\right ) = \n",
    "\\\\ \n",
    "\\\\\n",
    "&= \\Big ( 1 + \\left ( X_1 X'_1 + X_2 X'_2 \\right ) \\Big)^2\n",
    "\\end{align*}$$\n",
    "\n",
    "Agora, note que: $\\langle \\vec{x} , \\vec{x}' \\rangle = \\langle (X_1, X_2), (X'_1, X'_2) \\rangle = X_1 X'_1 + X_2 X'_2$, exatamente como aparece no resultado acima! Sendo assim, temos: \n",
    "\n",
    "$$\\langle \\Phi(\\vec{x}) , \\Phi(\\vec{x}') \\rangle = \\left ( 1 + \\langle \\vec{x} , \\vec{x}' \\rangle \\right)^2$$\n",
    "\n",
    "Ou seja, mostramos que é possível escrever o produto interno entre os vetores **no espaço de features** em termos (do produto interno) dos vetores **no espaço de input!**. Oras, isso é justamente o kernel:\n",
    "\n",
    "$$ \\kappa_\\Phi(\\vec{x}, \\vec{x}') = \\left ( 1 + \\langle \\vec{x} , \\vec{x}' \\rangle \\right)^2 = \\langle \\Phi(\\vec{x}) , \\Phi(\\vec{x}') \\rangle$$\n",
    "\n",
    "Este é o ponto! Fizemos o exemplo a seguir para ver de fato como a utilização do kernel é correspondente ao produto interno das observações no espaço de features! Isto é, de fato, $\\kappa_\\Phi(\\vec{x}, \\vec{x}') = \\langle \\Phi(\\vec{x}) , \\Phi(\\vec{x}') \\rangle$!\n",
    "\n",
    "É por isso que dizemos que o kernel nos permite **calcular o produto interno entre dois vetores no espaço de features** sem explicitamente \"**ter que visitar**\" o espaço de features! Este é o ganho de eficiência que os kernels proporcionam!\n",
    "_____________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da mesma forma que existem infinitos feature maps possíveis, a variedade de kernels também é imensa! Apesar dos kernels oferecerem uma vantagem operacional absurda com relação à aplicação explícita do feature map, o problema de escolha de um kernel adequado para um determinado problema ainda existe.\n",
    "\n",
    "Na prática, existem algumas formas de propor kernels, mas este não é um tema fácil. Existe todo um conjunto de métodos e técnicas que se utilizam de kerneles para tarefas de aprendizagem, os chamados [métodos de kernel](https://en.wikipedia.org/wiki/Kernel_method).\n",
    "\n",
    "Apesar da enorme liberdade no design de kernels, há algumas classes particulares de kernels que são comumente utilizados:\n",
    "\n",
    "- Linear kernel: $\\kappa_\\Phi(\\vec{x}, \\vec{x}') = \\langle \\vec{x}, \\vec{x}'\\rangle $\n",
    "<br><br>\n",
    "- Polynomial kernel: $\\kappa_\\Phi(\\vec{x}, \\vec{x}') = (\\gamma \\langle \\vec{x}, \\vec{x}'\\rangle + r)^d$\n",
    "<br><br>\n",
    "- Radial Basis Function (RBF) kernel: $\\kappa_\\Phi(\\vec{x}, \\vec{x}') = \\exp(-\\gamma \\|\\vec{x}-\\vec{x}'\\|^2)$\n",
    "<br><br>\n",
    "- Sigmoid kernel: $\\kappa_\\Phi(\\vec{x}, \\vec{x}') = \\tanh(\\gamma \\langle \\vec{x},\\vec{x}'\\rangle + r)$\n",
    "\n",
    "> No exemplo explícito que fizemos acima, usamos justamente um kernel polinomial com $r=\\gamma=1$ e $d=2$!\n",
    "\n",
    "Note que a dependência funcional dos kernels muda, dependendo exatamente do feature map específico que eles representam. No entanto, em todos os casos, as features no espaço de input são utilizadas, o que garante a eficiência!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "\n",
    "Agora que entendemos o SVM, vamos ver sua aplicação com o sklearn a um problema de classificação!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "____\n",
    "____"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
