{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula 2 - SVM\n",
    "\n",
    "Na aula de hoje, vamos explorar os seguintes tópicos em Python:\n",
    "\n",
    "- 1) Introdução\n",
    "- 2) Classificadores de margem\n",
    "- 3) Support Vector Machines\n",
    "- 4) Funções de kernel\n",
    "- 5) SVM na prática com o sklearn\n",
    "- 6) SVM para regressão "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-10T21:57:07.101052Z",
     "start_time": "2022-06-10T21:57:06.845196Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-10T21:57:46.293504Z",
     "start_time": "2022-06-10T21:57:07.105048Z"
    }
   },
   "outputs": [],
   "source": [
    "from ml_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "____\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Introdução\n",
    "\n",
    "Na aula de hoje, falaremos sobre um dos mais interessantes métodos de aprendizagm supervisionada: **SVM** (**S**upport **V**ector **M**achines).\n",
    "\n",
    "Este método tem uma construção extremamente elegante e robusta, que, apesar de complexa, pode ser entendida em termos geométricos simples. Nesta aula, vamos explorar os principais aspectos desta construção, em uma apresentação mais alto-nível, sem nos preocuparmos demais com os detalhes matemáticos.\n",
    "\n",
    "Um ponto importantíssimo sobre SVMs, que é o que de fato lhes confere poder tão elevado, é sua **capacidade de produzir uma hipótese simples, a partir de um conjunto de hipóteses complexo**, o que tem consequências diretas na **capacidade de generalização** de modelos SVM: eles são capazes de generalizar muito bem, apesar das hipóteses produzidas parecerem altamente complexas, o que é algo formidável!\n",
    "\n",
    "Não entraremos nos detalhes matemáticos que justificam e provam os pontos acima. Mas, para quem tiver interesse, quase todo livro-texto de machine learning aborda estas questões. Recomendo, em particular, [este livro](https://www.google.com.br/books/edition/Learning_with_Kernels/7r34DwAAQBAJ?hl=pt-BR&gbpv=1&dq=learning+with+kernels&printsec=frontcover), que detalha extensivamente SVM e métodos relacionados; ou então [este livro](https://cs.nyu.edu/~mohri/mlbook/), que aborda formalmente a teoria de aprendizagem, eventualmente culminando no enorme sucesso de SVMs. Por fim, aproveito também pra recomendar [este curso](https://work.caltech.edu/index.html) do Caltech, onde os tópicos abordados no livro do Mohri são apresentados de maneira clara, direta, e muito ilustrativa. Esta é minha maior recomendação para quem tem interesse em iniciar os estudos em teoria de aprendizagem estatística, e então ter todo o fundamento rigoroso pra entender porque SVMs são tão interessantes!\n",
    "\n",
    "Agora, vamos iniciar nossa exposição, introduzindo um conceito fundamental para SVMs: a **margem**, e os chamados **classificadores de margem**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "____\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Classificadores de margem\n",
    "\n",
    "Considere o seguinte dataset supervisionado de um problema de classificação binário:\n",
    "\n",
    "<img src=https://s3-sa-east-1.amazonaws.com/lcpi/d8d83e07-66d8-47c6-a5e4-4a3e232481e2.PNG width=400>\n",
    "\n",
    "É visível que os dados são linearmente separáveis. De fato, existem infinitas retas possíveis que separam perfeitamente as duas classes. Alguns exemplos:\n",
    "\n",
    "<img src=https://s3-sa-east-1.amazonaws.com/lcpi/bbf3d0a6-4707-471b-9f19-7290c13e4f23.PNG width=400>\n",
    "\n",
    "Vamos olhar separadamente para cada um destes três modelos:\n",
    "\n",
    "<img src=https://s3-sa-east-1.amazonaws.com/lcpi/6c0bea51-dda9-400c-bb68-d7396e4fa606.PNG width=900>\n",
    "\n",
    "Apesar de ambos separarem perfeitamente os dados de treino (erro de treino é nulo!), podemos nos perguntar: qual deles tem potencial de apresentar **melhor generalização?**\n",
    "\n",
    "Para refletirmos sobre isso, considere que queremos classificar o ponto de teste indicado em preto:\n",
    "\n",
    "<img src=https://s3-sa-east-1.amazonaws.com/lcpi/fbed2deb-bb38-4ef5-8558-99b7aff5c7b7.PNG width=900>\n",
    "\n",
    "É super razoável que este ponto seja classificado como sendo da classe vermelha, não é mesmo? No entanto, o primeiro classificador irá classificá-lo como pertencente à classe azul."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos nos perguntar: por que isso acontece?\n",
    "\n",
    "Intuitivamente, é possível dizer que a fronteira de decisão do primeiro modelo está \"**muito próxima**\" dos pontos da classe vermelha, não é? \n",
    "\n",
    "Mas, se estamos avaliando a **fronteira de decisão**, é razoável que nos importemos, na realidade, com a proximidade entre ela **e os \"pontos mais externos\"** da respectiva classe, não é mesmo? Afinal, intuitivamente, estes são os pontos que exercem maior influência sobre a fornteira de decisão, justo?\n",
    "\n",
    "É aqui que entra o conceito de **margem**:\n",
    "\n",
    "> Chamamos de **margem** a **menor distância** entre os pontos de treino e a fronteira de decisão\n",
    "\n",
    "A seguir, visualizamos as margens associadas a cada um dos três modelos:\n",
    "\n",
    "<img src=https://s3-sa-east-1.amazonaws.com/lcpi/72f191fc-7bc0-4451-923a-ecb939e88111.PNG width=900>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dada a definição de margem, fica claro que o terceiro modelo seria nossa melhor opção, pois ele **apresenta maior margem**. De fato, intuitivamente, um classificador que apresenta maior margem, terá melhores chances de generalização!\n",
    "\n",
    "> Chamamos de **classificador de margem máxima** um classificador **linear** que é construído de modo que a margem seja maximizada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"warning\" style='padding:0.1em; background-color:#E9D8FD; color:#69337A'>\n",
    "<span>\n",
    "<h2>Conclusões</h2>\n",
    "<ul>\n",
    "<li>Um classificador de margem máxima tem alto potencial de generalização, para dados linearmente separáveis;\n",
    "</ul>\n",
    "</span>\n",
    "<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classificador de margem suave\n",
    "\n",
    "Considere o dataset a seguir:\n",
    "\n",
    "<img src=https://s3-sa-east-1.amazonaws.com/lcpi/7be60cd3-e7f8-46bd-85bc-73ec1d4b0bb5.PNG width=400>\n",
    "\n",
    "Neste caso, nós não temos mais separabilidade linear, de modo que a construção de um classificador de margem máxima não é mais possível.\n",
    "\n",
    "Como resolver este problema?\n",
    "\n",
    "Com a introdução do **classificador de margem suave (*soft margin classifier*)**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um classificador de margem suave é obtido com uma modificação extremamente simples com relação ao classificador de margem máxima: a permissão de que **erros de classificação** sejam cometidos na base de treino! Algo assim:\n",
    "\n",
    "<img src=https://s3-sa-east-1.amazonaws.com/lcpi/cfacd48d-9713-4bec-9fd7-ea1b4ae380f5.PNG width=400>\n",
    "\n",
    "\n",
    "Neste caso, temos duas observações incorretas dentro da margem - e está tudo bem, pois dados que estas observações são outliers, não precisamos mos preocupar em ajustar a margem a elas!\n",
    "\n",
    "Com isso, temos um modelo com um viés um pouco maior, mas com variância bem menor - caminhamos na direção do **sweet spot** de generalização!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, portanto, que a introdução de uma margem suave pode ser visto como **um procedimento de regularização** aplicado ao classificador de margem! E é exatamente assim que veremos este procedimento quando chegarmos em SVM - um procedimento de regularização!\n",
    "\n",
    "Um classificador de margem suave é também chamado de **classificador de vetores de suporte (*support vector classifier*)**, sendo que os pontos que \"apoiam\" a margem são justamente os chamados **vetores de suporte (*support vectors*)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"warning\" style='padding:0.1em; background-color:#E9D8FD; color:#69337A'>\n",
    "<span>\n",
    "<h2>Conclusões</h2>\n",
    "<ul>\n",
    "<li>Um classificador de margem máxima tem alto potencial de generalização, para dados linearmente separáveis;<br><br>\n",
    "<li>Um classificador de margem suave (vetores de suporte) introduz um pouco mais de viés ao permitir erros de treino, mas, com isso, aumenta o potencial de generalização, e pode ser aplicável a dados que não são exatamente linearmente separáveis;\n",
    "</ul>\n",
    "</span>\n",
    "<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estamos chegando perto das SVMs!\n",
    "\n",
    "Mas, antes de chegarmos lá, é importante frisarmos um ponto fundamental:\n",
    "\n",
    "> A **fronteira de decisão** de um classificador de margem suave (de vetores de suporte) é **linear no espaço de features** em que o classificador é treinado, ou seja, a hipótese treinada (isto é, a superfície de decisão) será **um hiperplano** de dimensão $D-1$, onde $D$ é a dimensão do espaço de features\n",
    "\n",
    "Vamos entender melhor o comentário acima com alguns exemplos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $D=2$\n",
    "\n",
    "Se temos uma duas features$(X_1, X_2)$, teremos um **hiperplano $1-$dimensional**, que nada mais é que **uma reta** como fronteira de decisão:\n",
    "\n",
    "<img src=https://s3-sa-east-1.amazonaws.com/lcpi/6ca9b6d4-90fb-4d92-8a73-42ecec09d562.PNG width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $D=3$\n",
    "\n",
    "Se temos uma três features$(X_1, X_2, X_3)$, teremos um **hiperplano $2-$dimensional**, que nada mais é que **um plano** como fronteira de decisão:\n",
    "\n",
    "<img src=https://s3-sa-east-1.amazonaws.com/lcpi/b91951b3-9256-4a38-abd6-287811332db8.PNG width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $D=1$\n",
    "\n",
    "Se temos uma única feature $(X)$, teremos um **hiperplano $0-$dimensional**, que nada mais é que **um ponto** como fronteira de decisão:\n",
    "\n",
    "<img src=https://s3-sa-east-1.amazonaws.com/lcpi/000d5eb5-25e7-485b-bf36-6ad944970e27.PNG width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para dimensões maiores, $D > 3$, não conseguimos mais visualizar a fronteira de decisão, mas, se qualquer forma, ela será linear (por isso, um hiper**plano**!).\n",
    "\n",
    "Agora estamos prontos para introduzir as tão aguardadas **Support Vector Machines!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"warning\" style='padding:0.1em; background-color:#E9D8FD; color:#69337A'>\n",
    "<span>\n",
    "<h2>Conclusões</h2>\n",
    "<ul>\n",
    "<li>Um classificador de margem máxima tem alto potencial de generalização, para dados linearmente separáveis;<br><br>\n",
    "<li>Um classificador de margem suave (vetores de suporte) introduz um pouco mais de viés ao permitir erros de treino, mas, com isso, aumenta o potencial de generalização, e pode ser aplicável a dados que não são exatamente linearmente separáveis;<br><br>\n",
    "<li>A fronteira de decisão de um classificador de margem sempre será linear no espaço de features;\n",
    "</ul>\n",
    "</span>\n",
    "<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "____\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Support Vector Machines\n",
    "\n",
    "Imagine que temos o seguinte dataset (com uma única dimensão):\n",
    "\n",
    "<img src=https://s3-sa-east-1.amazonaws.com/lcpi/e00c77e0-14fb-472f-b9ae-f6af7b229ad5.PNG width=400>\n",
    "\n",
    "É evidente que este dataset **não é linearmente separável**! Portanto, não conseguimos produzir um classificador de vetores de suporte para este dataset **no espaço de features original**.\n",
    "\n",
    "Mas, aí entra uma ideia muito interessante: e se nós **levarmos os dados para um ou outro espaço?**\n",
    "\n",
    "Seria possível que no espaço original os dados não sejam linearmente separáveis, mas **o sejam** em algum outro espaço?\n",
    "\n",
    "Bom, a priori, vamos tentar algo bem simples... Que tal introduzirmos **uma nova feature** $X_2 = X_1^2$? O que aconteceria neste caso?\n",
    "\n",
    "De fato, ao **introduzirmos** uma nova feature, estamos fazendo com que **cada observação passe a ser caracterizada por duas features ao invés de uma única**!\n",
    "\n",
    "Ou seja, nosso espaço de features efetivamente muda de $\\mathbb{R}^1$ para $\\mathbb{R}^2$! Veja:\n",
    "\n",
    "<img src=https://s3-sa-east-1.amazonaws.com/lcpi/8d5e6199-8a33-45cf-9e4f-c6507024fb36.PNG width=800>\n",
    "\n",
    "O procedimento que fizemos é chamado de **feature map**, e ele é matematicamente representado pelo mapa (função) $\\Phi$. \n",
    "\n",
    "> Como $\\Phi$ leva observações do espaço original ($1$D, uma única feature $X_1$, para vetores do novo espaço ($2$D, duas features, $X_1$ e $X_2$), o denotamos como: \n",
    "$$\\begin{align*}\n",
    "\\Phi \\ \\colon \\ & \\mathbb{R}^1 \\longrightarrow \\mathbb{R}^2 \\\\\n",
    "& \\vec{x} = X_1 \\longmapsto \\Phi(\\vec{x}) = (X_1, X_2)\n",
    "\\end{align*}$$\n",
    "\n",
    "Note que no caso ilustrado acima, temos $X_2 = X_1^2$, isto é, $\\Phi(\\vec{x}) = (X_1, X_1^2)$.\n",
    "\n",
    "Então, apesar do nome assustador, o feature map é algo que já estávamos acostumados a fazer, quando usamos o [PolynomialFeatures](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html), lembra? :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um pouco mais de terminologia:\n",
    "\n",
    "> O \"espaço original\" é comumente chamado de **espaço de input** (representaremos por $\\mathcal{X}$); enquanto o espaço após a aplicação do feature map é chamado de **espaço de features (representaremos por $\\mathcal{Z}$)**\n",
    "\n",
    "<img src=https://miro.medium.com/max/872/1*zWzeMGyCc7KvGD9X8lwlnQ.png width=400>\n",
    "\n",
    "Também é comum se referir ao espaço de features como **espaço z**, devido à comum notação $\\Phi(\\vec{x}) \\equiv \\vec{z}$. Neste caso, teríamos:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\Phi \\ \\colon \\ & \\mathbb{R}^1 \\longrightarrow \\mathbb{R}^2 \\\\\n",
    "& \\vec{x} = X_1 \\longmapsto \\Phi(\\vec{x}) = \\vec{z} = (Z_1, Z_2)\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma vez que temos estas definições, podemos perceber a real utilidade do feature map: **os dados não eram linearmente separáveis no espaço de input, mas passaram a ser no espaço de features!**\n",
    "\n",
    "Isso é realmente formidável, pois, se temos dados linearmente separáveis, podemos **treinar um classificador de margem suave** no espaço de features!\n",
    "\n",
    "Isso pode parecer estranho, pois, afinal, gostaríamos de separar os dados no espaço original, não é mesmo?\n",
    "\n",
    "Na verdade, nosso objetivo é que os dados sejam separados, **não importa em que espaço**! Se conseguirmos encontrar um espaço onde há separabilidade através da aplicação de um feature map, bastaria **aplicar o mesmo feature map** aos dados de treino e de teste, e a separabilidade sempre estará garantida!\n",
    "\n",
    "Muito legal, não é mesmo?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E claro, embora tenhamos visto um feature map do tipo $\\Phi : \\mathbb{R}^1 \\rightarrow \\mathbb{R}^2$, eles podem assumir as mais diferentes formas! Em geral, podemos definir um feature map genérico como $\\Phi : \\mathcal{X} \\rightarrow \\mathcal{Z}$ (de forma concreta, $\\Phi : \\mathbb{R}^m \\rightarrow \\mathbb{R}^n$, para $m, n$ dimensões arbitrárias!)\n",
    "\n",
    "Mais um exemplo, $\\Phi : \\mathbb{R}^2 \\rightarrow \\mathbb{R}^3$:\n",
    "\n",
    "<img src=https://s3-sa-east-1.amazonaws.com/lcpi/105e0718-f938-4d09-919a-b37f79b410f7.PNG width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=https://s3-sa-east-1.amazonaws.com/lcpi/526d476e-614b-4acf-b6c5-41cad5567b98.gif width=400>\n",
    "\n",
    "Finalmente, agora temos todos os elementos necessários para entender o que são as SVMs:\n",
    "\n",
    "> Uma **Support Vector Machine** nada mais é que **um classificador de margem suave** treinado **no espaço de features**. Portanto, este classificador pressupõe a aplicação prévia de um **feature map** aos dados no espaço de input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"warning\" style='padding:0.1em; background-color:#E9D8FD; color:#69337A'>\n",
    "<span>\n",
    "<h2>Conclusões</h2>\n",
    "<ul>\n",
    "<li>Um classificador de margem máxima tem alto potencial de generalização, para dados linearmente separáveis;<br><br>\n",
    "<li>Um classificador de margem suave (vetores de suporte) introduz um pouco mais de viés ao permitir erros de treino, mas, com isso, aumenta o potencial de generalização, e pode ser aplicável a dados que não são exatamente linearmente separáveis;<br><br>\n",
    "<li>A fronteira de decisão de um classificador de margem sempre será linear no espaço de features;<br><br>\n",
    "<li>Se os dados não forem linearmente separáveis no espaço original (de input), podemos fazer um feature map para levá-los para um espaço de features onde haja separabilidade linear;\n",
    "</ul>\n",
    "</span>\n",
    "<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da discussão acima, ficou claro que é justamente o feature map que dá grande poder às SVMs. De fato, a possibilidade de conseguirmos separabilidade linear é algo formidável!\n",
    "\n",
    "Neste contexto, uma pergunta natural é: como escolher um bom feature map? Formalmente, existem infinitos feature maps possíveis! Como escolher, dentre infinitas opções, exatamente o mapa exato que nos garante separabilidade linear no espaço de features? Embora esta pergunta não seja fácil de responder, existem algumas técnicas para nos ajudar a escolher bons feature maps (vamos discutir sobre isso mais a diante)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Além disso, existe um segundo problema, ainda maior: suponha que queiramos introduzir um kernel que leva os pontos para um espaço de features de altíssima dimensionalidade (algo como $\\Phi : \\mathbb{R}^2 \\mapsto \\mathbb{R}^{10000}$).\n",
    "\n",
    "É de se esperar que este seja um feature map **operacionalmente custoso** de ser calculado, não é mesmo? Imagina, ter que aplicar esta transformação para todos os pontos de treino, e depois de teste! De fato, isso rapidamente se torna computacionalmente impraticável...\n",
    "\n",
    "Pra solucionar este problema, foi introduzido o uso de **funções de kernel** para capturar um aspecto importante dos feature maps! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mas, pra entendermos a importância das funções de kernel, é necessário entendermos uma coisa:\n",
    "\n",
    "> A hipótese do SVM depende apenas do **produto interno** entre as observações no espaço de features!!\n",
    "\n",
    "A hipótese é a seguinte:\n",
    "\n",
    "$$f_{H, \\alpha}(\\vec{x}) = \\text{sign} \\left ( \\sum_{i=1}^N y_i \\alpha_i \\langle \\Phi(\\vec{x}) , \\Phi(\\vec{x}_i) \\rangle + b\\right ) $$\n",
    "\n",
    "Entendo os termos:\n",
    "\n",
    "- Cada $\\vec{x}_i$, $i = 1, 2, \\cdots, N$ é uma das $N$ **observações de treino**; e cada $y_i$ é o respectivo target;\n",
    "- Cada $\\alpha_i$ é um [multiplicador de lagrange](https://en.wikipedia.org/wiki/Lagrange_multiplier). Podemos entendê-los simplesmente como **coeficientes numéricos associados a cada observação de treino**, sendo que $\\alpha_i > 0$. Podemos entender estes coeficientes como sendo substitutos ao $\\vec{w}$ (uma representação [dual](https://en.wikipedia.org/wiki/Duality_(optimization)) do hiperplano);\n",
    "- $\\langle \\Phi(\\vec{x}) , \\Phi(\\vec{x}_i) \\rangle$ é o produto interno entre a observação de teste $\\vec{x}$ e cada observação de treino $\\vec{x}_i$, **no espaço de features**, ou seja, após a aplicação do feature map!\n",
    "\n",
    "> **Obs.:** O produto interno (na forma de produto escalar) é dado por: $\\langle \\vec{a} , \\vec{b} \\rangle \\equiv \\vec{a} \\cdot \\vec{b} = a_1 b_1 + a_2 b_2 + \\cdots + a_d b_d = \\sum_{i=1}^d a_i b_i$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que, de fato, apenas o produto interno aparece! E é isso que permite o uso das funções de kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"warning\" style='padding:0.1em; background-color:#E9D8FD; color:#69337A'>\n",
    "<span>\n",
    "<h2>Conclusões</h2>\n",
    "<ul>\n",
    "<li>Um classificador de margem máxima tem alto potencial de generalização, para dados linearmente separáveis;<br><br>\n",
    "<li>Um classificador de margem suave (vetores de suporte) introduz um pouco mais de viés ao permitir erros de treino, mas, com isso, aumenta o potencial de generalização, e pode ser aplicável a dados que não são exatamente linearmente separáveis;<br><br>\n",
    "<li>A fronteira de decisão de um classificador de margem sempre será linear no espaço de features;<br><br>\n",
    "<li>Se os dados não forem linearmente separáveis no espaço original (de input), podemos fazer um feature map para levá-los para um espaço de features onde haja separabilidade linear;<br><br>\n",
    "<li>A única coisa que importa pro SVM é o produto interno entre as observações no espaço de features;\n",
    "</ul>\n",
    "</span>\n",
    "<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "____\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Funções de kernel\n",
    "\n",
    "Uma função de kernel $\\kappa$ nada mais é que uma **medida de similaridade** entre dois vetores $\\vec{x}$ e $\\vec{x}'$ (que no nosso caso, são observações). Definimos como:\n",
    "\n",
    "$$\\boxed{\\begin{align*}\n",
    "\\kappa \\ \\colon \\ & \\mathcal{X} \\times \\mathcal{X} \\longrightarrow \\mathbb{R} \\\\\n",
    "& (\\vec{x}, \\vec{x}') \\longmapsto \\kappa(\\vec{x}, \\vec{x}')\n",
    "\\end{align*}}$$\n",
    "\n",
    "Ou seja, um kernel é uma função que, dadas duas observações $\\vec{x}$ e $\\vec{x}'$, retorna um número real que caracteriza o quão similar as duas observações são entre si."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ponto fundamental**: uma função de kernel permite que **o produto escalar** entre duas observações seja calculado **no espaço de features**, sem que precisemos **explicitamente levar as observações pro espaço de features**.\n",
    "\n",
    "Ou seja, conseguimos **evitar** que o feautre map, que costuma ser custoso computacionalmente, seja explicitamente aplicado!\n",
    "\n",
    "E uma vez que pro SVM apenas o produto interno interessa, podemos usar diretamente a função de kernel, que é muito mais computacionalmente simples que o feature map explícito!\n",
    "\n",
    "Esse é o chamado **kernel trick**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importantíssimo:\n",
    "\n",
    "Assim, definimos **o produto interno no espaço de features** como sendo as medidas de similaridade entre os pontos neste espaço:\n",
    "\n",
    "$$ \\kappa_\\Phi(\\vec{x}, \\vec{x}') = \\langle \\Phi(\\vec{x}) , \\Phi(\\vec{x}') \\rangle \\ ,$$\n",
    "\n",
    "onde o subscrito $\\Phi$ deixa explícito que a dependência funcional explícita da função de kernel, depende da escolha do feature map considerado!\n",
    "\n",
    "Agora o ponto muitíssimo importante:\n",
    "\n",
    "> Note que $\\kappa_\\Phi$ nos dá **uma forma de calcular $\\langle \\Phi(\\vec{x}) , \\Phi(\\vec{x}') \\rangle$ diretamente a partir de $\\vec{x}$ e $\\vec{x}'$!**\n",
    "<br><br>\n",
    "Ou seja, apesar de **considerarmos implicitamente o feature map $\\Phi$ na dependêcia funcional de $\\kappa_\\Phi$**, nós não precisamos efetivamente calcular $\\Phi(\\vec{x})$ e $\\Phi(\\vec{x}')$!\n",
    "<br><br>\n",
    "Com isso, reduzimos drasticamente os custos computacionais associados à hipótese!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____________\n",
    "\n",
    "#### Exemplo de aplicação do kernel trick\n",
    "\n",
    "Considere que temos $\\mathcal{X} = \\mathbb{R}^2$, isto é, $\\vec{x} = (X_1, X_2)$, um espaço de input de duas dimensões.\n",
    "\n",
    "Vamos considerar um feature map $\\Phi : \\mathbb{R}^2 \\rightarrow \\mathbb{R}^6$, ou seja, teremos $\\mathcal{Z} = \\mathbb{R}^6$ como espaço de features. Explicitamente, a aplicação do feature map é:\n",
    "\n",
    "$$\\Phi(\\vec{x}) = \\left(1, X_1^2, X_2^2, \\sqrt{2}X_1, \\sqrt{2}X_2, \\sqrt{2}X_1 X_2 \\right )$$\n",
    "\n",
    "Assim, tomando duas observações genéricas $\\vec{x}$ e  $\\vec{x}'$, temos:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\langle \\Phi(\\vec{x}) , \\Phi(\\vec{x}') \\rangle &= \\langle \\left(1, X_1^2, X_2^2, \\sqrt{2}X_1, \\sqrt{2}X_2, \\sqrt{2}X_1 X_2 \\right ), \\left(1, X'^2_1, X'^2_2, \\sqrt{2}X'_1, \\sqrt{2}X'_2, \\sqrt{2}X'_1 X'_2 \\right )\\rangle =\n",
    "\\\\\n",
    "\\\\\n",
    "&= 1 + X_1^2 X'^2_1 + X_2^2 X'^2_2 + \\left ( \\sqrt{2}X_1 \\right) \\left ( \\sqrt{2}X'_1 \\right) + \\left ( \\sqrt{2}X_2 \\right )\\left ( \\sqrt{2}X'_2  \\right )+ \\left (\\sqrt{2}X_1 X_2 \\right )\\left (\\sqrt{2}X'_1 X'_2 \\right )  =\n",
    "\\\\\n",
    "\\\\\n",
    "&= 1 + X_1^2 X'^2_1 + X_2^2 X'^2_2 + 2 \\left ( X_1 X'_1 + X_2 X'_2 + X_1 X'_1 X_2 X'_2 \\right ) \n",
    "\\end{align*}$$\n",
    "\n",
    "Vamos rearranjar os termos acima da seguinte maneira:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\langle \\Phi(\\vec{x}) , \\Phi(\\vec{x}') \\rangle &= 1 + \\left ( X_1^2 X'^2_1 + X_2^2 X'^2_2 + 2  X_1 X'_1 X_2 X'_2  \\right ) + 2 \\left ( X_1 X'_1 + X_2 X'_2\\right ) =\n",
    "\\\\\n",
    "\\\\\n",
    "&= 1 + \\left ( X_1 X'_1 + X_2 X'_2\\right )^2 + 2 \\left ( X_1 X'_1 + X_2 X'_2\\right ) = \n",
    "\\\\ \n",
    "\\\\\n",
    "&= \\Big ( 1 + \\left ( X_1 X'_1 + X_2 X'_2 \\right ) \\Big)^2\n",
    "\\end{align*}$$\n",
    "\n",
    "Agora, note que: $\\langle \\vec{x} , \\vec{x}' \\rangle = \\langle (X_1, X_2), (X'_1, X'_2) \\rangle = X_1 X'_1 + X_2 X'_2$, exatamente como aparece no resultado acima! Sendo assim, temos: \n",
    "\n",
    "$$\\langle \\Phi(\\vec{x}) , \\Phi(\\vec{x}') \\rangle = \\left ( 1 + \\langle \\vec{x} , \\vec{x}' \\rangle \\right)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ou seja, mostramos que é possível escrever o produto interno entre os vetores **no espaço de features** em termos (do produto interno) dos vetores **no espaço de input!**. Oras, isso é justamente o kernel:\n",
    "\n",
    "$$ \\kappa_\\Phi(\\vec{x}, \\vec{x}') = \\left ( 1 + \\langle \\vec{x} , \\vec{x}' \\rangle \\right)^2 = \\langle \\Phi(\\vec{x}) , \\Phi(\\vec{x}') \\rangle$$\n",
    "\n",
    "Este é o ponto! Fizemos o exemplo a seguir para ver de fato como a utilização do kernel é correspondente ao produto interno das observações no espaço de features! Isto é, de fato, $\\kappa_\\Phi(\\vec{x}, \\vec{x}') = \\langle \\Phi(\\vec{x}) , \\Phi(\\vec{x}') \\rangle$!\n",
    "\n",
    "É por isso que dizemos que o kernel nos permite **calcular o produto interno entre dois vetores no espaço de features** sem explicitamente \"**ter que visitar**\" o espaço de features! Este é o ganho de eficiência que os kernels proporcionam!\n",
    "_____________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assim, definimos **o produto interno no espaço de features** como sendo as medidas de similaridade entre os pontos neste espaço:\n",
    "\n",
    "$$ \\kappa_\\Phi(\\vec{x}, \\vec{x}') = \\langle \\Phi(\\vec{x}) , \\Phi(\\vec{x}') \\rangle \\ ,$$\n",
    "\n",
    "E a hipótese é reescrita como:\n",
    "\n",
    "$$\\boxed{f_{H, \\alpha}(\\vec{x}) = \\text{sign} \\left ( \\sum_{i=1}^N y_i \\alpha_i\\kappa_\\Phi(\\vec{x}, \\vec{x}_i) + b\\right )} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"warning\" style='padding:0.1em; background-color:#E9D8FD; color:#69337A'>\n",
    "<span>\n",
    "<h2>Conclusões</h2>\n",
    "<ul>\n",
    "<li>Um classificador de margem máxima tem alto potencial de generalização, para dados linearmente separáveis;<br><br>\n",
    "<li>Um classificador de margem suave (vetores de suporte) introduz um pouco mais de viés ao permitir erros de treino, mas, com isso, aumenta o potencial de generalização, e pode ser aplicável a dados que não são exatamente linearmente separáveis;<br><br>\n",
    "<li>A fronteira de decisão de um classificador de margem sempre será linear no espaço de features;<br><br>\n",
    "<li>Se os dados não forem linearmente separáveis no espaço original (de input), podemos fazer um feature map para levá-los para um espaço de features onde haja separabilidade linear;<br><br>\n",
    "<li>A única coisa que importa pro SVM é o produto interno entre as observações no espaço de features;<br><br>\n",
    "<li>Uma função de kernel permite que calculemos o produto interno no espaço de features sem a necessidade de explicitamente aplicar o feature map.\n",
    "</ul>\n",
    "</span>\n",
    "<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da mesma forma que existem infinitos feature maps possíveis, a variedade de kernels também é imensa! Apesar dos kernels oferecerem uma vantagem operacional absurda com relação à aplicação explícita do feature map, o problema de escolha de um kernel adequado para um determinado problema ainda existe.\n",
    "\n",
    "Na prática, existem algumas formas de propor kernels, mas este não é um tema fácil. Existe todo um conjunto de métodos e técnicas que se utilizam de kerneles para tarefas de aprendizagem, os chamados [métodos de kernel](https://en.wikipedia.org/wiki/Kernel_method).\n",
    "\n",
    "Apesar da enorme liberdade no design de kernels, há algumas classes particulares de kernels que são comumente utilizados:\n",
    "\n",
    "- Linear kernel: $\\kappa_\\Phi(\\vec{x}, \\vec{x}') = \\langle \\vec{x}, \\vec{x}'\\rangle $\n",
    "<br><br>\n",
    "- Polynomial kernel: $\\kappa_\\Phi(\\vec{x}, \\vec{x}') = (\\gamma \\langle \\vec{x}, \\vec{x}'\\rangle + r)^d$\n",
    "<br><br>\n",
    "- Radial Basis Function (RBF) kernel: $\\kappa_\\Phi(\\vec{x}, \\vec{x}') = \\exp(-\\gamma \\|\\vec{x}-\\vec{x}'\\|^2)$\n",
    "<br><br>\n",
    "- Sigmoid kernel: $\\kappa_\\Phi(\\vec{x}, \\vec{x}') = \\tanh(\\gamma \\langle \\vec{x},\\vec{x}'\\rangle + r)$\n",
    "\n",
    "> No exemplo explícito que fizemos acima, usamos justamente um kernel polinomial com $r=\\gamma=1$ e $d=2$!\n",
    "\n",
    "Note que a dependência funcional dos kernels muda, dependendo exatamente do feature map específico que eles representam. No entanto, em todos os casos, as features no espaço de input são utilizadas, o que garante a eficiência!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "\n",
    "Agora que entendemos o SVM, vamos ver sua aplicação com o sklearn a um problema de classificação!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "____\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 5) SVM na prática com o sklearn\n",
    "\n",
    "Para construir um classificador SVM com o sklearn, basta usar a classe [SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html).\n",
    "\n",
    "Vamos utilizar o dataset German Credit Risk!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Para a construção de um modelo SVM, é **muito importante que os dados sejam normalizados!**\n",
    "\n",
    "O motivo é bem simples: como vimos acima, o SVM é completamente baseado no kernel, que por sua vez é dado por um produto interno. Já o produto interno, é altamente dependente da **escala das features** (lembre-se, $\\langle \\Phi(\\vec{x}) , \\Phi(\\vec{x}') \\rangle = \\left | \\Phi(\\vec{x}) \\right | \\left | \\Phi(\\vec{x}') \\right | \\cos \\left ({\\theta_{\\vec{x}, \\vec{x}'}} \\right )$, isto é, a norma dos vetores influencia o produto interno!).\n",
    "\n",
    "Portanto, para evitar que efeitos de escala influenciem a classificação, a normalização é um passo extremamente importante!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-10T21:57:46.800213Z",
     "start_time": "2022-06-10T21:57:46.293504Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../datasets/german_credit_data.csv\", index_col=0)\n",
    "\n",
    "X = df.select_dtypes(include=np.number)\n",
    "y = df[\"Risk\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, vamos instanciar a classe do [SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)!\n",
    "\n",
    "Perceba que há muitos hiperparâmetros. Vale a pena estudar um pouco mais o fundo o funcionamento de cada um, e sua influência. Para algumas dicas práticas do uso de SVMs com o sklearn, [clique aqui!](https://scikit-learn.org/stable/modules/svm.html#tips-on-practical-use)\n",
    "\n",
    "Na prática, os principais hiperparâmetros serão `C` e o `gamma` (se o kernel escolhido utilizar este parâmetro, como, por exemplo, o kernel rbf: por isso, o hiperparâmetro `kernel` também é importante!):\n",
    "\n",
    "- `C`: é um parâmetro de regularização, relacionado com a \"suavidade\" da margem. Ele controla o tradeoff entre a complexidade da fronteira de decisão, e erros de classificação que são permitidos. Quanto **menor** o C, mais suave será a fronteira de decisão, pois mais erros de classificação são permitidos (isto é, a margem fica **mais larga**); quanto **maior** C, a tolerância a erros de classificação é menor (e a margem fica menos suave, mais complexa);\n",
    "\n",
    "<img src=\"https://learnopencv.com/wp-content/uploads/2018/07/svm-parameter-c-example.png\" width=500>\n",
    "\n",
    "- `gamma`: define a influência que cada ponto tem na fronteira de decisão. É a \"abertura\" do kernel: quanto **maior** o gamma, a influência é de mais curto alcance, e vice-versa;\n",
    "\n",
    "<img src=\"https://sgao323.gitbooks.io/artificial-intelligence-projects/content/assets/svm_gamma.png\" width=400>\n",
    "\n",
    "- `kernel`: as opções disponíveis são `linear`, `poly`, `rbf` e `sigmoid`, que apresentamos acima. Também é possível utilizar um kernel personalizado pré-calculado (neste caso, usamos a opção `precomputed`).\n",
    "\n",
    "\n",
    "No que diz respeito a `C` e `gamma`, é importante que valores adequados sejam encontrados com o uso de Grid/Random search, usando **valores exponencialmente espaçados**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-10T21:57:46.910084Z",
     "start_time": "2022-06-10T21:57:46.800213Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-10T21:57:47.496509Z",
     "start_time": "2022-06-10T21:57:46.912268Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Métricas de avaliação de treino - com cutoff = 0.50\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bad       0.75      0.22      0.34       240\n",
      "        good       0.74      0.97      0.84       560\n",
      "\n",
      "    accuracy                           0.74       800\n",
      "   macro avg       0.75      0.59      0.59       800\n",
      "weighted avg       0.75      0.74      0.69       800\n",
      "\n",
      "\n",
      "################################################################################\n",
      "\n",
      "Métricas de avaliação de teste - com cutoff = 0.50\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bad       0.44      0.13      0.21        60\n",
      "        good       0.71      0.93      0.81       140\n",
      "\n",
      "    accuracy                           0.69       200\n",
      "   macro avg       0.58      0.53      0.51       200\n",
      "weighted avg       0.63      0.69      0.63       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# probability=True pra que possamos calcular o predict_proba, que é usado na função de métricas!\n",
    "\n",
    "pipe_svc = Pipeline([(\"standard_scaler\", StandardScaler()),\n",
    "                     (\"svc\", SVC(probability=True, random_state=42))])\n",
    "\n",
    "pipe_svc.fit(X_train, y_train)\n",
    "\n",
    "_ = clf_metrics_train_test(pipe_svc, X_train, y_train, X_test, y_test, cutoff=0.5, \n",
    "                           print_plot=True, plot_conf_matrix=False, print_cr=True, pos_label=\"bad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um hiperparâmetro importante, sobretudo quando temos classes desbalanceadas, é o `class_weight=\"balanced\"`.\n",
    "\n",
    "O que isso faz é atribuir um **peso maior na função de custo** a erros cometidos em observações da classe **menos frequente** na base, e vice-versa. \n",
    "\n",
    "Isso corresponde a implicitamente \"focar mais\" nas observações menos frequentes, com o objetivo de compensar o desbalanceamento das classes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-10T21:57:47.948135Z",
     "start_time": "2022-06-10T21:57:47.496509Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Métricas de avaliação de treino - com cutoff = 0.50\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bad       0.76      0.28      0.41       240\n",
      "        good       0.76      0.96      0.85       560\n",
      "\n",
      "    accuracy                           0.76       800\n",
      "   macro avg       0.76      0.62      0.63       800\n",
      "weighted avg       0.76      0.76      0.72       800\n",
      "\n",
      "\n",
      "################################################################################\n",
      "\n",
      "Métricas de avaliação de teste - com cutoff = 0.50\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bad       0.50      0.18      0.27        60\n",
      "        good       0.72      0.92      0.81       140\n",
      "\n",
      "    accuracy                           0.70       200\n",
      "   macro avg       0.61      0.55      0.54       200\n",
      "weighted avg       0.66      0.70      0.65       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipe_svc = Pipeline([(\"standard_scaler\", StandardScaler()),\n",
    "                     (\"svc\", SVC(class_weight=\"balanced\", probability=True, random_state=42))])\n",
    "\n",
    "pipe_svc.fit(X_train, y_train)\n",
    "\n",
    "_ = clf_metrics_train_test(pipe_svc, X_train, y_train, X_test, y_test, cutoff=0.5, \n",
    "                           print_plot=True, plot_conf_matrix=False, print_cr=True, pos_label=\"bad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, podemos montar um gridsearch para otimizar o modelo:\n",
    "\n",
    "**Obs importante: cuidado com o `max_iter`!** É prudente limitá-lo para evitar que o SVM fique em loop infinito sem convergir!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-10T21:57:48.091247Z",
     "start_time": "2022-06-10T21:57:47.950135Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.00000000e-03, 4.64158883e-03, 2.15443469e-02, 1.00000000e-01,\n",
       "       4.64158883e-01, 2.15443469e+00, 1.00000000e+01, 4.64158883e+01,\n",
       "       2.15443469e+02, 1.00000000e+03])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.logspace(-3, 3, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-10T21:58:11.615485Z",
     "start_time": "2022-06-10T21:57:48.096227Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=42, shuffle=True),\n",
       "             estimator=Pipeline(steps=[('ss', StandardScaler()),\n",
       "                                       ('svc',\n",
       "                                        SVC(max_iter=10000, probability=True,\n",
       "                                            random_state=42))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'svc__C': array([1.00000000e-03, 4.64158883e-03, 2.15443469e-02, 1.00000000e-01,\n",
       "       4.64158883e-01, 2.15443469e+00, 1.00000000e+01, 4.64158883e+01,\n",
       "       2.15443469e+02, 1.00000000e+03]),\n",
       "                         'svc__kernel': ['rbf', 'poly', 'sigmoid']},\n",
       "             return_train_score=True, scoring='f1_weighted', verbose=10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_svc = Pipeline([(\"ss\", StandardScaler()),\n",
    "                     (\"svc\", SVC(max_iter=10000, probability=True, random_state=42))])\n",
    "\n",
    "param_grid_svc = {\"svc__kernel\" : [\"rbf\", \"poly\", \"sigmoid\"],\n",
    "                  \"svc__C\" : np.logspace(-3, 3, 10)}\n",
    "\n",
    "splitter = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "grid_svc = GridSearchCV(estimator=pipe_svc,\n",
    "                       param_grid=param_grid_svc,\n",
    "                       scoring=\"f1_weighted\",\n",
    "                       cv=splitter,\n",
    "                       verbose=10,\n",
    "                       n_jobs=-1,\n",
    "                       return_train_score=True)\n",
    "\n",
    "grid_svc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-10T21:58:11.795407Z",
     "start_time": "2022-06-10T21:58:11.622482Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'svc__C': 10.0, 'svc__kernel': 'rbf'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_svc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-10T21:58:12.172191Z",
     "start_time": "2022-06-10T21:58:11.799404Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Métricas de avaliação de treino - com cutoff = 0.50\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bad       0.85      0.25      0.39       240\n",
      "        good       0.75      0.98      0.85       560\n",
      "\n",
      "    accuracy                           0.76       800\n",
      "   macro avg       0.80      0.62      0.62       800\n",
      "weighted avg       0.78      0.76      0.71       800\n",
      "\n",
      "\n",
      "################################################################################\n",
      "\n",
      "Métricas de avaliação de teste - com cutoff = 0.50\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bad       0.69      0.18      0.29        60\n",
      "        good       0.73      0.96      0.83       140\n",
      "\n",
      "    accuracy                           0.73       200\n",
      "   macro avg       0.71      0.57      0.56       200\n",
      "weighted avg       0.72      0.73      0.67       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_ = clf_metrics_train_test(grid_svc, X_train, y_train, X_test, y_test, cutoff=0.5, \n",
    "                           print_plot=True, plot_conf_matrix=False, print_cr=True, pos_label=\"bad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E esse é o SVM para classificação! :)\n",
    "\n",
    "O SVM també pode ser utilizado para regressão, vamos ver a seguir!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-10T21:58:12.347092Z",
     "start_time": "2022-06-10T21:58:12.176191Z"
    }
   },
   "outputs": [],
   "source": [
    "# lição de casa: continuar o grid search, ver se consegue mais que 0.67 no f1 ponderado de teste!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "____\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) SVM para regressão\n",
    "\n",
    "Embora fizemos nossa apresentação do SVM como um classificador, também é possível utilizar este método para regressão!\n",
    "\n",
    "Todos os elementos do classificador SVM (margem, kernel, etc.) também são relevantes aqui.\n",
    "\n",
    "A ideia é bem simples: utilização de um kernel para que um modelo de **regressão linear seja treinado no espaço de features**. No espaço de inputs, este modelo é refletido como uma regressão não-linear (da mesma forma que, no caso de classificação, fronteiras de decisão lineares no espaço de features são refletidas como fronteiras não-lineares no espaço de input).\n",
    "\n",
    "A principal diferença é que o conceito de margem também está presente, de modo que **apenas alguns pontos efetivamente vão contriuir para a regressão**. Neste caso, são os pontos **dentro da margem** (região conhecida como $\\epsilon-$tubo) que serão estes vetores de suporte. Ou seja, pontos que estão fora da margem não contribuem para a função de custo.\n",
    "\n",
    "<img src=https://www.saedsayad.com/images/SVR_5.png width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hiperparâmetro `epsilon`**\n",
    "\n",
    "Este hiperparâmetro determina o epsilon-tubo, dentro do qual nenhuma penalidade é aplicada aos pontos.\n",
    "\n",
    "Se recomenda testar valores entre 1e-3 e 1 ([veja aqui](http://adrem.uantwerpen.be/bibrem/pubs/IJCNN2007.pdf) maiores detalhes!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma comparação entre classificadores e regressores SVM:\n",
    "\n",
    "<img src=https://miro.medium.com/max/1100/1*XE9jt0r1yAW8LnliQ3mllQ.png width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A classe de regressores SVM no sklearn é a [SVR](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html). Vamos vê-la em ação!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-10T22:11:18.636189Z",
     "start_time": "2022-06-10T22:11:17.551570Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../datasets/house_prices.csv\")\n",
    "\n",
    "X = df.drop(columns=[\"Id\", \"SalePrice\"])\n",
    "y = df[\"SalePrice\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = X_train.select_dtypes(include=np.number).dropna(axis=\"columns\")\n",
    "X_test = X_test.loc[:, X_train.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-10T22:11:25.817965Z",
     "start_time": "2022-06-10T22:11:25.663916Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "254     145000\n",
       "1066    178000\n",
       "638      85000\n",
       "799     175000\n",
       "380     127000\n",
       "         ...  \n",
       "1095    176432\n",
       "1130    135000\n",
       "1294    115000\n",
       "860     189950\n",
       "1126    174000\n",
       "Name: SalePrice, Length: 1168, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-10T22:11:57.686522Z",
     "start_time": "2022-06-10T22:11:57.454686Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-10T22:14:24.777814Z",
     "start_time": "2022-06-10T22:14:23.553067Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Métricas de avaliação (dados de treino):\n",
      "\n",
      "R^2: -0.04\n",
      "MAE: 54498.41\n",
      "RMSE: 78919.65\n",
      "MAPE: 31.11%\n",
      "\n",
      "################################################################################\n",
      "\n",
      "Métricas de avaliação (dados de teste):\n",
      "\n",
      "R^2: -0.02\n",
      "MAE: 59512.30\n",
      "RMSE: 88624.65\n",
      "MAPE: 35.93%\n"
     ]
    }
   ],
   "source": [
    "pipe_svr = Pipeline([(\"ss\", StandardScaler()),\n",
    "                     (\"svr\", SVR())])\n",
    "\n",
    "pipe_svr.fit(X_train, y_train)\n",
    "\n",
    "_ = reg_metrics_train_test(pipe_svr, X_train, y_train, X_test, y_test, \n",
    "                           plot=False, dist_resids=False, print_stuff=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O modelo tá super underfitado!\n",
    "\n",
    "Vamos ver se conseguimos melhorar com uma busca aleatória, inicialmente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-10T22:19:35.634889Z",
     "start_time": "2022-06-10T22:19:35.119596Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0001, 10000.0001)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vamos considerar pro hipeparametro \"C\" valores uniformemente distribuidos entre 0.0001 e 10000\n",
    "scipy.stats.uniform(1e-4, 1e4).support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-10T22:20:39.710748Z",
     "start_time": "2022-06-10T22:20:39.478882Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.001, 1.001)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.stats.uniform(1e-3, 1).support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-10T22:25:25.962560Z",
     "start_time": "2022-06-10T22:22:57.485089Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=KFold(n_splits=5, random_state=42, shuffle=True),\n",
       "                   estimator=Pipeline(steps=[('ss', StandardScaler()),\n",
       "                                             ('svr', SVR(max_iter=10000))]),\n",
       "                   n_iter=100, n_jobs=-1,\n",
       "                   param_distributions={'svr__C': <scipy.stats._distn_infrastructure.rv_frozen object at 0x0000021984C29700>,\n",
       "                                        'svr__epsilon': <scipy.stats._distn_infrastructure.rv_frozen object at 0x0000021984C46A00>,\n",
       "                                        'svr__kernel': ['rbf', 'poly',\n",
       "                                                        'sigmoid']},\n",
       "                   random_state=42, return_train_score=True,\n",
       "                   scoring='neg_mean_absolute_error', verbose=10)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_svr = Pipeline([(\"ss\", StandardScaler()),\n",
    "                     (\"svr\", SVR(max_iter=10000))])\n",
    "\n",
    "param_distros_svr = {\"svr__kernel\" : [\"rbf\", \"poly\", \"sigmoid\"],\n",
    "                     \"svr__C\" : scipy.stats.uniform(1e-4, 1e4),\n",
    "                     \"svr__epsilon\" : scipy.stats.uniform(1e-3, 1)}\n",
    "\n",
    "splitter = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "grid_random_svr = RandomizedSearchCV(pipe_svr,\n",
    "                                    param_distros_svr,\n",
    "                                    n_iter=100, # aqui que escolhemos o número de combinações amostradas!\n",
    "                                    cv=splitter,\n",
    "                                    scoring=\"neg_mean_absolute_error\",\n",
    "                                    verbose=10,\n",
    "                                    n_jobs=-1,\n",
    "                                    return_train_score=True,\n",
    "                                    random_state=42)\n",
    "\n",
    "grid_random_svr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-10T22:27:08.592858Z",
     "start_time": "2022-06-10T22:27:08.340488Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'svr__C': 3109.823317156622,\n",
       " 'svr__epsilon': 0.32618332202674705,\n",
       " 'svr__kernel': 'sigmoid'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_random_svr.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-10T22:27:39.038386Z",
     "start_time": "2022-06-10T22:27:38.656058Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Métricas de avaliação (dados de treino):\n",
      "\n",
      "R^2: 0.75\n",
      "MAE: 22006.34\n",
      "RMSE: 38759.14\n",
      "MAPE: 11.47%\n",
      "\n",
      "################################################################################\n",
      "\n",
      "Métricas de avaliação (dados de teste):\n",
      "\n",
      "R^2: 0.72\n",
      "MAE: 23130.68\n",
      "RMSE: 45932.14\n",
      "MAPE: 11.92%\n"
     ]
    }
   ],
   "source": [
    "_ = reg_metrics_train_test(grid_random_svr, X_train, y_train, X_test, y_test, \n",
    "                           plot=False, dist_resids=False, print_stuff=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Já melhorou bastante!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-09T15:33:32.441797Z",
     "start_time": "2022-06-09T15:33:32.321219Z"
    }
   },
   "outputs": [],
   "source": [
    "# pra casa: continue o tuning de hiperparametros!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
