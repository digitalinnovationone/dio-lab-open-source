{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prova módulo 10 - ML2 - Gabarito\n",
    "\n",
    "_____\n",
    "_____\n",
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1) Julgue a afirmativa a seguir como verdadeira ou falsa, justificando sua resposta se a afirmativa for falsa**\n",
    "\n",
    "> Diz-se que um conjunto de dados é linearmente separável quando existe apenas um hiperplano no espaço de features que separa completamente as categorias dos dados. Se existir mais de um hiperplano separador, dizemos que o conjunto de dados é não linearmente separável.\n",
    "\n",
    "( ) Verdadeiro\n",
    "\n",
    "(x) Falso\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>\n",
    "Em problemas de classificação, os dados são ditos lineramente separáveis quando é possível separá-los por *algum* hiperplano (de dimensão adequada). Se houver mais de um hiperplano possível que separa os dados (que é o que tipicamente ocorre, com infinitos planos possíveis), eles continuam sendo linearmente separáveis! \n",
    "    \n",
    "Obs.: no contexto de SVM, escolhemos o melhor hiperplano separador como aquele que maximiza a margem.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "__________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2) Julgue a afirmativa a seguir como verdadeira ou falsa, justificando sua resposta se a afirmativa for falsa**\n",
    "\n",
    "> O random forest é um método de ensemble que se utiliza da metodologia de bagging para gerar modelos que são sequencialmente melhorados levando em consideração os erros cometidos pelos modelos anteriores; Já o adaboost se utiliza do procedimento de boosting para gerar árvores independentes entre si, cujo compartamento coletivo é levado em consideração para a predição final.\n",
    "\n",
    "( ) Verdadeiro\n",
    "\n",
    "(x) Falso\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>\n",
    "As definições estão trocadas!\n",
    "    \n",
    "O random forest de fato usa bagging, mas o objetivo é gerar árvores independentes entre si, cujo compartamento coletivo é levado em consideração para a predição final.\n",
    "    \n",
    "Já o adaboost de fato usa boosting, com o objetivo de gerar modelos fracos que são sequencialmente melhorados levando em consideração os erros cometidos pelos modelos anteriores; \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "__________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3) Qual método de ligação (linkage) no agrupamento hierárquico considera a menor distância entre os elementos dos clusters, e, portanto, é mais propenso a resultar em formas alongadas que não são necessariamente compactas ou circulares?**\n",
    "\n",
    "(x) Ligação única (single linkage)\n",
    "\n",
    "( ) Método de Ward (ward method)\n",
    "\n",
    "( ) Ligação média (average linkage)\n",
    "\n",
    "( ) Ligação completa (complete linkage)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>\n",
    "O método de ligação com as características indicadas no enunciado é o single linkage, veja:\n",
    "</font>\n",
    "\n",
    "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRyni7UH1Nm6STEwPAO2ntd8EtlnKFuxYWlKg&usqp=CAU\" width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "__________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4) Julgue a afirmativa a seguir como verdadeira ou falsa, justificando sua resposta se a afirmativa for falsa**\n",
    "\n",
    "> O método XGBoost é um método de gradient boosting com algumas importantes modificações. Uma destas modificações é a introdução de regularização L1 e L2 ao método.\n",
    "\n",
    "(x) Verdadeiro\n",
    "\n",
    "( ) Falso\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>\n",
    "Tudo certo!\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "__________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5) Julgue a afirmativa a seguir como verdadeira ou falsa, justificando sua resposta se a afirmativa for falsa**\n",
    "\n",
    "> A natureza dos algoritmos de aprendizagem associados a métodos de boosting fazem com que estes métodos sejam naturalmente paralelizáveis (métodos [embaraçosamente paralelizáveis](https://en.wikipedia.org/wiki/Embarrassingly_parallel)), o que reduz consideravelmente o tempo de treinamento de modelos de boosting, sempre que o hardware permitir paralelização (por exemplo, uma CPU com 8 núcleos).\n",
    "\n",
    "( ) Verdadeiro\n",
    "\n",
    "(x) Falso\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>\n",
    "Os métodos de boosting são baseados no princípio de \"melhora incremental\", de modo que os estimadores basais (weak learners) iniciais influenciam os próximos weak learners a serem introduzidos. Por este motivo, não é possível parelilizar o algoritmo de aprendizagem em si (embora seja possível nos aproveitarmos de paralelização em procedimentos de validação cruzada e gridsearch, por exemplo).\n",
    "    \n",
    "O random forest (método de bagging), por sua vez, é altamente paralelizável (veja uma discussão sobre isso [aqui!](https://en.wikipedia.org/wiki/Embarrassingly_parallel))\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "__________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6) Julgue a afirmativa a seguir como verdadeira ou falsa, justificando sua resposta se a afirmativa for falsa**\n",
    "\n",
    "> Na clusterização hierárquica, é possível construirmos um dendograma (diagrama de árvore) que representa a relação hierárquica entre as observações clusterizadas. Um dos usos do dendograma é a determinação do número de clusters, ao traçar retas horizontais (em distâncias fixadas) que intersectam o dendograma -- o número de clusters será o número de interseções entre a reta horizontal e as retas verticais do dendograma.\n",
    "\n",
    "(x) Verdadeiro\n",
    "\n",
    "( ) Falso\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>\n",
    "Tudo certo!\n",
    "</font>\n",
    "\n",
    "<img src=https://www.statisticshowto.com/wp-content/uploads/2016/11/clustergram.png widht=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "__________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7) Julgue a afirmativa a seguir como verdadeira ou falsa, justificando sua resposta se a afirmativa for falsa**\n",
    "\n",
    "> A aplicação do kernel trick permite com que, efetivamente, classificadores SVMs se utilizem de um support vector classifier para determinar a fronteira de decisão no espaço de features, que é diferente do espaço original, dependendo do kernel que foi aplicado. No espaço original, a projeção desta fronteira de decisão pode não ser linear, mas no espaço de features em que o support vector classifier é treinado, a fronteira é linear, tomando a forma de um hiperplano.\n",
    "\n",
    "(x) Verdadeiro\n",
    "\n",
    "( ) Falso\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>\n",
    "Tudo certo!\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "__________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8) Julgue a afirmativa a seguir como verdadeira ou falsa, justificando sua resposta se a afirmativa for falsa**\n",
    "\n",
    "> No contexto de SVMs, o kernel trick refere-se à aplicação explícita do feature map a todas as observações da base de treino e teste, o que faz com que estas observações sejam explicitamente transformadas (isto é, \"levadas\" para o espaço de features determinado pelo feature map). Esta operação explícita é o que faz com que o SVM seja um método tão computacionalmente eficiente.\n",
    "\n",
    "( ) Verdadeiro\n",
    "\n",
    "(x) Falso\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>\n",
    "O kernel trick é sim o que torna o SVM mais eficiente, mas ele substitui justamente a necessidade de aplicação explícita do feature map. Pelo contrário, a função de kernel se utiliza das observações no espaço original (sem a aplicação do feature map), e produz uma métrica de similaridade associada ao produto interno no espaço de features. Como é apenas este produto interno que importa para a construção da hipótese do SVM, a aplicação explícita do feature map (que pode ser computacionalmente custosa) deixa de ser necessária, e, assim, o kernel trick torna o método mais computacionalmente eficiente.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "__________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9) Julgue a afirmativa a seguir como verdadeira ou falsa, justificando sua resposta se a afirmativa for falsa**\n",
    "\n",
    "> Na aprendizagem não-supervisionada (que engloba os métodos de clusterização k-means e DBSCAN vistos em aula), é possível determinar o número adequado de clusters a partir do uso das métricas de classificação precision, recall e f1-score, comparando seus valores na base de treino e de teste.\n",
    "\n",
    "\n",
    "( ) Verdadeiro\n",
    "\n",
    "(x) Falso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>\n",
    "Lembrem-se que na aprendizagem não-supervisionada, nós não temos os targets. E, como vimos extensivamente nos métodos supervisionados, todas as métricas de avaliação (incluindo as citadas) comparam os targets reais com os targets preditos. Assim, como não temos targets, não é nem possível calcular as métricas citadas, tampouco utilizá-las para determinação do número de clusters. Esta determinação (no caso do k-means) se dá ou de maneira qualitativa (guiada pelo negócio), ou apartir de técnicas como o método do cotovelo ou da silhueta.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "__________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10) Julgue as afirmativas a seguir como verdadeiras ou falsas, justificando sua resposta para as afirmativas falsas, se houver alguma**\n",
    "\n",
    "> - 1 - O k-means é um método de clusterização que sempre produz exatamente k clusters com base na estrutura das features. Ele é um método iterativo, que consiste na atribuição de clusters aos pontos e sucessiva atualização dos centroides dos clusters, que são calculados a partir da média das posições dos pontos no respectivo cluster (daí o \"means\" que nomeia o método).\n",
    "\n",
    "> - 2 - O DBSCAN é um método de clusterização que, diferentemente do k-means, é baseado no conceito de densidade para gerar clusters como sendo regiões densas, separados entre si por regiões esparsas (vazias). Segundo este princípio, o método é capaz de identificar outliers.\n",
    "\n",
    "\n",
    "Julgando as afirmativas 1 e 2, temos, respectivamente:\n",
    "\n",
    "(x) Verdadeiro / Verdadeiro\n",
    "\n",
    "( ) Falso / Verdadeiro\n",
    "\n",
    "( ) Verdadeiro / Falso\n",
    "\n",
    "( ) Falso / Falso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>\n",
    "Tudo certo!\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "__________"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
