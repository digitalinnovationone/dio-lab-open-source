{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "362c03d1",
   "metadata": {
    "id": "362c03d1"
   },
   "source": [
    "# Projeto 2 - NLP\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9517a372",
   "metadata": {
    "id": "9517a372"
   },
   "source": [
    " Usaremos os algoritmos aprendidos e as técnicas vistas na segunda parte do curso para extrairmos informações relevantes de texto. Mais precisamente, de publicações no Twitter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955a3a98",
   "metadata": {
    "id": "955a3a98"
   },
   "source": [
    "## Os Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237d5657",
   "metadata": {
    "id": "237d5657"
   },
   "source": [
    "Utilizaremos um Dataset obtido do Twitter com 100K postagens entre os dias 01/08/2018 e 20/10/2018. Cada postagem é classificada como **positiva**, **negativa** ou **neutra**.  \n",
    "\n",
    "Descrição das colunas:\n",
    "\n",
    "- **id**: ID único para o tweet  \n",
    "- **tweet_text**: Texto da publicação no Twitter  \n",
    "- **tweet_date**: Data da publicação no Twitter  \n",
    "- **sentiment**: 0, se negativo; 1, se positivo; 2, se neutro  \n",
    "- **query_used**: Filtro utilizado para buscar a publicação"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc86eb5",
   "metadata": {
    "id": "9dc86eb5"
   },
   "source": [
    "## O Problema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0e1f6f",
   "metadata": {
    "id": "4b0e1f6f"
   },
   "source": [
    "Você deverá desenvolver um modelo para detectar o sentimento de uma publicação do Twitter a classificando em uma das três categorias: **positiva**, **negativa** ou **neutra**. O texto da publicação está disponível na coluna \"tweet_text\". Teste pelo menos 2 técnicas de NLP diferentes e escolha a métrica de avaliação que julgar mais pertinente.  \n",
    "\n",
    "Para ajudar no desenvolvimento, é possível dividir o projeto em algumas fases:\n",
    "\n",
    "- **Análise de consistência dos dados**: analise se os dados estão fazendo sentido, se os campos estão completos e se há dados duplicados ou faltantes. Se julgar necessário, trate-os.    \n",
    "\n",
    "\n",
    "- **Análise exploratória**: analise a sua base como um todo, verifique o balanceamento entre as classes e foque, principalmente, na coluna ```tweet_text```.    \n",
    "\n",
    "\n",
    "- **Pré-processamento e transformações**: projetos de NLP exigem um considerável pré-processamento. Foque no tratamento da string do texto. Procure começar com tratamentos simples e adicione complexidade gradualmente. Nessa etapa você testará diferentes técnicas de transformações, como o Bag Of Words e o TF-IDF.    \n",
    "\n",
    "\n",
    "- **Treinamento do modelo**: depois das transformações, você poderá executar o treinamento do modelo classificador. Nessa etapa o problema se torna semelhante aos abordados na primeira parte do módulo. Você pode testar diversos classificadores como RandomForest, AdaBoost, entre outros. Otimize os hiperparâmetros do modelo com técnicas como a GridSearch e a RandomizedSearch.    \n",
    "\n",
    "\n",
    "- **Conclusões**: descreva, em texto, as conclusões sobre os seus estudos. O modelo é capaz de identificar o sentimento das publicações? É possível extrapolar o modelo para outros contextos, como a análise de sentimento de uma frase qualquer? Pense em questões pertinentes e relevantes que você tenha obtido durante o desenvolvimento do projeto!     \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb23437",
   "metadata": {
    "id": "2bb23437"
   },
   "source": [
    "## Dicas\n",
    "     \n",
    "\n",
    "### Tente encontrar possíveis vieses\n",
    "\n",
    "É muito comum que modelos de NLP possuam fortes vieses, como a tendência de relacionar palavras específicas com alguma classe de saída. Tente encontrar vieses no seu estudo, isso pode ajudar a tirar boas conclusões. o campo \"query_used\" pode ser útil para essa análise.  \n",
    "\n",
    "### O pré-processamento é a chave para um bom desempenho\n",
    "\n",
    "Essa é a etapa que mais vai contribuir para o desempenho do seu modelo. Seja criativo e desenvolva essa etapa de uma maneira que seja fácil de aplicar o mesmo processamento para uma nova base, você terá que fazer isso para gerar a base de submissão.\n",
    "\n",
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "39f34a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from sklearn.metrics import classification_report\n",
    "from nltk import pos_tag\n",
    "import emoji\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem import RSLPStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from tensorflow import keras\n",
    "import glob\n",
    "import tensorflow\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Conv2D, MaxPool2D, Flatten\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow.keras as keras\n",
    "from skimage import feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "876504c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>tweet_date</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>query_used</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1049721159292346368</td>\n",
       "      <td>Rio elege maior bancada policial de sua histór...</td>\n",
       "      <td>Tue Oct 09 18:00:01 +0000 2018</td>\n",
       "      <td>2</td>\n",
       "      <td>folha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1046251157025423360</td>\n",
       "      <td>fiquei tão triste quando eu vi o preço da câme...</td>\n",
       "      <td>Sun Sep 30 04:11:28 +0000 2018</td>\n",
       "      <td>0</td>\n",
       "      <td>:(</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1041744620206653440</td>\n",
       "      <td>Para Theresa May, seu plano para o Brexit é a ...</td>\n",
       "      <td>Mon Sep 17 17:44:06 +0000 2018</td>\n",
       "      <td>2</td>\n",
       "      <td>exame</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1046937084727107589</td>\n",
       "      <td>caralho eu quero proteger a danielly em um pot...</td>\n",
       "      <td>Tue Oct 02 01:37:06 +0000 2018</td>\n",
       "      <td>0</td>\n",
       "      <td>:(</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1047326854229778432</td>\n",
       "      <td>@SiCaetano_ viva o caos :)</td>\n",
       "      <td>Wed Oct 03 03:25:55 +0000 2018</td>\n",
       "      <td>1</td>\n",
       "      <td>:)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94995</th>\n",
       "      <td>1041831666883321856</td>\n",
       "      <td>Cuba e defensor de direitos humanos se unem co...</td>\n",
       "      <td>Mon Sep 17 23:30:00 +0000 2018</td>\n",
       "      <td>2</td>\n",
       "      <td>jornaloglobo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94996</th>\n",
       "      <td>1032352892194369536</td>\n",
       "      <td>#Oportunidade ➡️ Venha fazer parte da nossa eq...</td>\n",
       "      <td>Wed Aug 22 19:44:44 +0000 2018</td>\n",
       "      <td>2</td>\n",
       "      <td>#oportunidade</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94997</th>\n",
       "      <td>1046785538542440448</td>\n",
       "      <td>@96syoo EU SEI 😭😭 é por isso que significa mui...</td>\n",
       "      <td>Mon Oct 01 15:34:55 +0000 2018</td>\n",
       "      <td>0</td>\n",
       "      <td>:(</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94998</th>\n",
       "      <td>1045457469110177792</td>\n",
       "      <td>@louistsexhes N te conheço mas posta :D</td>\n",
       "      <td>Thu Sep 27 23:37:38 +0000 2018</td>\n",
       "      <td>1</td>\n",
       "      <td>:)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94999</th>\n",
       "      <td>1046239135286136832</td>\n",
       "      <td>meu deus :( https://t.co/BlXazxZeKq</td>\n",
       "      <td>Sun Sep 30 03:23:42 +0000 2018</td>\n",
       "      <td>0</td>\n",
       "      <td>:(</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>95000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        id                                         tweet_text  \\\n",
       "0      1049721159292346368  Rio elege maior bancada policial de sua histór...   \n",
       "1      1046251157025423360  fiquei tão triste quando eu vi o preço da câme...   \n",
       "2      1041744620206653440  Para Theresa May, seu plano para o Brexit é a ...   \n",
       "3      1046937084727107589  caralho eu quero proteger a danielly em um pot...   \n",
       "4      1047326854229778432                         @SiCaetano_ viva o caos :)   \n",
       "...                    ...                                                ...   \n",
       "94995  1041831666883321856  Cuba e defensor de direitos humanos se unem co...   \n",
       "94996  1032352892194369536  #Oportunidade ➡️ Venha fazer parte da nossa eq...   \n",
       "94997  1046785538542440448  @96syoo EU SEI 😭😭 é por isso que significa mui...   \n",
       "94998  1045457469110177792            @louistsexhes N te conheço mas posta :D   \n",
       "94999  1046239135286136832                meu deus :( https://t.co/BlXazxZeKq   \n",
       "\n",
       "                           tweet_date  sentiment     query_used  \n",
       "0      Tue Oct 09 18:00:01 +0000 2018          2          folha  \n",
       "1      Sun Sep 30 04:11:28 +0000 2018          0             :(  \n",
       "2      Mon Sep 17 17:44:06 +0000 2018          2          exame  \n",
       "3      Tue Oct 02 01:37:06 +0000 2018          0             :(  \n",
       "4      Wed Oct 03 03:25:55 +0000 2018          1             :)  \n",
       "...                               ...        ...            ...  \n",
       "94995  Mon Sep 17 23:30:00 +0000 2018          2   jornaloglobo  \n",
       "94996  Wed Aug 22 19:44:44 +0000 2018          2  #oportunidade  \n",
       "94997  Mon Oct 01 15:34:55 +0000 2018          0             :(  \n",
       "94998  Thu Sep 27 23:37:38 +0000 2018          1             :)  \n",
       "94999  Sun Sep 30 03:23:42 +0000 2018          0             :(  \n",
       "\n",
       "[95000 rows x 5 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the cvs and transform into dataset\n",
    "tweets = pd.read_csv('./Dados/train/Train3Classes.csv', sep = ',')\n",
    "#dataset visualization\n",
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d070b6",
   "metadata": {},
   "source": [
    "### Data consistence evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "787c0801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 95000 entries, 0 to 94999\n",
      "Data columns (total 5 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   id          95000 non-null  int64 \n",
      " 1   tweet_text  95000 non-null  object\n",
      " 2   tweet_date  95000 non-null  object\n",
      " 3   sentiment   95000 non-null  int64 \n",
      " 4   query_used  95000 non-null  object\n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 3.6+ MB\n"
     ]
    }
   ],
   "source": [
    "#Geting info about the types and absents data\n",
    "tweets.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f49846ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    95000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Verifying if there'is duplicated data\n",
    "tweets.duplicated().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8146e789",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f48764ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = tweets.sample(frac=0.05, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "27f7bcfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9500, 5)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "213606ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Grafic view about setiment column\n",
    "plt.title('Sentimentos')\n",
    "plt.xlabel('sentimento')\n",
    "plt.ylabel('Frequência Absoluta')\n",
    "plt.hist(tweet['sentiment'], 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a40866e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    31696\n",
       "1    31678\n",
       "2    31626\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verifying the balance of dataset\n",
    "tweets['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af24328",
   "metadata": {},
   "source": [
    "### Pre processing and transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "24000727",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpezaTexto(string):\n",
    "    import spacy\n",
    "    nlp = spacy.load(\"pt_core_news_sm\")\n",
    "    from nltk.corpus import stopwords\n",
    "    words_pt = nltk.corpus.stopwords.words('portuguese')\n",
    "    import emoji\n",
    "    happy =  re.sub(r'[:.][)]', 'feliz', string) #char replace: from  :) to feliz\n",
    "    happy_D =  re.sub(r'[:.][D]', 'feliz', happy)#char replace: from  :D to feliz\n",
    "    sad = re.sub(r'[:.][(]', 'triste', happy_D) #char replace: from  :() to triste\n",
    "    html = re.sub(r'ht.*/.*', ' ', sad)#removal of hyperlinks\n",
    "    unicode = unidecode(html)#removal of Grafic accentuations\n",
    "    emoji = emoji.demojize(unicode)#emojis transform to code\n",
    "    no_emoji = re.sub(r'[:][\\w]*[:]', \" \", emoji)#removal of generic emojis\n",
    "    no_arroba = re.sub(r'[\\Wa-zA-Z0-9]*[\\W][@][\\w.]+|[@.][\\w.]+', ' ', no_emoji)#quote removal\n",
    "    especial = re.sub(r'[^a-zA-Z0-9]+', ' ', no_arroba)#specials chars removal\n",
    "    no_number = re.sub(r'[\\d]', ' ', especial)#digits removal\n",
    "    doc = nlp(no_number)#lemmatization instance  \n",
    "    tokens = [token.lemma_.lower() for token in doc] #lemmatization. The consequence is the string tokenization too\n",
    "    for palavra in tokens: #loop to stopwords removals from dataset\n",
    "        if palavra in tokens:\n",
    "            i = tokens.index(palavra)\n",
    "            tokens.pop(i)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "362d7df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying the datacleaning function. The data exit is a list with tokenized words in each row of dataset\n",
    "tweet['tweet_text_clean'] = tweet['tweet_text'].apply(lambda x: limpezaTexto(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8c9fb1a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49959    [com, tag, mande, recado, um, de, delinquente,...\n",
       "21566                               [clima, muito, triste]\n",
       "28463                                                 [na]\n",
       "7366                     [ja, todo, querer, de, em o, cel]\n",
       "72624                                     [vc, eu, triste]\n",
       "                               ...                        \n",
       "32044                [vaga, ajudante, producao, e, de, em]\n",
       "44121                                    [seu, amiga, ter]\n",
       "52111    [ahhh, demais, tempo, passar, manau, quiser, m...\n",
       "64382                         [xr, novo, baratinho, apple]\n",
       "10462           [reafirmar, eleitor, usar, de, em o, de o]\n",
       "Name: tweet_text_clean, Length: 4750, dtype: object"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet['tweet_text_clean']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef04aa6",
   "metadata": {},
   "source": [
    "Turning the list of word of each row in a unique string by applying the join function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "60ff1671",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet['tweet_text_join'] = tweet['tweet_text_clean'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c004e28f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49959    com tag mande recado um de delinquente usar tr...\n",
       "21566                                   clima muito triste\n",
       "28463                                                   na\n",
       "7366                            ja todo querer de em o cel\n",
       "72624                                         vc eu triste\n",
       "                               ...                        \n",
       "32044                       vaga ajudante producao e de em\n",
       "44121                                        seu amiga ter\n",
       "52111    ahhh demais tempo passar manau quiser marcar n...\n",
       "64382                              xr novo baratinho apple\n",
       "10462                  reafirmar eleitor usar de em o de o\n",
       "Name: tweet_text_join, Length: 4750, dtype: object"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cleaned dataset view\n",
    "tweet['tweet_text_join']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "666858c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>tweet_date</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>query_used</th>\n",
       "      <th>tweet_text_clean</th>\n",
       "      <th>tweet_text_join</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49959</th>\n",
       "      <td>1031557679808962560</td>\n",
       "      <td>Vamos com a #Tag #JackDorsey !!! Mande o recad...</td>\n",
       "      <td>Mon Aug 20 15:04:50 +0000 2018</td>\n",
       "      <td>2</td>\n",
       "      <td>#trabalho</td>\n",
       "      <td>[com, tag, mande, recado, um, de, delinquente,...</td>\n",
       "      <td>com tag mande recado um de delinquente usar tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21566</th>\n",
       "      <td>1049286945900249090</td>\n",
       "      <td>O clima tá muito pesado. :(</td>\n",
       "      <td>Mon Oct 08 13:14:37 +0000 2018</td>\n",
       "      <td>0</td>\n",
       "      <td>:(</td>\n",
       "      <td>[clima, muito, triste]</td>\n",
       "      <td>clima muito triste</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28463</th>\n",
       "      <td>1047471835284008960</td>\n",
       "      <td>SOBRA NA :)</td>\n",
       "      <td>Wed Oct 03 13:02:01 +0000 2018</td>\n",
       "      <td>1</td>\n",
       "      <td>:)</td>\n",
       "      <td>[na]</td>\n",
       "      <td>na</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7366</th>\n",
       "      <td>1050729399073542144</td>\n",
       "      <td>@mylifeasmermaid ja curti todas haha queria po...</td>\n",
       "      <td>Fri Oct 12 12:46:25 +0000 2018</td>\n",
       "      <td>1</td>\n",
       "      <td>:)</td>\n",
       "      <td>[ja, todo, querer, de, em o, cel]</td>\n",
       "      <td>ja todo querer de em o cel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72624</th>\n",
       "      <td>1046239958451855362</td>\n",
       "      <td>@dihzaoricardo Vc soh me ilude :(</td>\n",
       "      <td>Sun Sep 30 03:26:58 +0000 2018</td>\n",
       "      <td>0</td>\n",
       "      <td>:(</td>\n",
       "      <td>[vc, eu, triste]</td>\n",
       "      <td>vc eu triste</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32044</th>\n",
       "      <td>1039592115083374592</td>\n",
       "      <td>#Vaga para Ajudante de Produção detalhes e env...</td>\n",
       "      <td>Tue Sep 11 19:10:49 +0000 2018</td>\n",
       "      <td>2</td>\n",
       "      <td>#oportunidade</td>\n",
       "      <td>[vaga, ajudante, producao, e, de, em]</td>\n",
       "      <td>vaga ajudante producao e de em</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44121</th>\n",
       "      <td>1049303038417129473</td>\n",
       "      <td>Qual sua melhor amiga? — Não tenho :( https://...</td>\n",
       "      <td>Mon Oct 08 14:18:34 +0000 2018</td>\n",
       "      <td>0</td>\n",
       "      <td>:(</td>\n",
       "      <td>[seu, amiga, ter]</td>\n",
       "      <td>seu amiga ter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52111</th>\n",
       "      <td>1050718435531993088</td>\n",
       "      <td>@chellbuzogany ahhh, que demais! quanto tempo ...</td>\n",
       "      <td>Fri Oct 12 12:02:51 +0000 2018</td>\n",
       "      <td>1</td>\n",
       "      <td>:)</td>\n",
       "      <td>[ahhh, demais, tempo, passar, manau, quiser, m...</td>\n",
       "      <td>ahhh demais tempo passar manau quiser marcar n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64382</th>\n",
       "      <td>1039950582340100096</td>\n",
       "      <td>iPhone XR: o novo iPhone “baratinho” da Apple ...</td>\n",
       "      <td>Wed Sep 12 18:55:14 +0000 2018</td>\n",
       "      <td>2</td>\n",
       "      <td>exame</td>\n",
       "      <td>[xr, novo, baratinho, apple]</td>\n",
       "      <td>xr novo baratinho apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10462</th>\n",
       "      <td>1048362201663324160</td>\n",
       "      <td>TSE reafirma que eleitor pode usar camiseta de...</td>\n",
       "      <td>Sat Oct 06 00:00:01 +0000 2018</td>\n",
       "      <td>2</td>\n",
       "      <td>jornaloglobo</td>\n",
       "      <td>[reafirmar, eleitor, usar, de, em o, de o]</td>\n",
       "      <td>reafirmar eleitor usar de em o de o</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4750 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        id                                         tweet_text  \\\n",
       "49959  1031557679808962560  Vamos com a #Tag #JackDorsey !!! Mande o recad...   \n",
       "21566  1049286945900249090                        O clima tá muito pesado. :(   \n",
       "28463  1047471835284008960                                        SOBRA NA :)   \n",
       "7366   1050729399073542144  @mylifeasmermaid ja curti todas haha queria po...   \n",
       "72624  1046239958451855362                  @dihzaoricardo Vc soh me ilude :(   \n",
       "...                    ...                                                ...   \n",
       "32044  1039592115083374592  #Vaga para Ajudante de Produção detalhes e env...   \n",
       "44121  1049303038417129473  Qual sua melhor amiga? — Não tenho :( https://...   \n",
       "52111  1050718435531993088  @chellbuzogany ahhh, que demais! quanto tempo ...   \n",
       "64382  1039950582340100096  iPhone XR: o novo iPhone “baratinho” da Apple ...   \n",
       "10462  1048362201663324160  TSE reafirma que eleitor pode usar camiseta de...   \n",
       "\n",
       "                           tweet_date  sentiment     query_used  \\\n",
       "49959  Mon Aug 20 15:04:50 +0000 2018          2      #trabalho   \n",
       "21566  Mon Oct 08 13:14:37 +0000 2018          0             :(   \n",
       "28463  Wed Oct 03 13:02:01 +0000 2018          1             :)   \n",
       "7366   Fri Oct 12 12:46:25 +0000 2018          1             :)   \n",
       "72624  Sun Sep 30 03:26:58 +0000 2018          0             :(   \n",
       "...                               ...        ...            ...   \n",
       "32044  Tue Sep 11 19:10:49 +0000 2018          2  #oportunidade   \n",
       "44121  Mon Oct 08 14:18:34 +0000 2018          0             :(   \n",
       "52111  Fri Oct 12 12:02:51 +0000 2018          1             :)   \n",
       "64382  Wed Sep 12 18:55:14 +0000 2018          2          exame   \n",
       "10462  Sat Oct 06 00:00:01 +0000 2018          2   jornaloglobo   \n",
       "\n",
       "                                        tweet_text_clean  \\\n",
       "49959  [com, tag, mande, recado, um, de, delinquente,...   \n",
       "21566                             [clima, muito, triste]   \n",
       "28463                                               [na]   \n",
       "7366                   [ja, todo, querer, de, em o, cel]   \n",
       "72624                                   [vc, eu, triste]   \n",
       "...                                                  ...   \n",
       "32044              [vaga, ajudante, producao, e, de, em]   \n",
       "44121                                  [seu, amiga, ter]   \n",
       "52111  [ahhh, demais, tempo, passar, manau, quiser, m...   \n",
       "64382                       [xr, novo, baratinho, apple]   \n",
       "10462         [reafirmar, eleitor, usar, de, em o, de o]   \n",
       "\n",
       "                                         tweet_text_join  \n",
       "49959  com tag mande recado um de delinquente usar tr...  \n",
       "21566                                 clima muito triste  \n",
       "28463                                                 na  \n",
       "7366                          ja todo querer de em o cel  \n",
       "72624                                       vc eu triste  \n",
       "...                                                  ...  \n",
       "32044                     vaga ajudante producao e de em  \n",
       "44121                                      seu amiga ter  \n",
       "52111  ahhh demais tempo passar manau quiser marcar n...  \n",
       "64382                            xr novo baratinho apple  \n",
       "10462                reafirmar eleitor usar de em o de o  \n",
       "\n",
       "[4750 rows x 7 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414de2de",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4563ba65",
   "metadata": {},
   "source": [
    "To lead with words in Machine Learn, we need to translate to the algorithm the words into numbers, this transformation we call bag of words - BOW. In this project it will be used Count Vectorizer, TF-IDF and Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "da9ab513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slicing the dataset in feature and target. Used the columns tweet_text_join to apply the method TF-IDF Vectorizer and \n",
    "#CountVectorizer\n",
    "\n",
    "X = tweet['tweet_text_join']\n",
    "y = tweet['sentiment']\n",
    "\n",
    "\n",
    "\n",
    "# slicing the dataset in train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                     y,\n",
    "                                                     test_size = 0.3,\n",
    "                                                     random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9d5479",
   "metadata": {},
   "source": [
    "Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b451b0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insance\n",
    "cv = CountVectorizer()\n",
    "\n",
    "# Transforma os dados nas matrizes de saída\n",
    "X_train_cv = cv.fit_transform(X_train).toarray()\n",
    "X_test_cv = cv.transform(X_test).toarray()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e556dbdc",
   "metadata": {},
   "source": [
    "TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d8eda047",
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying the TfIdf vectorizer\n",
    "tfidf = TfidfVectorizer(use_idf = True)\n",
    "\n",
    "X_train_tfidf = tfidf.fit_transform(X_train).todense()\n",
    "X_test_tfidf  = tfidf.transform(X_test).todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9ae4b6",
   "metadata": {},
   "source": [
    "Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cdfd90ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slicing the dataset in feature and target. Used the columns tweet_text_clean to apply the method word embeddings. \n",
    "# To do this transformation it will need da tokenized data, thats why we will use the tweet_text_clean column\n",
    "\n",
    "X_we = tweet['tweet_text_clean']\n",
    "y_we = tweet['sentiment']\n",
    "\n",
    "\n",
    "# slicing the dataset in train and test\n",
    "X_train_we, X_test_we, y_train_we, y_test_we = train_test_split(X_we,\n",
    "                                                     y_we,\n",
    "                                                     test_size = 0.3,\n",
    "                                                     random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f1277212",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mari.fernandes\\AppData\\Local\\Temp\\ipykernel_18960\\2954864449.py:17: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  X_train_vect = np.array([np.array([modelWord2Vec.wv[i] for i in ls if i in words])\n",
      "C:\\Users\\mari.fernandes\\AppData\\Local\\Temp\\ipykernel_18960\\2954864449.py:19: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  X_test_vect = np.array([np.array([modelWord2Vec.wv[i] for i in ls if i in words])\n"
     ]
    }
   ],
   "source": [
    "X_we = tweet['tweet_text_clean']\n",
    "y_we = tweet['sentiment']\n",
    "\n",
    "\n",
    "# slicing the dataset in train and test\n",
    "X_train_we, X_test_we, y_train_we, y_test_we = train_test_split(X_we,\n",
    "                                                     y_we,\n",
    "                                                     test_size = 0.3,\n",
    "                                                     random_state = 42)\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "#applying the word embeddings\n",
    "modelWord2Vec = Word2Vec(X_train_we, min_count = 2)\n",
    "\n",
    "words = set(modelWord2Vec.wv.index_to_key )\n",
    "\n",
    "X_train_vect = np.array([np.array([modelWord2Vec.wv[i] for i in ls if i in words])\n",
    "                         for ls in X_train_we])\n",
    "X_test_vect = np.array([np.array([modelWord2Vec.wv[i] for i in ls if i in words])\n",
    "                         for ls in X_test_we])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a89592",
   "metadata": {},
   "source": [
    "### Trainning and evaluating the models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993aac2c",
   "metadata": {},
   "source": [
    "Using the Count Vectorizer to fit the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7e3e026f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(X_train, y_train, X_test, y_test):\n",
    "    modelMultinomial = MultinomialNB()\n",
    "    modelGaussian = GaussianNB()\n",
    "    modelRandomforest = RandomForestClassifier()\n",
    "    modelSVC = LinearSVC()\n",
    "\n",
    "    model_list = [modelMultinomial, modelGaussian, modelRandomforest, modelSVC]\n",
    "    resultado_experimentos = {\"estimador\" : [],\n",
    "                              \"f1_treino\" : [],\n",
    "                              \"f1_teste\" : []\n",
    "                             }\n",
    "    for model in model_list:\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        y_pred = model.predict(X_test)\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        #calculating the gap (diference) between f1_train and f1_test \n",
    "        dict_metricas_treino = classification_report(y_train, y_train_pred, output_dict=True)\n",
    "        dict_metricas_test = classification_report(y_test, y_pred, output_dict=True)\n",
    "        f1_treino = dict_metricas_treino[\"weighted avg\"][\"f1-score\"]\n",
    "        f1_teste = dict_metricas_test[\"weighted avg\"][\"f1-score\"]\n",
    "        resultado_experimentos[\"estimador\"].append(str(model))\n",
    "        resultado_experimentos[\"f1_treino\"].append(f1_treino)\n",
    "        resultado_experimentos[\"f1_teste\"].append(f1_teste)\n",
    "        \n",
    "        print('REPORT')\n",
    "        print(f'Model: {model}')\n",
    "        print('TRAIN')\n",
    "        print(classification_report(y_train, y_train_pred))\n",
    "        print('-'*80)\n",
    "        print('TEST')\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        print(\"#\"*80)\n",
    "    \n",
    "    df_results = pd.DataFrame(resultado_experimentos)\n",
    "    df_results[\"gap\"] = (df_results[\"f1_treino\"] - df_results[\"f1_teste\"]).apply(lambda x: x if x > 0 else np.inf)\n",
    "    df_results = df_results.sort_values(\"f1_teste\", ascending=False).sort_values(\"gap\")\n",
    "    return df_results\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af9a7ef",
   "metadata": {},
   "source": [
    "Using Count Vectorizer data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d481866f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPORT\n",
      "Model: MultinomialNB()\n",
      "TRAIN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.92      0.92      1115\n",
      "           1       0.90      0.90      0.90      1087\n",
      "           2       0.94      0.94      0.94      1123\n",
      "\n",
      "    accuracy                           0.92      3325\n",
      "   macro avg       0.92      0.92      0.92      3325\n",
      "weighted avg       0.92      0.92      0.92      3325\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "TEST\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.79      0.78       483\n",
      "           1       0.72      0.71      0.72       464\n",
      "           2       0.83      0.82      0.82       478\n",
      "\n",
      "    accuracy                           0.77      1425\n",
      "   macro avg       0.77      0.77      0.77      1425\n",
      "weighted avg       0.77      0.77      0.77      1425\n",
      "\n",
      "################################################################################\n",
      "REPORT\n",
      "Model: GaussianNB()\n",
      "TRAIN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      1.00      0.83      1115\n",
      "           1       0.96      0.66      0.78      1087\n",
      "           2       1.00      0.88      0.93      1123\n",
      "\n",
      "    accuracy                           0.85      3325\n",
      "   macro avg       0.89      0.85      0.85      3325\n",
      "weighted avg       0.89      0.85      0.85      3325\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "TEST\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.82      0.58       483\n",
      "           1       0.40      0.22      0.28       464\n",
      "           2       0.81      0.51      0.62       478\n",
      "\n",
      "    accuracy                           0.52      1425\n",
      "   macro avg       0.55      0.51      0.50      1425\n",
      "weighted avg       0.56      0.52      0.50      1425\n",
      "\n",
      "################################################################################\n",
      "REPORT\n",
      "Model: RandomForestClassifier()\n",
      "TRAIN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99      1115\n",
      "           1       1.00      0.99      0.99      1087\n",
      "           2       0.98      1.00      0.99      1123\n",
      "\n",
      "    accuracy                           0.99      3325\n",
      "   macro avg       0.99      0.99      0.99      3325\n",
      "weighted avg       0.99      0.99      0.99      3325\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "TEST\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.72      0.72       483\n",
      "           1       0.69      0.69      0.69       464\n",
      "           2       0.76      0.76      0.76       478\n",
      "\n",
      "    accuracy                           0.72      1425\n",
      "   macro avg       0.72      0.72      0.72      1425\n",
      "weighted avg       0.72      0.72      0.72      1425\n",
      "\n",
      "################################################################################\n",
      "REPORT\n",
      "Model: LinearSVC()\n",
      "TRAIN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97      1115\n",
      "           1       0.99      0.97      0.98      1087\n",
      "           2       0.99      0.99      0.99      1123\n",
      "\n",
      "    accuracy                           0.98      3325\n",
      "   macro avg       0.98      0.98      0.98      3325\n",
      "weighted avg       0.98      0.98      0.98      3325\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "TEST\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.81      0.76       483\n",
      "           1       0.73      0.68      0.71       464\n",
      "           2       0.85      0.81      0.83       478\n",
      "\n",
      "    accuracy                           0.77      1425\n",
      "   macro avg       0.77      0.76      0.76      1425\n",
      "weighted avg       0.77      0.77      0.77      1425\n",
      "\n",
      "################################################################################\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>estimador</th>\n",
       "      <th>f1_treino</th>\n",
       "      <th>f1_teste</th>\n",
       "      <th>gap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.919390</td>\n",
       "      <td>0.774037</td>\n",
       "      <td>0.145353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LinearSVC()</td>\n",
       "      <td>0.980181</td>\n",
       "      <td>0.765462</td>\n",
       "      <td>0.214718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RandomForestClassifier()</td>\n",
       "      <td>0.992186</td>\n",
       "      <td>0.724147</td>\n",
       "      <td>0.268040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GaussianNB()</td>\n",
       "      <td>0.848679</td>\n",
       "      <td>0.499235</td>\n",
       "      <td>0.349444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  estimador  f1_treino  f1_teste       gap\n",
       "0           MultinomialNB()   0.919390  0.774037  0.145353\n",
       "3               LinearSVC()   0.980181  0.765462  0.214718\n",
       "2  RandomForestClassifier()   0.992186  0.724147  0.268040\n",
       "1              GaussianNB()   0.848679  0.499235  0.349444"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_model(X_train_cv, y_train, X_test_cv, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90c1421",
   "metadata": {},
   "source": [
    "Using TF-IDF data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "401bc2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPORT\n",
      "Model: MultinomialNB()\n",
      "TRAIN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.94      0.92      1115\n",
      "           1       0.92      0.90      0.91      1087\n",
      "           2       0.96      0.94      0.95      1123\n",
      "\n",
      "    accuracy                           0.93      3325\n",
      "   macro avg       0.93      0.93      0.93      3325\n",
      "weighted avg       0.93      0.93      0.93      3325\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "TEST\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.81      0.77       483\n",
      "           1       0.72      0.69      0.71       464\n",
      "           2       0.85      0.80      0.82       478\n",
      "\n",
      "    accuracy                           0.77      1425\n",
      "   macro avg       0.77      0.77      0.77      1425\n",
      "weighted avg       0.77      0.77      0.77      1425\n",
      "\n",
      "################################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPORT\n",
      "Model: GaussianNB()\n",
      "TRAIN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      1.00      0.83      1115\n",
      "           1       0.95      0.67      0.79      1087\n",
      "           2       1.00      0.88      0.94      1123\n",
      "\n",
      "    accuracy                           0.85      3325\n",
      "   macro avg       0.89      0.85      0.85      3325\n",
      "weighted avg       0.89      0.85      0.85      3325\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "TEST\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.78      0.58       483\n",
      "           1       0.39      0.23      0.29       464\n",
      "           2       0.76      0.53      0.62       478\n",
      "\n",
      "    accuracy                           0.52      1425\n",
      "   macro avg       0.54      0.52      0.50      1425\n",
      "weighted avg       0.54      0.52      0.50      1425\n",
      "\n",
      "################################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPORT\n",
      "Model: RandomForestClassifier()\n",
      "TRAIN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99      1115\n",
      "           1       1.00      0.99      0.99      1087\n",
      "           2       0.98      1.00      0.99      1123\n",
      "\n",
      "    accuracy                           0.99      3325\n",
      "   macro avg       0.99      0.99      0.99      3325\n",
      "weighted avg       0.99      0.99      0.99      3325\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "TEST\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.73      0.74       483\n",
      "           1       0.73      0.64      0.68       464\n",
      "           2       0.75      0.87      0.81       478\n",
      "\n",
      "    accuracy                           0.75      1425\n",
      "   macro avg       0.75      0.75      0.74      1425\n",
      "weighted avg       0.75      0.75      0.74      1425\n",
      "\n",
      "################################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPORT\n",
      "Model: LinearSVC()\n",
      "TRAIN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.97      1115\n",
      "           1       0.98      0.96      0.97      1087\n",
      "           2       0.98      1.00      0.99      1123\n",
      "\n",
      "    accuracy                           0.98      3325\n",
      "   macro avg       0.98      0.98      0.98      3325\n",
      "weighted avg       0.98      0.98      0.98      3325\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "TEST\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.77      0.77       483\n",
      "           1       0.76      0.67      0.71       464\n",
      "           2       0.78      0.88      0.83       478\n",
      "\n",
      "    accuracy                           0.77      1425\n",
      "   macro avg       0.77      0.77      0.77      1425\n",
      "weighted avg       0.77      0.77      0.77      1425\n",
      "\n",
      "################################################################################\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>estimador</th>\n",
       "      <th>f1_treino</th>\n",
       "      <th>f1_teste</th>\n",
       "      <th>gap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.929366</td>\n",
       "      <td>0.768432</td>\n",
       "      <td>0.160935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LinearSVC()</td>\n",
       "      <td>0.977089</td>\n",
       "      <td>0.769592</td>\n",
       "      <td>0.207497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RandomForestClassifier()</td>\n",
       "      <td>0.992187</td>\n",
       "      <td>0.744993</td>\n",
       "      <td>0.247194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GaussianNB()</td>\n",
       "      <td>0.851940</td>\n",
       "      <td>0.501731</td>\n",
       "      <td>0.350209</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  estimador  f1_treino  f1_teste       gap\n",
       "0           MultinomialNB()   0.929366  0.768432  0.160935\n",
       "3               LinearSVC()   0.977089  0.769592  0.207497\n",
       "2  RandomForestClassifier()   0.992187  0.744993  0.247194\n",
       "1              GaussianNB()   0.851940  0.501731  0.350209"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_model(X_train_tfidf,y_train, X_test_tfidf, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fa82c9",
   "metadata": {},
   "source": [
    "Using Word Embeddings data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "95fffc39",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [100]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mfit_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_vect\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_vect\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [97]\u001b[0m, in \u001b[0;36mfit_model\u001b[1;34m(X_train, y_train, X_test, y_test)\u001b[0m\n\u001b[0;32m      8\u001b[0m resultado_experimentos \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mestimador\u001b[39m\u001b[38;5;124m\"\u001b[39m : [],\n\u001b[0;32m      9\u001b[0m                           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1_treino\u001b[39m\u001b[38;5;124m\"\u001b[39m : [],\n\u001b[0;32m     10\u001b[0m                           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1_teste\u001b[39m\u001b[38;5;124m\"\u001b[39m : []\n\u001b[0;32m     11\u001b[0m                          }\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m model_list:\n\u001b[1;32m---> 13\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m     16\u001b[0m     y_train_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_train)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:663\u001b[0m, in \u001b[0;36m_BaseDiscreteNB.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    643\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    644\u001b[0m     \u001b[38;5;124;03m\"\"\"Fit Naive Bayes classifier according to X, y.\u001b[39;00m\n\u001b[0;32m    645\u001b[0m \n\u001b[0;32m    646\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    661\u001b[0m \u001b[38;5;124;03m        Returns the instance itself.\u001b[39;00m\n\u001b[0;32m    662\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 663\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    664\u001b[0m     _, n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    666\u001b[0m     labelbin \u001b[38;5;241m=\u001b[39m LabelBinarizer()\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:523\u001b[0m, in \u001b[0;36m_BaseDiscreteNB._check_X_y\u001b[1;34m(self, X, y, reset)\u001b[0m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_X_y\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    522\u001b[0m     \u001b[38;5;124;03m\"\"\"Validate X and y in fit methods.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 523\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:581\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    579\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 581\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    582\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:964\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m    961\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    962\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my cannot be None\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 964\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    967\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    969\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    974\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    975\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    976\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    977\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    979\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric)\n\u001b[0;32m    981\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:746\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    744\u001b[0m         array \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mastype(dtype, casting\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munsafe\u001b[39m\u001b[38;5;124m\"\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    745\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 746\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    749\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m    750\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "fit_model(X_train_vect, X_test, X_test_vect, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90db9d37",
   "metadata": {},
   "source": [
    "### Tunning the models with Hyperparameters to Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836a3ae3",
   "metadata": {},
   "source": [
    "GridSearch with TF-IDF data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "7fd12a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_instance(X_train, y_train):\n",
    "    parametrosRandomForest = {'n_estimators': [10, 20, 50, 100],\n",
    "                              'criterion': ['gini', 'entropy'],\n",
    "                              'max_depth' : [15, 25, 40],\n",
    "                              'min_samples_split': [2, 5, 10],\n",
    "                              'min_samples_leaf' : [1, 5, 10],\n",
    "                              'max_features': ['sqrt', 'log2']\n",
    "                         }\n",
    "\n",
    "    modelRandomforest = RandomForestClassifier()\n",
    "    gridSearch = GridSearchCV(estimator = modelRandomforest, param_grid = parametrosRandomForest)\n",
    "    gridSearch.fit(X_train, y_train)\n",
    "    best_params = gridSearch.best_params_\n",
    "    best_result = gridSearch.best_score_\n",
    "    print(f'Melhores parâmetros do modelo Random Forest:')\n",
    "    for key, value in best_params.items():\n",
    "        print(f'{key}: {value}')\n",
    "    print('Score: ',best_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b39dddca",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [104]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mgrid_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_cv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [103]\u001b[0m, in \u001b[0;36mgrid_instance\u001b[1;34m(X_train, y_train)\u001b[0m\n\u001b[0;32m     13\u001b[0m best_params \u001b[38;5;241m=\u001b[39m gridSearch\u001b[38;5;241m.\u001b[39mbest_params_\n\u001b[0;32m     14\u001b[0m best_result \u001b[38;5;241m=\u001b[39m gridSearch\u001b[38;5;241m.\u001b[39mbest_score_\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMelhores parâmetros do modelo \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m best_params\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "grid_instance(X_train_cv, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ad8dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_instance(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc95d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_instance(X_train_vec, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5751289",
   "metadata": {},
   "source": [
    "##### Tunning the models with the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589f2652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# models instances\n",
    "\n",
    "def tunning(X_train, y_train, X_test, y_test, name_of_bow)\n",
    "\n",
    "    modelRandomforest = RandomForestClassifier(criterion = 'gini', \n",
    "                                           max_depth = 40, \n",
    "                                           max_features = 'log2', \n",
    "                                           min_samples_split =  5)\n",
    "    modelSVC = LinearSVC()\n",
    "\n",
    "    # fitting the models\n",
    "    modelRandomforest.fit(X_train, y_train)\n",
    "    modelSVC.fit(X_train, y_train)\n",
    "# Teste Predictions generates\n",
    "    y_pred_random = modelRandomforest.predict(X_test)\n",
    "    y_train_pred_random = model.predict(X_train)\n",
    "    y_pred_SVC = modelRandomforest.predict(X_test)\n",
    "    y_train_pred_SVC = model.predict(X_train)\n",
    "       \n",
    "    print(f'REPORT with {name_of_the_bow}')\n",
    "    print(f'Model: Random Forest')\n",
    "    print('TRAIN')\n",
    "    print(classification_report(y_train, y_train_pred_random))\n",
    "    print('-'*80)\n",
    "    print('TEST')\n",
    "    print(classification_report(y_test, y_pred_random))\n",
    "    print(\"#\"*80)\n",
    "    print(f'Model: SVC')\n",
    "    print('TRAIN')\n",
    "    print(classification_report(y_train, y_train_pred_SVC))\n",
    "    print('-'*80)\n",
    "    print('TEST')\n",
    "    print(classification_report(y_test, y_pred_SVC))\n",
    "    print(\"#\"*80)\n",
    "    #calculating the gap (diference) between f1_train and f1_test \n",
    "    dict_metricas_treino_random = classification_report(y_train, y_train_pred_random, output_dict = True)\n",
    "    dict_metricas_test_random = classification_report(y_test, y_pred_random, output_dict = True)\n",
    "    \n",
    "    dict_metricas_treino_SVC = classification_report(y_train, y_train_pred_SVC, output_dict = True)\n",
    "    dict_metricas_test_SVC = classification_report(y_test, y_pred_SVC, output_dict = True)\n",
    "    \n",
    "    f1_treino_random = dict_metricas_treino_random[\"weighted avg\"][\"f1-score\"]\n",
    "    f1_teste_random = dict_metricas_test_random[\"weighted avg\"][\"f1-score\"]\n",
    "    \n",
    "    f1_treino_SVC = dict_metricas_treino_SVC[\"weighted avg\"][\"f1-score\"]\n",
    "    f1_teste_SVC = dict_metricas_test_SVC[\"weighted avg\"][\"f1-score\"]\n",
    "    \n",
    "    resultado_experimentos[\"estimador\"].append('Random Forest'))\n",
    "    resultado_experimentos[\"estimador\"].append('SVC'))\n",
    "    resultado_experimentos[\"f1_treino\"].append(f1_treino_random)\n",
    "    resultado_experimentos[\"f1_teste\"].append(f1_teste_random)\n",
    "    resultado_experimentos[\"f1_treino\"].append(f1_treino_SVC)\n",
    "    resultado_experimentos[\"f1_teste\"].append(f1_teste_SVC)\n",
    "    df_results = pd.DataFrame(resultado_experimentos)\n",
    "    \n",
    "    df_results[\"gap\"] = (df_results[\"f1_treino\"] - df_results[\"f1_teste\"]).apply(lambda x: x if x > 0 else np.inf)\n",
    "    df_results = df_results.sort_values(\"f1_teste\", ascending=False).sort_values(\"gap\")\n",
    "    return df_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4953f29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tunning(X_train_cv, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b62d3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tunning(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87c8614",
   "metadata": {},
   "outputs": [],
   "source": [
    "tunning(X_train_vec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9b9c0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58648dca",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe7114c",
   "metadata": {},
   "source": [
    "Using Count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "4d566661",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tweet['tweet_text_join']\n",
    "y = tweet['sentiment']\n",
    "X_train, X_val, y_train, y_val = train_test_split(X,\n",
    "                                                     y,\n",
    "                                                     test_size = 0.3,\n",
    "                                                     random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "c722df52",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "\n",
    "X_train_cv = cv.fit_transform(X_train).toarray()\n",
    "X_val_cv = cv.transform(X_val).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "672369af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2e</th>\n",
       "      <th>aa</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aaaa</th>\n",
       "      <th>aaaaa</th>\n",
       "      <th>aaaaaa</th>\n",
       "      <th>aaaaaaaaaaaaaaaa</th>\n",
       "      <th>aaaaaaaaaaaaaaaaaaaaaaaaah</th>\n",
       "      <th>aaaaaaah</th>\n",
       "      <th>aaaaaar</th>\n",
       "      <th>...</th>\n",
       "      <th>york</th>\n",
       "      <th>youtube</th>\n",
       "      <th>zap</th>\n",
       "      <th>zelandia</th>\n",
       "      <th>zema</th>\n",
       "      <th>zequinha</th>\n",
       "      <th>zerar</th>\n",
       "      <th>zero</th>\n",
       "      <th>ziraldo</th>\n",
       "      <th>zona</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3320</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3321</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3322</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3323</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3324</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3325 rows × 4744 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      2e  aa  aaa  aaaa  aaaaa  aaaaaa  aaaaaaaaaaaaaaaa  \\\n",
       "0      0   0    0     0      0       0                 0   \n",
       "1      0   0    0     0      0       0                 0   \n",
       "2      0   0    0     0      0       0                 0   \n",
       "3      0   0    0     0      0       0                 0   \n",
       "4      0   0    0     0      0       0                 0   \n",
       "...   ..  ..  ...   ...    ...     ...               ...   \n",
       "3320   0   0    0     0      0       0                 0   \n",
       "3321   0   0    0     0      0       0                 0   \n",
       "3322   0   0    0     0      0       0                 0   \n",
       "3323   0   0    0     0      0       0                 0   \n",
       "3324   0   0    0     0      0       0                 0   \n",
       "\n",
       "      aaaaaaaaaaaaaaaaaaaaaaaaah  aaaaaaah  aaaaaar  ...  york  youtube  zap  \\\n",
       "0                              0         0        0  ...     0        0    0   \n",
       "1                              0         0        0  ...     0        0    0   \n",
       "2                              0         0        0  ...     0        0    0   \n",
       "3                              0         0        0  ...     0        0    0   \n",
       "4                              0         0        0  ...     0        0    0   \n",
       "...                          ...       ...      ...  ...   ...      ...  ...   \n",
       "3320                           0         0        0  ...     0        0    0   \n",
       "3321                           0         0        0  ...     0        0    0   \n",
       "3322                           0         0        0  ...     0        0    0   \n",
       "3323                           0         0        0  ...     0        0    0   \n",
       "3324                           0         0        0  ...     0        0    0   \n",
       "\n",
       "      zelandia  zema  zequinha  zerar  zero  ziraldo  zona  \n",
       "0            0     0         0      0     0        0     0  \n",
       "1            0     0         0      0     0        0     0  \n",
       "2            0     0         0      0     0        0     0  \n",
       "3            0     0         0      0     0        0     0  \n",
       "4            0     0         0      0     0        0     0  \n",
       "...        ...   ...       ...    ...   ...      ...   ...  \n",
       "3320         0     0         0      0     0        0     0  \n",
       "3321         0     0         0      0     0        0     0  \n",
       "3322         0     0         0      0     0        0     0  \n",
       "3323         0     0         0      0     0        0     0  \n",
       "3324         0     0         0      0     0        0     0  \n",
       "\n",
       "[3325 rows x 4744 columns]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cv = pd.DataFrame(X_train_cv, columns=cv.get_feature_names())\n",
    "df_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "d7cf1ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "7/7 [==============================] - 2s 92ms/step - loss: 1.0894 - accuracy: 0.4271 - val_loss: 1.0669 - val_accuracy: 0.6161\n",
      "Epoch 2/3\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 1.0284 - accuracy: 0.7579 - val_loss: 1.0183 - val_accuracy: 0.7130\n",
      "Epoch 3/3\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 0.9463 - accuracy: 0.8418 - val_loss: 0.9453 - val_accuracy: 0.7368\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(50, activation = \"relu\", input_shape=(4744,)))\n",
    "model.add(Dense(50))\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(\n",
    "              optimizer = \"adam\",\n",
    "              loss = \"sparse_categorical_crossentropy\",\n",
    "              metrics = [\"accuracy\"]\n",
    "             )\n",
    "\n",
    "results = model.fit(\n",
    "                     X_train_cv, y_train,\n",
    "                     epochs= 3,\n",
    "                     batch_size = 500,\n",
    "                     validation_data = (X_val_cv, y_val)\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "08acec2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6886549790700277\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(results.history[\"val_accuracy\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "f688c9a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.089414</td>\n",
       "      <td>0.427068</td>\n",
       "      <td>1.066922</td>\n",
       "      <td>0.616140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.028412</td>\n",
       "      <td>0.757895</td>\n",
       "      <td>1.018335</td>\n",
       "      <td>0.712982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.946328</td>\n",
       "      <td>0.841805</td>\n",
       "      <td>0.945319</td>\n",
       "      <td>0.736842</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss  accuracy  val_loss  val_accuracy\n",
       "0  1.089414  0.427068  1.066922      0.616140\n",
       "1  1.028412  0.757895  1.018335      0.712982\n",
       "2  0.946328  0.841805  0.945319      0.736842"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(results.history)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7835b05b",
   "metadata": {},
   "source": [
    "TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "398c6c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tweet['tweet_text_join']\n",
    "y = tweet['sentiment']\n",
    "X_train, X_val, y_train, y_val = train_test_split(X,\n",
    "                                                     y,\n",
    "                                                     test_size = 0.3,\n",
    "                                                     random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "d71fbb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(use_idf = True)\n",
    "\n",
    "X_train_tfidf = tfidf.fit_transform(X_train).todense()\n",
    "X_val_tfidf  = tfidf.transform(X_val).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "bbf77f73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "7/7 [==============================] - 2s 96ms/step - loss: 1.0918 - accuracy: 0.4355 - val_loss: 1.0768 - val_accuracy: 0.5642\n",
      "Epoch 2/4\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 1.0499 - accuracy: 0.6869 - val_loss: 1.0245 - val_accuracy: 0.6695\n",
      "Epoch 3/4\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.9496 - accuracy: 0.8256 - val_loss: 0.9108 - val_accuracy: 0.7137\n",
      "Epoch 4/4\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.7548 - accuracy: 0.8827 - val_loss: 0.7481 - val_accuracy: 0.7277\n"
     ]
    }
   ],
   "source": [
    "df_tfidf = pd.DataFrame(X_train_tfidf, columns=cv.get_feature_names())\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(50, activation = \"relu\", input_shape=(df_cv.shape[1],)))\n",
    "model.add(Dense(50))\n",
    "model.add(Dense(50))\n",
    "model.add(Dense(50))\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(\n",
    "              optimizer = \"adam\",\n",
    "              loss = \"sparse_categorical_crossentropy\",\n",
    "              metrics = [\"accuracy\"]\n",
    "             )\n",
    "\n",
    "results = model.fit(\n",
    "                     X_train_tfidf, y_train,\n",
    "                     epochs= 4,\n",
    "                     batch_size = 500,\n",
    "                     validation_data = (X_val_tfidf, y_val)\n",
    "                    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "1cbec5ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6687719374895096\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(results.history[\"val_accuracy\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "81b6689a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.091807</td>\n",
       "      <td>0.435489</td>\n",
       "      <td>1.076808</td>\n",
       "      <td>0.564211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.049915</td>\n",
       "      <td>0.686917</td>\n",
       "      <td>1.024460</td>\n",
       "      <td>0.669474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.949587</td>\n",
       "      <td>0.825564</td>\n",
       "      <td>0.910814</td>\n",
       "      <td>0.713684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.754828</td>\n",
       "      <td>0.882707</td>\n",
       "      <td>0.748094</td>\n",
       "      <td>0.727719</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss  accuracy  val_loss  val_accuracy\n",
       "0  1.091807  0.435489  1.076808      0.564211\n",
       "1  1.049915  0.686917  1.024460      0.669474\n",
       "2  0.949587  0.825564  0.910814      0.713684\n",
       "3  0.754828  0.882707  0.748094      0.727719"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(results.history)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcda818c",
   "metadata": {},
   "source": [
    "Word Embedings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "f295ea94",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 20000\n",
    "maxlen = 80  \n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "2e36c5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_we = tweet['tweet_text_clean']\n",
    "y_we = tweet['sentiment']\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_we,\n",
    "                                                  y_we,\n",
    "                                                  test_size = 0.3,\n",
    "                                                  random_state = 42)\n",
    "\n",
    "vocab_size = 10000\n",
    "embedding_dim = 16\n",
    "max_length = 120\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(num_words = vocab_size)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "sequences = tokenizer.texts_to_sequences(X_train)\n",
    "\n",
    "padded = pad_sequences(sequences,maxlen=max_length)\n",
    "\n",
    "val_sequences = tokenizer.texts_to_sequences(X_val)\n",
    "val_padded = pad_sequences(val_sequences,maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "74f21633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "52/52 [==============================] - 2s 14ms/step - loss: 1.0808 - accuracy: 0.4611 - val_loss: 1.0243 - val_accuracy: 0.5937\n",
      "Epoch 2/5\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.7467 - accuracy: 0.7128 - val_loss: 0.6260 - val_accuracy: 0.7081\n",
      "Epoch 3/5\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.3442 - accuracy: 0.8710 - val_loss: 0.5309 - val_accuracy: 0.7586\n",
      "Epoch 4/5\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.1708 - accuracy: 0.9462 - val_loss: 0.5861 - val_accuracy: 0.7495\n",
      "Epoch 5/5\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0949 - accuracy: 0.9666 - val_loss: 0.6362 - val_accuracy: 0.7586\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_length=max_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation = \"relu\"))\n",
    "model.add(Dense(25))\n",
    "model.add(Dense(25))\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(\n",
    "              optimizer = \"adam\",\n",
    "              loss = \"sparse_categorical_crossentropy\",\n",
    "              metrics = [\"accuracy\"]\n",
    "             )\n",
    "\n",
    "results = model.fit(\n",
    "                     padded, y_train,\n",
    "                     epochs= 5,\n",
    "                     batch_size = 64,\n",
    "                     validation_data = (val_padded, y_val)\n",
    "                    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "cd2f8760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7149473667144776\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(results.history[\"val_accuracy\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "064852ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.081902</td>\n",
       "      <td>0.414436</td>\n",
       "      <td>1.014091</td>\n",
       "      <td>0.615439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.776532</td>\n",
       "      <td>0.666466</td>\n",
       "      <td>0.677765</td>\n",
       "      <td>0.698947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.420119</td>\n",
       "      <td>0.823459</td>\n",
       "      <td>0.575606</td>\n",
       "      <td>0.741754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.240391</td>\n",
       "      <td>0.915188</td>\n",
       "      <td>0.603740</td>\n",
       "      <td>0.760000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.134166</td>\n",
       "      <td>0.955789</td>\n",
       "      <td>0.646227</td>\n",
       "      <td>0.758596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss  accuracy  val_loss  val_accuracy\n",
       "0  1.081902  0.414436  1.014091      0.615439\n",
       "1  0.776532  0.666466  0.677765      0.698947\n",
       "2  0.420119  0.823459  0.575606      0.741754\n",
       "3  0.240391  0.915188  0.603740      0.760000\n",
       "4  0.134166  0.955789  0.646227      0.758596"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(results.history)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05c24a9",
   "metadata": {},
   "source": [
    "### Implications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68377ea4",
   "metadata": {},
   "source": [
    "Descreva, em texto, as conclusões sobre os seus estudos. O modelo é capaz de identificar o sentimento das publicações? É possível extrapolar o modelo para outros contextos, como a análise de sentimento de uma frase qualquer? Pense em questões pertinentes e relevantes que você tenha obtido durante o desenvolvimento do projeto!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a93952",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Projeto 2 - NLP.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
